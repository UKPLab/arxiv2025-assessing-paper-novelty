The ability to memorize and recall sequences of events with temporal dependencies is crucial for biological memory systems that also underpins many other neural processes [1; 2; 3]. For example, forming correct memories of words requires humans to memorize not only individual letters but also the sequential order following which the letters appear (e.g., "cat" and "act"). However, despite extensive research into models of _static_, temporally unrelated memories from both neuroscience and machine learning [4; 5; 6; 7; 8; 9; 10], computational modeling of _sequential_ memory is not as developed. Existing models for sequential memory are either analytically intractable or have not yet been systematically evaluated in challenging sequential memory tasks [11; 12; 13; 14; 15], which hinders a comprehensive understanding of the computational mechanism underlying sequential memory, arguably a more general form of memory in the natural world than static memories.

In this work, we propose a novel approach to modeling sequential memory based on predictive coding (PC) [16; 17; 18], a biologically plausible neural network model able to reproduce many phenomena observed in the brain [19], which has also shown to have a close relationship to backpropagation in artificial neural networks [20; 21]. Using PC to model sequential memory is motivated by two key factors: Firstly, neuroscience experiments and theories have suggested that (temporal) predictive processing and memory are two highly related computations in the hippocampus, the brain region crucial for memory [22; 23]. Secondly, the modeling of static memories (e.g., a single image) using PC has recently demonstrated significant success [8; 10; 24; 25], raising the question of whether PC can also be employed to model sequential memory. To take into account the temporal dimension in sequential memory, in this work we adopt a temporal extension of the original PC models, which has been employed in filtering problems and in modeling the visual system [26; 27; 28; 29]. Here we investigate its performance in sequential memory tasks. Our contributions can be summarized as follows:* We propose _temporal predictive coding_ (tPC), a family of PC models capable of sequential memory tasks that inherit the biologically plausible neural implementation of classical PC models [18];
* We present an analytical result showing that the single-layer tPC can be viewed as the classical Asymmetric Hopfield Network (AHN) performing an implicit _statistical whitening_ step during memory recall, providing a possible mechanism of statistical whitening in the brain [30; 31; 32];
* Experimentally, we show that the whitening step in single-layer tPC models results in more stable performance than the AHN and its modern variants [15] in sequential memory, due to the highly variable and correlated structure of natural sequential inputs;
* We show that tPC can successfully reproduce several behavioral observations in humans, including the impact of sequence length in word memories and the primacy/recency effect;
* Beyond memory, we show that our tPC model can also develop context-dependent representations [2; 11] and generalize learned dynamics to unseen sequences, suggesting a potential connection to cognitive maps in the brain [33; 34].