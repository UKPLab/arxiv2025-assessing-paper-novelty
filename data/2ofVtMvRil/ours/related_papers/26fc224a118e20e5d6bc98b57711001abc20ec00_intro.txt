Memory systems in the brain often store information about the relationships or associations between objects or concepts. This particular type of memory, referred to as _Associative Memory_ (AM), is ubiquitous in our everyday lives. For example, we memorize the smell of a particular brand of perfume, the taste of a kind of coffee, or the voice of different singers we like. After a memory is formed, AM will support its retrieval when a related cue is presented to our senses using the learned association between the provided cues and missing components.

It has long been argued that the hippocampus and adjacent cortical areas, located in the medial temporal lobe of the brain, are crucial for AM [1, 2, 3]. Historically, various theoretical and computational works have been developed in an effort to model the hippocampus in AM tasks [4, 5, 6]. In this work, we are interested in a particular approach to modelling, which assumes that the hippocampus employs its _predictive activities_ to perform AM. Predictive processing is thought to be a key computational principle underlying various hippocampal activities: Experimentally, abundant evidence has suggested that the hippocampus is capable of predicting ongoing sensory inputs [7], whereas high-level theories and computational models have also been proposed to explain how predictive coding (PC) may support various properties of the hippocampus, including the formation of cognitive maps [8] and memory [9]. In particular, Barron et al. [9] proposed that the hippocampus sits at the top of a hierarchical generative model that generates predictions of neocortical activities based on past experiences, thus enabling retrieval of activities from memory at lower neocortical levels. The ability of predictive coding networks (PCNs) to complete previously learned patterns have been already demonstrated in pioneering work by Rao [10], although the model in this work was not formally introduced within the PC framework. More recently, the ability of the hierarchical PCNs introduced by Rao and Ballard [11] in performing AM has been computationally analysed [12, 13]. Specifically, Salvatori et al. [12] showed that hierarchical PCNs can store training data points as memories, and retrieve these memories given partial or noisy cues. Under this PC framework, which is characterized by prediction error neurons, the memorization of a sensory input is driven by the Hebbian learning dynamics [14] that minimize the error between the input and a prediction generated by the network, and the retrieval of this input is performed by theinferential neural dynamics, also to minimize the error between the internal predictions and the corrupted sensory input. It is worth mentioning that AM in this PC-based model is defined as the memorization and recall of _static_ inputs, where the associations are learned between individual components of static patterns, and the temporal dimension is ignored. In this scenario, memory retrieval is equivalent to pattern completion. On the other hand, AM can also be defined as memorizing the association along the temporal dimension, where the associations between two (or multiple) inputs along time are memorized [10]. In this work we focus on the first i.e., static type of AM.

The aforementioned hierarchical PC-based model provides a possible network mechanism that the hippocampus-neocortical entity employs to support AM, providing a possible implementation of the theory by Barron et al. [2]. However, this model only included feedforward and feedback connections between layers of neurons (representing different cortical areas), and thus failed to provide an account of how PC can also be employed within the known connectivity of the hippocampal network, where the _recurrent_ connections in subfield CA3 are thought to play a key role in AM [5]. Furthermore, the absence of recurrent connections in the hierarchical PC models for AM dissociates them from earlier recurrent models of AM such as Hopfield Network [4], which assume that the recurrent connections in the hippocampal network learns a covariance matrix representing the association between individual neurons activated by a memory item, thus hindering a unified understanding of the computational principles adopted by the hippocampus to support AM. This brings us to ask: **how can the recurrent hippocampal network employ PC to perform AM, in a biologically plausible and computationally stable manner, and can recurrent connections in such network encode the covariance of neural activity?** Here, by computationally stable, we mean the ability of the model to steadily converge to a fixed point both during learning (memory) and inference (retrieval), and the criteria for biological plausibility include [15]:

1. _Local computation_: A neuron's computation is only dependent on its input neurons and weights connecting itself to its input neurons.
2. _Local plasticity_: The plasticity rule of synapses in a model only depends on quantities encoded by pre- and post-synaptic neurons.
3. _Architectural similarity_: Components of a model resembles architectures of real neurons, such as the recurrent connections and the apical dendrites of the pyramidal neurons in the hippocampus.

The hierarchical PC models for AM are stable and satisfy the first two criteria of plausibility [12]. However, as we pointed out above, they fail to meet the third criterion, architectural similarity, due to the missing recurrent connections. In this work, we propose a family of PCNs with recurrent connections between neurons, which we call covariance-learning PCNs (covPCNs), as candidate models satisfying these criteria. In particular, we first identify that an earlier type of PCN has already incorporated recurrent synaptic connections encoding the covariance information of inputs [16, 17, 18], and can thus be considered as a PC model meeting the criterion of architectural similarity. We refer to it as the _explicit covPCN_, as it encodes the covariance matrix explicitly into its recurrent synapses. The explicit covPCN was originally proposed as a model for learning representations of sensory inputs, and we show in this work that its covariance-learning nature can be utilized to perform simple AM tasks. However, we note that the learning rule for the recurrent connections in this model is non-Hebbian, and poses significant computational issues as well, which makes it fail to satisfy the local plasticity and stability conditions. To address these issues, we propose in this work a novel recurrent PCN, which also encodes the covariance matrix via its recurrent connections, but in an implicit manner, thus we refer to it as _implicit covPCN_. We show that the new implicit model also performs AM via covariance learning, and it is equivalent to the explicit covPCN both theoretically and empirically in simple AM tasks, while only employing local Hebbian plasticity. We also show that the implicit model can be further modified to achieve biological resemblance to the hippocampal pyramidal cells by incorporating a dendritic structure, while retaining the theoretical and empirical equivalence to the explicit covPCN at convergence. We name it the _dendritic covPCN_ in this work. Importantly, we show that both the implicit and dendritic models can perform more complex AM tasks in which the explicit covPCN would fail due to its unstable dynamics. Finally, we propose a _hybrid PCN_ that combines the implicit covPCN with a hierarchical PCN [11, 12] to model the whole hippocampo-neocortical region, and show that it performs challenging AM tasks efficiently.

In this work, we describe a series of more stable and more plausible implementations of recurrent PC to model AM in the hippocampus, to shed light on how the hippocampal _structure_ (apical dendrite and recurrent network) may support its _computations_ (PC and covariance learning) underlying its _functions_ (AM). The key contribution that we bring to the table is a reparameterization of the (weighted) prediction errors in the explicit covPCN [16], which leads to simplified forms of a free energy. Crucially, the simplified forms of free energy that we consider all share the same minima or fixed points with that of the explicit covPCN, thereby leading to the same inference and learning. Practically, this allows us to drop certain terms from the gradients, leading to more robust convergence to free energy minima and, crucially, affording more biologically plausible implementation. To unpack the basic ideas of how PC can be implemented in CA3-like recurrent network to support AM, we will focus on a simple kind of PC, in which we ignore temporal predictions (i.e., predicting into the future). Rather, in our models, the prediction of one neuron's activities is from all other neurons in the recurrent network i.e., "spatial" prediction. This particular focus enables us to derive interpretable analytical results that enhance our understanding of what is encoded in the synaptic weights in our models i.e., the covariance matrix and the relationship between models. Furthermore, we will restrict our initial analyses to linear systems, under standard Gaussian parametric assumptions. In this setting, a memory is simply the ability of the PCN to recognize the most likely cause of a particular pattern of inputs (e.g., an image), which is similar to the AM tasks discussed in [12]. We are not addressing episodic memory--of the sort sometimes associated with hippocampal function--rather, we are focusing on how statistical regularities in a series of inputs are learned and then used to predict the missing or noisy part of a input. This contrasts our linear recurrent PCNs with recurrent AM models such as the Hopfield Network [4], where the memories are stored as point attractors of the network dynamics. At the end of the Results section, we provide results of an empirical analysis of the attractor behavior of our model, showing that adding nonlinearities to our model will enable it to store memories as point attractors.