Animals should build representations that afford flexible behaviours. However, different representation make some tasks easy and others hard; representing red versus white is good for understanding wines but less good for opening screw-top versus corked bottles. A central mystery in neuroscience is the relationship between tasks and their optimal representations. Resolving this requires understanding the representational principles that permit flexible behaviours such as zero-shot inference.

Here, we introduce **actionable representations**, a representation that permits flexible behaviours. Being actionable means encoding not only variables of interest, but also how the variable transforms. Actions cause many variables to transform in predictable ways. For example, actions in 2D space obey rules; north, east, south, and west, have a universal meaning, and combine in the same way everywhere. Embedding these rules into a representation of self-position permits deep inferences: having stepped north, then east, then south, an agent can infer that stepping west will lead home, having never taken that path - a zero-shot inference (Figure 1A).

Figure 1: **A** 2D space is defined by rules, e.g. at all positions north \(=-\)south. **B** Left: Entorhinal grid cells are hexagonally tuned cells (orange). Different cells within a module are translated copies (orange/blue/green). Right: Different modules have different lattice scale (pink/blue/green).

Indeed biology represents 2D space in a structured manner. Grid cells in medial entorhinal cortex represent an abstracted 'cognitive map' of 2D space (Tolman, 1948). These cells fire in a hexagonal lattice of positions (Hafting et al., 2005), (Figure 1B), and are organised in modules; cells within one module have receptive fields that are translated versions of one another, and different modules have firing lattices of different scales and orientations (Figure 1B), (Stensola et al., 2012).

Biological representations must be more than just actionable - they must be **functional**, encoding the world efficiently, and obey **biological** constraints. We formalise these three ideas - actionable, functional, and biological - and analyse the resulting optimal representations. We define _actionability_ using group and representation theory, as the requirement that each action has a corresponding matrix that linearly updates the representation; for example, the'step north' matrix updates the representation to its value one step north. _Functionally_, we want different points in space to be represented maximally differently, allowing inputs to be distinguished from one another. _Biologically_, we ensure all neurons have non-negative and bounded activity. From this constrained optimisation problem we derive optimal representations that resemble multiple modules of grid cells.

Our problem formulation allows analytic explanations for grid cell phenomena, matches experimental findings, such as the alignment of grids cells to room geometry (Stensola et al., 2015), and predicts some underappreciated aspects, such as the relative angle between modules. In sum, we 1) propose actionable neural representations to support flexible behaviours; 2) formalise the actionable constraint with group and representation theory; 3) mix actionability with biological and functional constraints to create a constrained optimisation problem; 4) analyse this problem and show that in 2D the optimal representation is a good model of grid cells, thus offering a mathematical understanding of why grid cells look the way they do; 5) provide several neural predictions; 6) highlight the generality of this normative method beyond 2D space.

### Related Work

Neuroscientists have long explained representations with normative principles like information maximisation (Attneave, 1954; Barlow et al., 1961), sparse (Olshausen & Field, 1996) or independent (Hyvarinen, 2010) latent encodings, often mixed with biological constraints such as non-negativity (Sengupta et al., 2018), energy budgets (Niven et al., 2007), or wiring minimisation (Hyvarinen et al., 2001). On the other hand, deep learning learns task optimised representations. A host of representation-learning principles have been considered (Bengio et al., 2013); but our work is most related to geometric deep learning (Bronstein et al., 2021) which emphasises input transformations, and building neural networks which respect (equivariant) or ignore (invariant) them. This is similar in spirit but not in detail to our approach, since equivariant networks do not build representations in which all transformations of the input are implementable through matrices. Most related are Paccanaro & Hinton (2001), who built representations in which relations (e.g. \(\mathbf{x}\) is the father of \(\mathbf{y}\)) are enacted by a corresponding linear transform, exactly like our actionable!

There is much previous theory on grid cells, which can be categorised as relating to our actionable, functional, and biological constraints. **Functional:** Many works argue that grid cells provide an efficient representation of position, that hexagons are optimal (Mathis et al., 2012;a;b; Sreenivasan & Fiete, 2011; Wei et al., 2015) and make predictions for relative module lengthscales (Wei et al., 2015). Since we use similar functional principles, we suspect that some of our novel results, such as grid-to-room alignment, could have been derived by these authors. However, in contrast to our work, these authors assume a grid-like tuning curve. Instead we give a normative explanation of why be grid-like at all, explaining features like the alignment of grid axes within a module, which are detrimental from a pure decoding view (Stemmler et al., 2015). **Actionability:** Grid cells are thought to a basis for predicting future outcomes (Stachenfeld et al., 2017; Yu et al., 2020), and have been classically understood as affording path-integration (integrating velocity to predict position) with units from both hand-tuned Burak & Fiete (2009) and trained recurrent neural network resembling grid cells (Cueva & Wei, 2018; Banino et al., 2018; Sorscher et al., 2019). Recently, these recurrent network approaches have been questioned for their parameter dependence (Schaeffer et al., 2022), or relying on decoding place cells with bespoke shapes that are not observed experimentally (Sorscher et al., 2019; Dordek et al., 2016). Our mathematical formalisation of path-integration, combined with biological and functional constraints, provides clarity on this issue. Our approach is linear, in that actions update the representation linearly, which has previously been explored theoretically (Issa & Zhang, 2012), and numerically, in two works that learnt grid cells (Whittington et al., 2020; Gaoet al., 2021). Our work could be seen as extracting and simplifying the key ideas from these papers that make hexagonal grids optimal (see Appendix H), and extending them to multiple modules, something both papers had to hard code. **Biological:** Lastly, both theoretically (Sorscher et al., 2019) and computationally (Dordek et al., 2016; Whittington et al., 2021), non-negativity has played a key role in normative derivations of hexagonal grid cells, as it will here.