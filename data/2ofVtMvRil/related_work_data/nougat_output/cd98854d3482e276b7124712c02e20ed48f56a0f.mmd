bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.12.571268](https://doi.org/10.1101/2023.12.12.571268); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.

###### Abstract

Advances in generative models have recently revolutionised machine learning. Meanwhile, in neuroscience, generative models have long been thought fundamental to animal intelligence. Understanding the biological mechanisms that support these processes promises to shed light on the relationship between biological and artificial intelligence. In animals, the hippocampal formation is thought to learn and use a generative model to support its role in spatial and non-spatial memory. Here we introduce a biologically plausible model of the hippocampal formation tantamount to a Helmholtz machine that we apply to a temporal stream of inputs. A novel component of our model is that fast theta-band oscillations (5-10 Hz) gate the direction of information flow throughout the network, training it akin to a high-frequency wake-sleep algorithm. Our model accurately infers the latent state of high-dimensional sensory environments and generates realistic sensory predictions. Furthermore, it can learn to path integrate by developing a ring attractor connectivity structure matching previous theoretical proposals and flexibly transfer this structure between environments. Whereas many models trade-off biological plausibility with generality, our model captures a variety of hippocampal cognitive functions under one biologically plausible local learning rule.

## 1 Introduction

Generative models seek to create new data samples which are similar to those from the training set. To do so they must learn the probability distribution of the training data, comprising a rich, generalisable and accurate model of the world. Many of the recent advances in AI have involved types of generative models: VAEs [1], GANs [2], diffusion models [3] and autoregressive models [4] have seeded improvements in AI capabilities ranging from data compression [5] to image generation [6] and natural language [7]. In neuroscience, the animal brain has long been known to exploit generative models [8; 9]. The ability to generate representative sensory data samples can be used directly, for example during offline planning or memory recall. It can also be used indirectly to aid training of inference networks with the goal of processing rich, noisy and high dimensional streams of incoming sensory stimuli, as discussed in the predictive coding literature [10]. In a sentence: "What I cannot create [generate], I do not understand [inference]" (R. Feynman).

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571268](https://doi.org/10.1101/2023.12.2571268); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aC-BY-NC-ND 4.0 International license.

The hippocampal-entorhinal system (aka. hippocampal formation) - a brain structure implicated in spatial [11] and non-spatial [12] memory - provides a pertinent example. Its primary role seems to be inference [13]: mapping sensory inputs into a robust and decodable representation of state (grid cells [14], place cells [11] etc. [15]). A generative model is thought to have a dual role in learning: supporting offline tasks such as route planning [16] and memory consolidation [17], and online during behaviour with path integration [18]. Path integration enables the hippocampal network to maintain an up-to-date and accurate estimate of its position in the absence of reliable sensory data by integrating self-motion cues. A recent flurry of computational [19, 20, 21] and theoretical [22, 21] work has highlighted the importance of path integration as a key objective explaining hippocampal function and representations.

Existing computational generative models of the hippocampal formation [23, 24] account for many of its cognitive functions and internal representations but require non-trivial learning rules and message passing protocols which don't connect with known aspects of biology. Computational models of path integration [25, 26, 27] have mostly focussed on continuous attractor networks which, although experimentally supported [28], alone lack the complexity or expressivity required of a fully general model of the hippocampal memory system.

The primary contribution of this paper is to introduce a biologically plausible model of sequence learning in the hippocampus which unifies its capacities as a generative model of sensory stimuli and path integration under one schema. To do this we propose modeling the hippocampal formation as a Helmholtz machine [29] which learns to predict sensory stimuli given the current hidden state and action (e.g. velocity). We propose a deep connection between the hippocampal theta oscillation [30] and the unsupervised wake-sleep algorithm [31] for training Helmholtz machines. Though this class of generative models isn't widely used, and lacks the scalability of the lastest transformer-based sequence learners, it excels in this context since is has many natural points of contact with biology (both in terms of architecture and neural dynamics) yet still maintains the expressiveness afforded to models of the brain by deep neural networks.

In this paper we:

* introduce a new model of the hippocampal formation which learns the latent structure of an incoming stream of sensory stimuli analogous to a Helmholtz machine.
* describe a biologically plausible learning regime: Theta-oscillations gate information flow through multi-compartmental neurons which rapidly switches the system between "wake" and "sleep" phases. All plasticity is local.

Figure 1: A biologically plausible generative model is trained with theta frequency wake-sleep cycles and a local learning rule. **a** Network schematic: high-D stimuli from an underlying environmental latent, \(z\), arrive at the basal dendrites of the sensory layer, \(p\), and map to the hidden layer, \(g\) (this is the inference model, weights in green). Simultaneously, top-down predictions from the hidden layer \(g\) arrive at the apical dendrites of \(p\) (this is the generative model, weights in blue). **b** Neurons in layers \(p\) and \(g\) have three compartments. A fast oscillation, \(\theta(t)\), gates which dendritic compartment – basal (\(p_{B}\), \(g_{B}\)) or apical (\(p_{A}\), \(g_{A}\)) – drives the soma. A local learning rule adjusts input weights to minimise the prediction error between dendritic compartments and the soma. **c** This equates to rapidly switching “wake” and “sleep” cycles which train the generative and inference models. Panel **c** displays just two updates per theta-cycle, in reality there are many (\(\delta t<<T_{\theta}\)).

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571268](https://doi.org/10.1101/2023.12.2571268); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under ACC-BY-NC-ND 4.0 International license.
* train our model on stimuli from a biologically relevant spatial exploration task and show it learns to path integrate by developing a ring attractor connectivity structure (comparable to theoretical predictions and empirical results in deep recurrent neural networks trained with gradient descent). Learning generalises: when the agent moves to a new environment, path integration capabilities recover without needing to relearn the path integration weights.

Our model of the hippocampal formation simultaneously (i) accounts for its role as a generative model of sensory stimuli, (ii) can learn to path integrate and (iii) can transfer structural knowledge between environments. The model, though here applied to the hippocampus, can be viewed as a step towards a general solution for how biological neural networks in many brain regions (for example visual cortex [10]) can learn generative models of the world.1

Footnote 1: Code provided at [https://github.com/TomGeorge1234/HelmholtzHippocampus](https://github.com/TomGeorge1234/HelmholtzHippocampus)

### Related work

A recent generative model of the hippocampus, the Tolman-Eichenbaum Machine [23], proposed that the hippocampal formation be thought of as a hierarchical network performing latent state inference. Medial entorhinal cortex (MEC) sits atop the hierarchy and learns an abstract representation of space which is mapped to the hippocampus (HPC) where it is bound onto incoming sensory stimuli. Once trained the system can act in a generative fashion by updating the hidden representation with idiothetic action signals and then predicting the upcoming sensory experience. The drawback of this model, and others which share a similar philosophical approach [32; 24], is that it requires training via backpropagation through time (or equivalent end-to-end optimisation schemes, as in [24]) without clear biological correlates. Related hierarchical network architectures have also been studied in the context of reinforcement learning [33] and hippocampal associative memory [34].

Historically, hippocampal models of path integration have focused on continuous attractor networks (CANs) [25; 26; 27; 21] in entorhinal cortex. A bump of activity representing location is pushed around the CAN by speed and/or head-direction selective inputs, thus integrating self-motion. CANs have received substantial experimental support [28] but few studies adequately account for _how_ this structure is learned by the brain in the first place. One exception exists outside the hippocampal literature: Vafidis et al. [35] built a model of path integration in the fly head-direction system which uses local learning rules. Here we go further by embedding our path integrator inside a hierarchical generative model. Doing so additionally relaxes the assumption (made by Vafidis et al. [35] and others [36]) that sensory inputs into the path integrator are predefined and fixed. Instead, by allowing all incoming and outgoing synapses to be learned from random initialisations, we achieve a more generalisable model capable of transferring structure between environments (see section 3.3).

Hippocampal theta oscillations have been linked to predictive sequence learning before [37; 38; 39; 40] where research has focused on the compressive effects of theta _sequences_ and how these interplay with short timescale synaptic plasticity. Instead of compression, here we hypothesize the role of theta is to control the direction information flows through the hierarchical network.

Finally, a recent theoretical work by Bredenberg et al. [41] derived, starting from principles of Bayesian variational inference, a biologically plausible learning algorithm for approximate Bayesian inference of a hierarchical network model built from multi-compartmental neurons and trained with local learning rules using wake-sleep cycles. Here we build a similar network to theirs (i) extending it to a spatial exploration task and mapping the hidden layers onto those in the hippocampal formation, (ii) simplifying the learning rules and relaxing a discrete-time assumption - instead, opting for a temporally continuous formulation more applicable to biological tasks such as navigation - and (iii) adapting the hidden layer to allow idiothetic action signals to guide updates (aka. path integration). Their work provides a theoretical foundation for our own, helping to explaining _why_ learning converges on accurate generative models.

A biologically plausible generative model trained with rapidly switching wake-sleep cycles and local learning rules

In sections 2 and 3 we give concise, intuitive descriptions of the model and experiments; expanded details can be found in the supplementary material.

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571268](https://doi.org/10.1101/2023.12.2571268); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under ACC-BY-NC-ND 4.0 International license.

### Basic model summary

We consider learning in an environment defined by a latent state, \(z(t)\), which updates according to stochastic dynamics initially unknown to the network,

\[\frac{dz}{dt}=f_{z}(t). \tag{1}\]

These dynamics depends on the task; first we consider \(z(t)\) to be a set of mutually independent random variables and later we consider the more realistic task of an agent moving on a 1D track.

The network receives sensory input which is a function of the latent state into a sensory layer, \(\mathbf{p}(t)\), and communicates this to a hidden layer (aka "internal state"), \(\mathbf{g}(t)\). The network contains both an _inference_ (aka. _recognition_) model which infers the hidden state from the sensory input (green arrows, Fig. 1a) and a _generative_ model which updates the hidden state with recurrent synapses and maps this back to the sensory layer (blue arrows). As we will soon identify these processes with Basal and Apical dendritic compartments of pyramidal neurons we label activations sampled from the inference model with the subscript \(B\) and those from the generative model with the subscript \(A\). 2 In summary

Footnote 2: These labellings conveniently match the notion that inferences are made from layers Below in the sensory hierarchy (bottom-up) whereas generative predictions arrive from Above (top-down).

\[\mathbf{p}_{B}(t+\delta t) =\bar{\mathbf{p}}(z(t))\] \[\mathbf{g}_{B}(t+\delta t) =\sigma_{g_{B}}(\mathbf{w}_{g_{B}}\mathbf{p}(t)) \tag{2}\] \[\mathbf{g}_{A}(t+\delta t) =\sigma_{g_{A}}(\mathbf{w}_{g_{A}}\mathbf{g}(t))\] \[\mathbf{p}_{A}(t+\delta t) =\sigma_{g_{A}}(\mathbf{w}_{p_{A}}\mathbf{g}(t)) \tag{3}\]

\(\mathbf{w}_{g_{B}},\mathbf{w}_{p_{A}},\mathbf{w}_{g_{B}}\) are matrices of randomly initialised and plastic synaptic weights. \(\bar{\mathbf{p}}\) maps the environmental latent into a vector of neural inputs. \(\sigma\)'s denote activation functions applied to the dendritic pre-activations - either the identity (\(\sigma(x)=x\)) or rectified tanh functions (\(\sigma(x)=\max(0,tanh(x))\)). A small amount of noise is added to the dendritic activations to simulate realistic biological learning.

We believe that the widely adopted convention of modelling neurons as single-compartment perceptrons is limiting. By considering, in a minimal extension, the distributed dendritic structure of real neurons we can tap into significant potential for explaining hippocampal learning. Theoretical [42; 43; 44; 45] and experimental [46; 47; 48] research into credit assignment in biological neurons has identified different roles for basal and apical dendrites: basal dendrites are thought to receive bottom-up drive from sensory inputs whereas apical dendrites receive top-down drive from higher layers in the sensory hierarchy [49]. Following this line of research -- and matching an equivalent theoretical model of latent state inference described by [41] -- we identify the inference process with synaptic inputs into a basal dendritic compartment of pyramidal neurons and the generative process with synaptic inputs into an apical dendritic compartment. In summary, each \(\mathbf{p}\) and \(\mathbf{g}\) neuron in our model has three compartments: a somatic compartment, a basal dendritic compartment and an apical dendritic compartment (Fig. 1b). Only the somatic activation is used for communication between layers (right hand side of Eqns. (2) and (3)) while dendritic compartment activations are variables affecting internal neuronal dynamics and learning as described below (Eqns. (4) and (6)).

### Theta oscillations gate the direction of information flow through the network

The dynamics of the somatic activations \(\mathbf{p}(t)\) and \(\mathbf{g}(t)\) are as follows: the voltage in each soma is either equal to the voltage in the basal compartment _or_ the voltage in the apical compartment depending on the phase of an underlying theta oscillation. This is achieved by a simple theta-gating mechanism (Fig. 1b):

\[\mathbf{p}(t) =\theta(t)\mathbf{p}_{B}(t)+(1-\theta(t))\mathbf{p}_{A}(t)\] \[\mathbf{g}(t) =\theta(t)\mathbf{g}_{B}(t)+(1-\theta(t))\mathbf{g}_{A}(t). \tag{4}\]

where \(\theta(t)\) is a 5 Hz global theta oscillation variable defined by the square wave function:

\[\theta(t)=\begin{cases}1,&\text{if }t/T\mod 1\leq 0.5\\ 0,&\text{if }t/T\mod 1>0.5\end{cases} \tag{5}\]for \(T=1/f_{\theta}\) and \(f_{\theta}=5\) Hz, matching the hippocampal theta frequency (5-10 Hz) [50]. According to this model theta-band oscillations in the hippocampal local field potential gate which dendritic compartment drives the soma. Experimental [47; 51; 52] and modelling work [53] gives provisional support for this assumption.

These local theta-dynamics have global consequences: the early phase (\(\theta(t)=1\)) of each theta cycle can be thought of as a "wake" phase where information flows upwards through the network from the environment to the hidden layer, sampling the inference model. The latter phase (\(\theta(t)=0\)) of each theta cycle is a "sleep" phase where information flows down from the hidden layer to the sensory units, sampling the generative model. These dynamics are displayed in Fig. 1.

### Hebbian-style learning rules train synapses to minimise local prediction errors

In contrast to comparable models which are optimised end-to-end using backpropagation through time our model learns synaptic weights according to a local plasticity rule which is a simplified variant of a rule proposed by Urbanczik and Senn [43]. Incoming synaptic projections are continually adjusted in order to minimize the discrepancy between the somatic activation and the dendritic activation. The full learning rules are described in the supplement but simplified versions are given here:

\[\frac{d\mathbf{w}_{g_{R}}}{dt} \propto(\mathbf{g}(t)-\mathbf{g}_{B}(t))\mathbf{p}(t)^{\mathsf{ T}}\] \[\frac{d\mathbf{w}_{pA}}{dt} \propto(\mathbf{p}(t)-\mathbf{p}_{A}(t))\mathbf{g}(t)^{\mathsf{ T}}\] \[\frac{d\mathbf{w}_{g_{A}}}{dt} \propto(\mathbf{g}(t)-\mathbf{g}_{A}(t))\mathbf{g}(t)^{\mathsf{ T}} \tag{6}\]

Notably this learning rule is equivalent for _all_ plastic synapses in the model: \(\mathbf{p}\) to \(\mathbf{g}\), \(\mathbf{g}\) to \(\mathbf{p}\) and the recurrent \(\mathbf{g}\) to \(\mathbf{g}\) synapses (see Fig. 1b). If a local prediction error is detected, for example the somatic activation is larger than the dendritic activation, then the synaptic strength of inputs into that dendritic compartment which are positive/negative are strengthened/weakened to reduce the error. This model can equivalently be viewed as a type of Hebbian learning - weight change is proportional to the correlation of pre- and post-synaptic activity (the first term) - regularised (by the second term) to prevent unbounded growth.

During the wake phase the weights of the generative model (\(\mathbf{w}_{pA}\) and \(\mathbf{w}_{g_{A}}\)) are trained and plasticity on the inference weights (\(\mathbf{w}_{g_{R}}\)) falls to zero. This occurs naturally because \(\mathbf{p}=\mathbf{p}_{B}\) so there will be no basal prediction errors to correct. During sleep the reverse occurs; the weights of the inference model are trained and plasticity on the generative model falls to zero. Experimentally, apical activity is known to guide plasticity at basal synapses in CA1 [46]. This alternating, coordinated regime of sampling and learning (sample-inference-train-generative, then sample-generative-train-inference) is a hallmark of the wake-sleep algorithm. It fundamentally differs from the forward and backward sweeps of backpropagation since neurons remain provisionally active at all times so the process of learning minimally perturbs perception. Also, whereas backpropagation sends error signals down through the network to train synaptic weights, here only predictions are sent between layers and error signals are calculated locally at each dendrite.

As discussed in section 1, Bredenberg et al. [41] mathematically derive learning rules similar to these starting from a loss function closely related to the evidence lower bound (ELBO). As such our identification of early- and late-theta phases as "wake" and "sleep" cycles can be considered precise: from a Bayesian perspective our hippocampal model is minimising a modified ELBO loss (see supplement) thus learns to find approximately optimal inference and generative models accounting from the temporally varying stimulus stream it is presented.

### Velocity inputs into the hidden layer

For path integration, the hidden state needs access to an idiothetic (internally generated) velocity signal. To satisfy this we endow the hidden layer, \(\mathbf{g}\), with conjunctive velocity inputs, henceforth "conjunctive cells", as shown in Fig. 3a & b. Conjunctive cells are organised into two groups: \(\mathbf{g}_{v_{L}}\) is responsible for leftward motion and \(\mathbf{g}_{v_{R}}\) for rightward motion. Each conjunctive cell receives input from the hidden units and either the leftward (\(v_{L}=\max(0,-\dot{x})\)) or rightward (\(v_{R}=\max(0,\dot{x})\)) component of the velocity. For the results shown this connectivity is one-to-one \([\mathbf{w}_{g_{R}}]_{ij}=[\mathbf{w}_{g_{R}}]_{ij}=\delta_{ij}\) but random connectivity works too, see supplement. Finally, conjunctive cells send return connections back to the apical dendritic compartment of the hidden units via a randomly initialised plastic synaptic weight matrix. This inputs are what drive the hidden units to path integrate.

This model takes inspiration from so-called conjunctive grid cells [54] found in the medial entorhinal cortex (MEC). These cells, though to be an integral component of the mammilian path integration system[27], are jointly tuned to head direction and location much like the conjunctive cells in our model. An important and novel aspect of our model is that synaptic weights between or into the hidden units are _learned_. This deviates from other models for example that by Burak and Fiete [27] (where all connectivity is predefined and fixed) or Vafidis et al. [35] and Widloski and Fiete [36] (where sensory inputs to the hidden units are pre-defined and fixed). This is not only more realistic but affords the model flexibility to translate path integration abilities between environments without having to relearn them, a form of transfer learning which we demonstrate in section 3.3.

## 3 Results

### Validation on an artifical latent learning task

We begin by testing the basic model (i.e. without conjunctive inputs, Fig. 1a) on an artificial task. \(N_{z}=5\) latents, \(z_{i}(t)\), are independently sampled from a smooth, random process with an autocorrelation timescale of 1 second (Fig. 2a). The sensory layer, \(N_{p}=50\), then receives a high-dimensional random linear mixture of the latents into the basal compartments:

\[\mathbf{p}_{B}(t)=\mathbf{A}\mathbf{z}(t), \tag{7}\]

where \(\mathbf{A}\in\mathbb{R}^{50\times 5}\) and \([\mathbf{A}]_{ij}\sim\mathcal{N}(0,\frac{1}{\sqrt{N_{z}}})\). The hidden layer, \(\mathbf{g}(t)\), is matched in size to the latent process, \(N_{g}=N_{z}=5\), and all dendritic activation functions are linear. We train the model for 30 minutes of simulated time and track prediction errors, the difference between the basal and apical activations in the sensory and hidden layers, which reliably decreased throughout training (Fig. 2b). We then perform two tests designed to confirm whether the model has learnt accurate inference and generative models.

Figure 2: Learning in an environment of temporally varying latents. **a** In this artifical task the latent space comprises of \(N_{z}=5\) independent random variables with an autocorrelation decay timescale of 1 s. **b** Prediction errors (difference between apical and basal activations) in sensory and hidden layers reduce over training time. **c** Tested in wake mode (\(\theta=1\)) after training, the ground truth stimulus matches apical prediction for all stimulus dimensions (one shown) implying the network is efficiently “autoencoding” the sensory inputs into and back out of the compressed hidden layer. **d** Tested in sleep mode (\(\theta=0\), no environmental inputs), generated data from the hidden units, \(g\), have an autocorrelation curve which matches that of the true latents implying a statistically accurate generative model has been learned. More extensive samples from this model, before and after training, can be found in Fig. S1

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571268](https://doi.org/10.1101/2023.12.2571268); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.

First, we set the dynamics of the model to "wake" mode (\(\theta=1\)) and measure the basal and apical activations of one of the sensory neurons for 60 seconds. Close correspondence (Fig. 2c) confirms that the network accurately "autoencodes" the high-dimensional sensory inputs through the compressed hidden layer. Since all activation functions are linear this implies that \(\mathbf{w}_{g_{B}}\) and \(\mathbf{w}_{p_{A}}\) are pseudoinverses. Next, we place the network in "sleep" mode (\(\theta=0\)) and allow the generative model to run freely. The autocorrelation of the generated hidden states (\(\mathbf{g}(t|\theta=0)\), displayed fully in the supplement) match that of the true environmental latents (\(\mathbf{z}(t)\)), Fig. 2d, implying the generative model has statistics closely matching those of the true underlying generative process.

### Learnable path integration with a hidden ring attractor

Next we turn our attention to the hippocampal formation's role in spatial navigation, and our central result. The environment consists of an agent randomly moving around a 1 m 1D circular track (motion and cell data is generated using the RatInABox package [55]). The basal compartment of each HPC neuron is spatially tuned to a single different Gaussian input however non-Gaussian

Figure 3: The hippocampal model learns to path integrate on a 1D track using a ring attractor. **a** Position selective (place cell) inputs drive basal dendrites of the sensory layer **p** (HPC). **b** Hidden units (MEC) are connected to two sets of “conjunctive cells” which each connect back to one of the hidden neurons (**g**) and either the leftward (for \(\mathbf{g}_{e_{L}}\)) or rightward (for \(\mathbf{g}_{e_{L}}\)) velocity of the agent allowing velocity information to enter the network. Synaptic strengths of the return connections from the conjunctive cells to the MEC hidden units, as well as those for the MEC recurrent connectivity (collective denoted \(\mathbf{w}_{g_{A}}\)), are randomly initialised and plastic. **c** After training, reordering the hidden units by the position of peak activity reveals a ring attractor in the synaptic weight matrices. Centre-surround recurrent connectivity stabilises an activity bump which is then “pushed” around the attractor manifold by asymmetric connections from the conjunctive cells, integrating velocity. Bands of zero weights show MEC neurons which have become perpetually inactive (aka “died”). The bottom panel displays the matrix row-averages, utilizing the circular symmetry of the environment to align rows before averaging. **d** Learning plateaus after 15 mins of simulated time. **e** Path integration ability is demonstrated in a lesion study: after 10 seconds in the normal oscillatory mode the network is placed into sleep mode (aka generative mode), lesioning the position-dependent sensory inputs. Despite this HPC continues to accurately encode position, evidence that the MEC ring attractor is path integrating the velocity inputs and sending predictions back to HPC. Lower panel shows the accumulated decoding error as well as the mean\(\pm\)SEM over 50 trials.

randomly spatially tuned inputs work as well (see supplement Fig. S2b):

\[[\mathbf{p}_{B}(t)]_{i}=\exp\bigg{[}-\frac{(x(t)-x_{i})}{2\sigma^{2}}\bigg{]}. \tag{8}\]

\(x(t)\) is the position of the agent and \(\{x_{i}\}_{i=1}^{N_{p}}\) are the centres of the Gaussian inputs (\(\sigma=6\) cm), intended to simulate hippocampal place fields, evenly spaced at 1 cm intervals along the track. MEC (i.e. the hidden layer, \(\mathbf{g}(t)\)) is matched in size \(N_{g}=N_{p}=100\) with rectified land activation functions on both dendritic compartments (\(\sigma_{g_{B}}(x)=\sigma_{g_{A}}(x)=\max(0,\tanh(x))\)) and HPC (the sensory layer \(\mathbf{p}(t)\)) is linear (\(\sigma_{p_{A}}(x)=x\)). Two populations of conjunctive cells (Fig. 3a & b) feed into the apical compartments of the MEC recurrent units. Random initialisation of \(\mathbf{w}_{g_{B}}\) means that MEC neurons start off with random non-Gaussian spatial tunings. \(\mathbf{w}_{g_{A}}\) and \(\mathbf{w}_{p_{A}}\) are also randomly initialised.

The network is trained for 30 minutes with learning plateauing after 15 (Fig. 3d). A lesion study, designed to test path integration, is then performed as follows: First, the network is run for 10 seconds normally (i.e. with theta-oscillating periods of wake and sleep). Since the simulated HPC neurons receive place-tuned inputs uniformly ordered along the track (i.e. \(x_{j}>x_{i}\forall i,j>i\)) an activity heatmap of HPC reveals a bump of activity accurately tracking agent's position (Fig. 3e, left). The network is then placed into a sleep phase (\(\theta=0\)) for 20 seconds. This amounts to a full sensory lesion since top-down MEC inputs, not bottom-up place-tuned sensory inputs, drive HPC. Despite the full sensory lesion, hippocampal activity remains approximately unperturbed and the activity bump continues to accurately track position, slowly accumulating errors (Fig. 3e right). Since our HPC layer has no recurrent connectivity it cannot support this post-lesion activity on its own. Instead feed-forward drive from an MEC ring attractor, which we turn our attention to now, is responsible for maintaining the HPC code.

To find the ring attractor we must first reorder the MEC cells. We do this according to the position of the peak of their receptive fields (defined in the supplement). After reordering, the recurrent connectivity matrix can be seen to have acquired a centre-surround connectivity profile. Nearby MEC cells were, on average, strongly and positively recurrently connected to one another. Those far apart weakly inhibit one another (Fig. 3c, left; band of strong positive weights along diagonal flanked by weak negative weights). This profile matches that of a quasi-continuous ring attractor: local excitatory and long-range inhibitory connections stabilise a bump of activity on the attractor manifold in the absence of sensory input [56]. Weights from the conjunctive cells acquired asymmetric connectivity (Fig. 3c, middle & right) skewed towards the velocity direction for which they are selective. These asymmetric connections enable conjunctive cells to "push" the activity bump around the manifold, integrating velocity (see supplement for a visualisation of the MEC bump attractor). Theoretical work on ring attractors has demonstrated that for accurate path integration the asymmetric weights must be proportional to the derivative of the symmetric weights [56], approximately observed here. A noteworthy observation is that some MEC neurons become perpetually inactive; this is a consequence of the fact that _both_ top-down and bottom-up synapses into the hidden layer are plastic and can fall to zero (Fig. 3c bands of zero-weights) satisfying a trivial \(g_{A}=g_{B}=0\) solution for minimising the prediction error. Despite this, not all MEC neurons die and the surviving subset are sufficient for path integration. In supplementary section 5.4.2 we discuss additional results showing when the network learns robust path integrate under a variety of plasticity, initialisation and noise manipulations.

Crucially, what sets this model apart from others [19; 20; 21; 22] is that the network is not optimized using a conventional path-integration objective and backpropagation. Instead, it has been demonstrated how path integration can naturally arise in a biologically constrained network subject to a much simpler (yet more broadly applicable) local objective, in cases where idiothetic velocity signals are available to the hidden layers.

### Remapping: transfer of structural knowledge between environments

Finally, we demonstrate how our trained network can transfer structural knowledge - which here means the ring attractor and thereby path integration - between environments. We start by training the network as in section 3.2; the only difference is that for simplicity we choose to fix \(\mathbf{w}_{g_{B}}=\delta_{ij}\) giving rise to MEC representations which, like HPC, are unimodal (this constraint can be relaxed and, in the more general case, MEC units typically have multiple receptive fields, Fig S4d, reminiscent of grid cells). We then simulate a hippocampal "remapping" event by shuffling the sensory inputs to the HPC layer (Fig. 4a & b, top panel) and retraining the network for a further 30 minutes but this time holding weights in the hidden layer, \(\mathbf{w}_{g_{A}}\). Only the HPC \(\leftrightarrow\) MEC synapses (\(\mathbf{w}_{g_{B}}\) & \(\mathbf{w}_{p_{A}}\)) remain plastic during retraining. Biologically this may be accounted for by the observation that cortical plasticity is substantially slower than hippocampal plasticity [57].

During biological remapping events place cells remap independently whereas grid cells remap _en masse_ with entire modules shifting by the same constant phase [58]. This observation is reproduced in our model: after retraining MEC units regroup with receptive fields as they were before remapping but with a constant phase shift along the track. This re-emergence of structure occurs because the ring attractor seeds a bump of activity on the attractor manifold (during the "sleep" phases of retraining) onto which the shuffled HPC inputs then bind. Since nothing constrains _where_ on the circularly symmetric attractor manifold this regrouping can initiate, only relative correlations, modulo a phase shift, are preserved.

Decoding error one second after a sensory lesion is tested just _before_ remapping, just _after_ remapping and after retraining (Fig. 4c). After the remapping path integration abilities temporarily disappear because the MEC ring attractor is still tuned to the old and invalid HPC receptive fields. After relearning - and despite _no adjustments to the MEC weights_, \(\mathbf{w}_{g_{A}}\), _where the ring attractor is stored_ - path integration recovers to almost the level before remapping. This differs substantially from other local models of path integration learning [35; 36] which don't consider plasticity on the ring attractor inputs. In these models, adaptation to a new environment necessarily requires complete relearning of the ring attractor. Instead our model exploits the basic fact that movement (path integration) in one environment is fundamentally the same as in another, one must simply learn a new mapping to/from the ring attractor, "translating" it to fit the new sensory stimuli.

## 4 Discussion

We propose that the hippocampal formation resembles a Helmholtz machine, simultaneously learning an inference and generative model of sensory stimuli. Like previous models [23] medial entorhinal

Figure 4: Remapping and transfer of structural knowledge between environments. **a** After training (as in Fig. 2) place cell inputs are shuffled to simulate a “remapping” event observed when an agent moves to a new environment. The agent then retrains for an additional 30 minutes: during this period internal MEC weights, and weights from the conjuctive cells to MEC are held fixed while MEC \(\leftrightarrow\) HPC weights remain plastic. **b** Recptive fields of the HPC and MEC neuronal populations at different stages in the experiment: Initially after remapping HPC and MEC inputs are randomised. MEC relearns rate maps as they were before remapping but with a constant phase shift. Note: neurons are ordered by the position of their peak activity on the track _before_ remapping and this ordering is maintained in subsequent panels. **c** The error (\(\pm\) SEM over 50 trials) after 1 second of path integration is shown at different stages of the experiment. Although path integration is initially disrupted after remapping it recovers despite no relearning of the MEC synapses where the ring attractor is stored.

-corex (MEC) sits hierarchically above the hippocampus (HPC) to which it sends generative predictions. Our model differs in the learning rules and neural dynamics: local prediction errors are minimised between distinct dendritic compartments receiving bottom-up and top-down signals. Theta oscillations regulate internal neural dynamics, switching the network between wake and sleep phases. In a navigation task our MEC model forms a ring attractor capable of path integration. Despite simple learning rules and dynamics our model retains key cognitive capabilities of the hippocampal formation including the ability to transfer knowledge across different sensory environments.

Local learning rules are commonly recognised as essential in biologically plausible learning algorithms [43]. However, the importance of learning _scheduling_ - how neural systems coordinate or multiplex distinct phases of forward and backward information flow - is often overlooked[59]. Neural oscillations such as theta, hypothesized to temporally coordinate communication between neuronal populations [60], likely play an underexplored role in this regard (neural "bursting" has also been pointed out as a potential solution to multiplexing [61]). One advantage of the wake-sleep algorithm, which this study suggests neural oscillations can support, compared to forward and backward sweeps is that, during convergence, the two phases become highly similar, allowing learning to proceed without affecting perception.

While our discussion has primarily focused on theta oscillations as a mechanism for learning, they have also been proposed as a mechanism for short-range future prediction via so-called "mind-travel"[62]. During the latter phase of each theta cycle (i.e. the sleep phase) gain amplified velocity signals might rapidly drive the MEC activity bump along the manifold allowing the agent to assess nearby upcoming locations. This complimentary proposition could neatly integrate into the framework proposed here and emphasizes the need for further investigation into the multifaceted functions of neural rhythms within the hippocampal/entorhinal system.

Beyond theta oscillations, both faster gamma cycles [63] and the slower physiological states of sleep and wake [64] have been associated with learning. Based on our model we suggest a tentative hypothesis that theta oscillations may be favored due to an optimality criterion; whilst faster oscillations could be a mechanism to prevent extreme drift during sleep that might disrupt learning their frequency might by upper bounded biophysically by the neural time constants associated with the biophysical processes supporting dendritic gating the soma. These ideas, their relevance to other brain regions involved in generative learning, 2D spatial dynamics, and offline memory consolidation/replay remain exciting questions for future theoretical and experimental investigation.

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571286](https://doi.org/10.1101/2023.12.2571286); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.

## References

* Kingma and Welling [2022] Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022.
* Goodfellow et al. [2014] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.
* Sohl-Dickstein et al. [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 2256-2265, Lille, France, 07-09 Jul 2015. PMLR. URL [https://proceedings.mlr.press/v37/sohl-dickstein15.html](https://proceedings.mlr.press/v37/sohl-dickstein15.html).
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _CoRR_, abs/1706.03762, 2017. URL [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762).
* George and Li [2019] Tom M George and Pietro Liq. Unsupervised machine learning for data encoding applied to ovarian cancer transcriptomes. November 2019. doi: 10.1101/855593. URL [https://doi.org/10.1101/855593](https://doi.org/10.1101/855593).
* Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. _CoRR_, abs/2102.12092, 2021. URL [https://arxiv.org/abs/2102.12092](https://arxiv.org/abs/2102.12092).
* Bubeck et al. [2023] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.
* Friston [2010] Karl Friston. The free-energy principle: a unified brain theory? _Nature Reviews Neuroscience_, 11(2):127-138, January 2010. doi: 10.1038/nrn2787. URL [https://doi.org/10.1038/nrn2787](https://doi.org/10.1038/nrn2787).
* Gershman [2019] Samuel J. Gershman. The generative adversarial brain. _Frontiers in Artificial Intelligence_, 2, September 2019. doi: 10.3389/frai.2019.00018. URL [https://doi.org/10.3389/frai.2019.00018](https://doi.org/10.3389/frai.2019.00018).
* Rao and Ballard [1999] Rajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. _Nature Neuroscience_, 2(1):79-87, January 1999. doi: 10.1038/4580. URL [https://doi.org/10.1038/4580](https://doi.org/10.1038/4580).
* Okeefe [1976] John Okeefe. Place units in the hippocampus of the freely moving rat. _Experimental Neurology_, 51(1):78-109, January 1976. doi: 10.1016/0014-4886(76)90055-8. URL [https://doi.org/10.1016/0014-4886](https://doi.org/10.1016/0014-4886)(76)90055-8.
* Squire [1992] Larry R. Squire. "memory and the hippocampus: A synthesis from findings with rats, monkeys, and humans": Correction. _Psychological Review_, 99(3):582-582, July 1992. doi: 10.1037/0033-295x.99.3.582. URL [https://doi.org/10.1037/0033-295x.99.3.582](https://doi.org/10.1037/0033-295x.99.3.582).
* Sanders et al. [2020] Honi Sanders, Matthew A Wilson, and Samuel J Gershman. Hippocampal remapping as hidden state inference. _eLife_, 9, June 2020. doi: 10.7554/elife.51140. URL [https://doi.org/10.7554/elife.51140](https://doi.org/10.7554/elife.51140).
* Hafting et al. [2005] Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, and Edvard I. Moser. Microstructure of a spatial map in the entorhinal cortex. _Nature_, 436(7052):801-806, June 2005. doi: 10.1038/nature03721. URL [https://doi.org/10.1038/nature03721](https://doi.org/10.1038/nature03721).
* Moser et al. [2017] Edvard I Moser, May-Britt Moser, and Bruce L McNaughton. Spatial representation in the hippocampal formation: a history. _Nature Neuroscience_, 20(11):1448-1464, November 2017. doi: 10.1038/nn.4653. URL [https://doi.org/10.1038/nn.4653](https://doi.org/10.1038/nn.4653).
* Spiers and Maguire [2006] Hugo J. Spiers and Eleanor A. Maguire. Thoughts, behaviour, and brain dynamics during navigation in the real world. _NeuroImage_, 31(4):1826-1840, July 2006. doi: 10.1016/j.neuroimage.2006.01.037. URL [https://doi.org/10.1016/j.neuroimage.2006.01.037](https://doi.org/10.1016/j.neuroimage.2006.01.037).
* Carr et al. [2011] Margaret F Carr, Shantanu P Jadhav, and Loren M Frank. Hippocampal replay in the awake state: a potential substrate for memory consolidation and retrieval. _Nature Neuroscience_, 14(2):147-153, January 2011. doi: 10.1038/nn.2732. URL [https://doi.org/10.1038/nn.2732](https://doi.org/10.1038/nn.2732).

bioRxiv preprint doi: [https://doi.org/10.1101/20203.12.21571268](https://doi.org/10.1101/20203.12.21571268); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under ACC-BY-NC-ND 4.0 International license.
* McNaughton et al. [1996] B. L. McNaughton, C. A. Barnes, J. L. Gerrard, K. Gothard, M. W. Jung, J. J. Knierim, H. Kudrimoti, Y. Qin, W. E. Skaggs, M. Suster, and K. L. Weaver. Deciphering the hippocampal polyglot- the hippocampus as a path integration system. _Journal of Experimental Biology_, 199(1):173-185, January 1996. doi: 10.1242/jeb.199.1.173. URL [https://doi.org/10.1242/jeb.199.1.173](https://doi.org/10.1242/jeb.199.1.173).
* Cueva and Wei [2018] Christopher J. Cueva and Xue-Xin Wei. Emergence of grid-like representations by training recurrent neural networks to perform spatial localization, 2018.
* Banino et al. [2018] Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin J. Chadwick, Thomas Degris, Joseph Modayil, Greg Wayne, Hubert Soyer, Fabio Viola, Brian Zhang, Ross Goroshin, Neil Rabinowitz, Razvan Pascanu, Charlie Beattie, Stig Petersen, Amir Sadik, Stephen Gaffney, Helen King, Koray Kavukcuoglu, Dennis Hassabis, Raia Hadsell, and Dharshan Kumaran. Vector-based navigation using grid-like representations in artificial agents. _Nature_, 557(7705):429-433, May 2018. doi: 10.1038/s41586-018-0102-6. URL [https://doi.org/10.1038/s41586-018-0102-6](https://doi.org/10.1038/s41586-018-0102-6).
* Sorscher et al. [2023] Ben Sorscher, Gabriel C. Mel, Samuel A. Ocko, Lisa M. Giocomo, and Surya Ganguli. A unified theory for the computational and mechanistic origins of grid cells. _Neuron_, 111(1):121-137.e13, 2023. ISSN 0896-6273. doi: [https://doi.org/10.1016/j.neuron.2022.10.003](https://doi.org/10.1016/j.neuron.2022.10.003). URL [https://www.sciencedirect.com/science/article/pii/S0896627322009072](https://www.sciencedirect.com/science/article/pii/S0896627322009072).
* Dorrell et al. [2023] William Dorrell, Peter E. Latham, Timothy E. J. Behrens, and James C. R. Whittington. Actionable neural representations: Grid cells from minimal constraints, 2023.
* Whittington et al. [2020] James C.R. Whittington, Timothy H. Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil Burgess, and Timothy E.J. Behrens. The tolman-eichenbaum machine: Unifying space and relational memory through generalization in the hippocampal formation. _Cell_, 183(5):1249-1263.e23, November 2020. doi: 10.1016/j.cell.2020.10.024. URL [https://doi.org/10.1016/j.cell.2020.10.024](https://doi.org/10.1016/j.cell.2020.10.024).
* George et al. [2021] Dileep George, Rajeev V. Rikhye, Nishad Gothoskar, J. Swaroop Guntupalli, Antoine Dedieu, and Miguel Lazaro-Gredilla. Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps. _Nature Communications_, 12(1), April 2021. doi: 10.1038/s41467-021-22559-5. URL [https://doi.org/10.1038/s41467-021-22559-5](https://doi.org/10.1038/s41467-021-22559-5).
* Skaggs et al. [1995] W. E. Skaggs, J. J. Knierim, H. S. Kudrimoti, and B. L. McNaughton. A model of the neural basis of the rat's sense of direction. _Advances in neural information processing systems_, 7:173-180, 1995. ISSN 1049-5258.
* Samsonovich and McNaughton [1997] Alexei Samsonovich and Bruce L. McNaughton. Path integration and cognitive mapping in a continuous attractor neural network model. _Journal of Neuroscience_, 17(15):5900-5920, 1997. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.17-15-05900.1997. URL [https://www.jneurosci.org/content/17/15/5900](https://www.jneurosci.org/content/17/15/5900).
* Burak and Fiete [2009] Yoram Burak and Ila R. Fiete. Accurate path integration in continuous attractor network models of grid cells. _PLoS Computational Biology_, 5(2):e1000291, February 2009. doi: 10.1371/journal.pcbi.1000291. URL [https://doi.org/10.1371/journal.pcbi.1000291](https://doi.org/10.1371/journal.pcbi.1000291).
* Khona and Fiete [2021] Mikail Khona and Ila R Fiete. Attractor and integrator networks in the brain. _arXiv_, 2021. doi: 10.48550/arxiv.2112.03978.
* Dayan et al. [1995] Peter Dayan, Geoffrey E. Hinton, Radford M. Neal, and Richard S. Zemel. The helmholtz machine. _Neural Computation_, 7(5):889-904, September 1995. doi: 10.1162/neco.1995.7.5.889. URL [https://doi.org/10.1162/neco.1995.7.5.889](https://doi.org/10.1162/neco.1995.7.5.889).
* Buzsaki [2002] Gyorgy Buzsaki. Theta oscillations in the hippocampus. _Neuron_, 33(3):325-340, January 2002. doi: 10.1016/s0896-6273(02)00586-x. URL [https://doi.org/10.1016/s0896-6273](https://doi.org/10.1016/s0896-6273)(02)00586-x.
* Hinton et al. [1995] Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and Radford M. Neal. The "wake-sleep" algorithm for unsupervised neural networks. _Science_, 268(5214):1158-1161, May 1995. doi: 10.1126/science.7761831. URL [https://doi.org/10.1126/science.7761831](https://doi.org/10.1126/science.7761831).
* Uria et al. [2020] Benigno Uria, Borja Ibarz, Andrea Banino, Vinicius Zambaldi, Dharshan Kumaran, Demis Hassabis, Caswell Barry, and Charles Blundell. A model of egocentric to allocentric understanding in mammalian brains. November 2020. doi: 10.1101/2020.11.11.378141. URL [https://doi.org/10.1101/2020.11.11.378141](https://doi.org/10.1101/2020.11.11.378141).

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571268](https://doi.org/10.1101/2023.12.2571268); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under ACC-BY-NC-ND 4.0 International license.
* Han et al. [2020] Dongqi Han, Kenji Doya, and Jun Tani. Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent neural networks. _Neural Networks_, 129:149-162, September 2020. doi: 10.1016/j.neunet.2020.06.002. URL [https://doi.org/10.1016/j.neunet.2020.06.002](https://doi.org/10.1016/j.neunet.2020.06.002).
* Sharma et al. [2022] Sugandha Sharma, Sarthak Chandra, and Ila Fiete. Content addressable memory without catastrophic forgetting by heteroassociation with a fixed scaffold. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 19658-19682. PMLR, 17-23 Jul 2022. URL [https://proceedings.mlr.press/v162/sharma22b.html](https://proceedings.mlr.press/v162/sharma22b.html).
* Vafidis et al. [2022] Pantelis Vafidis, David Owald, Tiziano D'Albis, and Richard Kempter. Learning accurate path integration in ring attractor models of the head direction system. _eLife_, 11:e69841, jun 2022. ISSN 2050-084X. doi: 10.7554/eLife.69841. URL [https://doi.org/10.7554/eLife.69841](https://doi.org/10.7554/eLife.69841).
* Widloski and Fiete [2014] John Widloski and Ila R. Fiete. A Model of Grid Cell Development through Spatial Exploration and Spike Time-Dependent Plasticity. _Neuron_, 83(2):481-495, 2014. ISSN 0896-6273. doi: 10.1016/j.neuron.2014.06.018.
* Skaggs et al. [1996] William E. Skaggs, Bruce L. McNaughton, Matthew A. Wilson, and Carol A. Barnes. Theta phase precession in hippocampal neuronal populations and the compression of temporal sequences. _Hippocampus_, 6(2):149-172, 1996. doi: 10.1002/(sici)1098-1063(1996)6:2<149::aid-hipo6>3.0.co;2-k. URL [https://doi.org/10.1002/](https://doi.org/10.1002/)(sici)1098-1063(1996)6:2<149::aid-hipo6>3.0.co;2-k.
* Mehta et al. [2000] M.R. Mehta, M.C. Quirk, and M.A. Wilson. Experience-Dependent Asymmetric Shape of Hippocampal Receptive Fields. _Neuron_, 25:707-715, 2000.
* George et al. [2023] Tom M George, William de Cothi, Kimberly L Stachenfeld, and Caswell Barry. Rapid learning of predictive maps with STDP and theta phase precession. _eLife_, 12, March 2023. doi: 10.7554/elife.80663. URL [https://doi.org/10.7554/elife.80663](https://doi.org/10.7554/elife.80663).
* George [2023] Tom M George. Theta sequences as eligibility traces: A biological solution to credit assignment. In _International Conference on Learning Representations 2023 (TinyPapers track)_, 2023. doi: [https://doi.org/10.48550/arXiv.2305.08124](https://doi.org/10.48550/arXiv.2305.08124). URL [https://openreview.net/forum?id=v16AFJbenz3Z](https://openreview.net/forum?id=v16AFJbenz3Z).
* Bredenberg et al. [2021] Colin Bredenberg, Eero P Simoncelli, Benjamin S H Lyo, and Cristina Savin. Impression learning: Online representation learning with synaptic plasticity. page 13, 2021.
* Kording and Konig [2001] Konrad P. Kording and Peter Konig. Supervised and unsupervised learning with two sites of synaptic integration. _Journal of Computational Neuroscience_, 11(3):207-215, 2001. doi: 10.1023/a:1013776130161. URL [https://doi.org/10.1023/a:1013776130161](https://doi.org/10.1023/a:1013776130161).
* Urbanczik and Senn [2014] Robert Urbanczik and Walter Senn. Learning by the Dendritic Prediction of Somatic Spiking. _Neuron_, 81(3):521-528, 2014. ISSN 08966273. doi: 10.1016/j.neuron.2013.11.030. URL [https://linkinghub.elsevier.com/retrieve/pii/S0896627313011276](https://linkinghub.elsevier.com/retrieve/pii/S0896627313011276).
* Sacramento et al. [2018] Joao Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic cortical microcircuits approximate the backpropagation algorithm. In _Advances in Neural Information Processing Systems_, pages 8721-8732, 2018.
* Richards and Lillicrap [2019] Blake A Richards and Timothy P Lillicrap. Dendritic solutions to the credit assignment problem. _Current Opinion in Neurobiology_, 54:28-36, February 2019. doi: 10.1016/j.conb.2018.08.003. URL [https://doi.org/10.1016/j.conb.2018.08.003](https://doi.org/10.1016/j.conb.2018.08.003).
* Bittner et al. [2015] Katie C Bittner, Christine Grienberger, Sachin P Vaidya, Aaron D Milstein, John J Macklin, Junghyup Suh, Susumu Tonegawa, and Jeffrey C Magee. Conjunctive input processing drives feature selectivity in hippocampal CA1 neurons. _Nature Neuroscience_, 18(8):1133-1142, July 2015. doi: 10.1038/nn.4062. URL [https://doi.org/10.1038/nn.4062](https://doi.org/10.1038/nn.4062).
* Brankack et al. [1993] Jurij Brankack, Mark Stewart, and Steven E. Fox. Current source density analysis of the hippocampal theta rhythm: associated sustained potentials and candidate synaptic generators. _Brain Research_, 615(2):310-327, July 1993. doi: 10.1016/0006-8993(93)90043-m. URL [https://doi.org/10.1016/0006-8993](https://doi.org/10.1016/0006-8993)(93)90043-m.

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571268](https://doi.org/10.1101/2023.12.2571268); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under ACC-BY-NC-ND 4.0 International license.
* Mizuseki et al. [2009] Kenji Mizuseki, Anton Sirota, Eva Pastalkova, and Gyorgy Buzsaki. Theta oscillations provide temporal windows for local circuit computation in the entorhinal-hippocampal loop. _Neuron_, 64(2):267-280, October 2009. doi: 10.1016/j.neuron.2009.08.037. URL [https://doi.org/10.1016/j.neuron.2009.08.037](https://doi.org/10.1016/j.neuron.2009.08.037).
* Larkum [2022] Matthew E. Larkum. Are dendrites conceptually useful? _Neuroscience_, 489:4-14, May 2022. doi: 10.1016/j.neuroscience.2022.03.008. URL [https://doi.org/10.1016/j.neuroscience.2022.03.008](https://doi.org/10.1016/j.neuroscience.2022.03.008).
* Foster and Wilson [2007] David J. Foster and Matthew A. Wilson. Hippocampal theta sequences. _Hippocampus_, 17(11):1093-1099, 2007. doi: 10.1002/hipo.20345. URL [https://doi.org/10.1002/hipo.20345](https://doi.org/10.1002/hipo.20345).
* Holscher et al. [1997] Christian Holscher, Roger Anwyl, and Michael J. Rowan. Stimulation on the positive phase of hippocampal theta rhythm induces long-term potentiation that can be depotentiated by stimulation on the negative phase in area cal. _The Journal of Neuroscience_, 17(16):6470-6477, August 1997. doi: 10.1523/jneurosci.17-16-06470.1997. URL [https://doi.org/10.1523/jneurosci.17-16-06470.1997](https://doi.org/10.1523/jneurosci.17-16-06470.1997).
* Yamaguchi et al. [2002] Yoko Yamaguchi, Yoshito Aota, Bruce L. McNaughton, and Peter Lipa. Bimodality of theta phase precession in hippocampal place cells in freely running rats. _Journal of Neurophysiology_, 87(6):2629-2642, June 2002. doi: 10.1152/jn.2002.87.6.2629. URL [https://doi.org/10.1152/jn.2002.87.6.2629](https://doi.org/10.1152/jn.2002.87.6.2629).
* Hasselmo et al. [2002] Michael E. Hasselmo, Clara Bodelon, and Bradley P. Wyble. A proposed function for hippocampal theta rhythm: Separate phases of encoding and retrieval enhance reversal of prior learning. _Neural Computation_, 14(4):793-817, April 2002. doi: 10.1162/089976602317318965. URL [https://doi.org/10.1162/089976602317318965](https://doi.org/10.1162/089976602317318965).
* Sargolini et al. [2006] Francesca Sargolini, Marianne Fyhn, Torkel Hafting, Bruce L. McNaughton, Menno P. Witter, May-Britt Moser, and Edward I. Moser. Conjunctive representation of position, direction, and velocity in entorhinal cortex. _Science_, 312(5774):758-762, May 2006. doi: 10.1126/science.1125572. URL [https://doi.org/10.1126/science.1125572](https://doi.org/10.1126/science.1125572).
* George et al. [2022] Tom M George, William de Cothi, Claudia Clopath, Kimberly Stachenfeld, and Caswell Barry. RatInABox: A toolkit for modelling locomotion and neuronal activity in continuous environments. aug 2022. doi: 10.1101/2022.08.10.503541. URL [https://doi.org/10.1101%2F2022.08.10.503541](https://doi.org/10.1101%2F2022.08.10.503541).
* Zhang [1996] K Zhang. Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory. _The Journal of Neuroscience_, 16(6):2112-2126, March 1996. doi: 10.1523/jneurosci.16-06-02112.1996. URL [https://doi.org/10.1523/jneurosci.16-06-02112.1996](https://doi.org/10.1523/jneurosci.16-06-02112.1996).
* Ergorbul and Eichenbaum [2006] Ceren Ergorbul and Howard Eichenbaum. Essential role of the hippocampal formation in rapid learning of higher-order sequential associations. _The Journal of Neuroscience_, 26(15):4111-4117, April 2006. doi: 10.1523/jneurosci.0441-06.2006. URL [https://doi.org/10.1523/jneurosci.0441-06.2006](https://doi.org/10.1523/jneurosci.0441-06.2006).
* Fyhn et al. [2007] Marianne Fyhn, Torkel Hafting, Alessandro Treves, May-Britt Moser, and Edvard I. Moser. Hippocampal remapping and grid realignment in entorhinal cortex. _Nature_, 446(7132):190-194, February 2007. doi: 10.1038/nature05601. URL [https://doi.org/10.1038/nature05601](https://doi.org/10.1038/nature05601).
* Guerguiev et al. [2017] Jordan Guerguiev, Timothy P Lillicrap, and Blake A Richards. Towards deep learning with segregated dendrites. eLife, 6, December 2017. doi: 10.7554/elife.22901. URL [https://doi.org/10.7554/elife.22901](https://doi.org/10.7554/elife.22901).
* Fries [2015] Pascal Fries. Rhythms for cognition: Communication through coherence. _Neuron_, 88(1):220-235, October 2015. doi: 10.1016/j.neuron.2015.09.034. URL [https://doi.org/10.1016/j.neuron.2015.09.034](https://doi.org/10.1016/j.neuron.2015.09.034).
* Payeur et al. [2021] Alexandre Payeur, Jordan Guerguiev, Friedemann Zenke, Blake A. Richards, and Richard Naud. Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits. _Nature Neuroscience_, 24(7):1010-1019, May 2021. doi: 10.1038/s41593-021-00857-x. URL [https://doi.org/10.1038/s41593-021-00857-x](https://doi.org/10.1038/s41593-021-00857-x).
* Sanders et al. [2021] Honi Sanders, Cesar Renno-Costa, Marco Idiart, and John Lisman. Grid cells and place cells: An integrated view of their navigational and memory function. _Trends in Neurosciences_, 38bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571286](https://doi.org/10.1101/2023.12.2571286); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under ACC-BY-NC-ND 4.0 International license.
* [63] Kwan Tung Li, Junhao Liang, and Changsong Zhou. Gamma oscillations facilitate effective learning in excitatory-inhibitory balanced neural circuits. _Neural Plasticity_, 2021:1-18, January 2021. doi: 10.1155/2021/6668175. URL [https://doi.org/10.1155/2021/6668175](https://doi.org/10.1155/2021/6668175).
* [64] William E. Skaggs and Bruce L. McNaughton. Replay of neuronal firing sequences in rat hippocampus during sleep following spatial experience. _Science_, 271(5257):1870-1873, March 1996. doi: 10.1126/science.271.5257.1870. URL [https://doi.org/10.1126/science.271.5257.1870](https://doi.org/10.1126/science.271.5257.1870).

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571286](https://doi.org/10.1101/2023.12.2571286); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under ACC-BY-NC-ND 4.0 International license.

## 5 Supplementary Material

### Basic model description

Here we give a general decription of the model. Specifics for each experiment (i.e. learning rates, layer sizes, time constants etc.) are given in later sections.

#### 5.1.1 Dendritic updates

Complete versions of the dendritic update rules (summarised in Eqns (2) & (3)) are given below. We assume dendrites recieve and integrate synaptic inputs according to the following dynamics:

\[\left.\begin{array}{l}\tau\frac{d\mathbf{p}_{B}(t)}{dt}=-\mathbf{p}_{B}(t)+ \bar{\mathbf{p}}(z(t))\\ \tau\frac{d\mathbf{z}_{B}(t)}{dt}=-\mathbf{g}_{B}(t)+\sigma_{g_{B}}(\mathbf{w} _{g_{B}}\mathbf{p}(t))\end{array}\right\}\quad\text{ Inference model} \tag{9}\]

\[\left.\begin{array}{l}\tau\frac{d\mathbf{g}_{A}(t)}{dt}=-\mathbf{g}_{A}(t)+ \sigma_{g_{A}}(\mathbf{w}_{g_{A}}\mathbf{g}(t))\\ \tau\frac{d\mathbf{g}_{A}(t)}{dt}=-\mathbf{p}_{A}(t)+\sigma_{p_{A}}(\mathbf{w} _{p_{A}}\mathbf{g}(t))\end{array}\right\}\quad\text{ Generative model}. \tag{10}\]

We discretise these dynamics in order to implement them computationally by making the common assumption that neural dynamics are fast (\(\tau\approx 0\) ms) relative to the timescale of the synaptic inputs and so the compartments are always at equilibrium, recovering Eqns (2) & (3). This is valid in our regime where the environmental latent updates slowly compared to neural timescales. The notation we're using admits the possible presence of biases as well as the weights (though biases typically aren't used) by assuming a row of constant 1's could be added to the synaptic inputs effectively absorbing a bias into the weight matrix without loss of generality, for example \(\mathbf{w}_{g_{B}}\mathbf{p}(t)\leftarrow\mathbf{w}_{g_{B}}\mathbf{p}(t)+b_{g_ {B}}\).

#### 5.1.2 Somatic updates

Somatic updates rules (Eqns (4) & (5)) and are repeated here for completeness:

\[\mathbf{p}(t)=\theta(t)\mathbf{p}_{B}(t)+(1-\theta(t))\mathbf{p} _{A}(t)\] \[\mathbf{g}(t)=\theta(t)\mathbf{g}_{B}(t)+(1-\theta(t))\mathbf{g} _{A}(t). \tag{11}\]

where \(\theta(t)\) is a 5 Hz global theta oscillation variable defined by the square wave function:

\[\theta(t)=\begin{cases}1,&\text{if }t/T\mod 1\leq 0.5\\ 0,&\text{if }t/T\mod 1>0.5\end{cases} \tag{12}\]

#### 5.1.3 Update ordering

For this hierarchical network of multicompartmental neurons we must specify the order in which we perform these discrete updates to the different layers and the different compartments within these layers. Strictly speaking, when the discretisation timestep \(dt\) is small this ordering is arbitrary, but we include it here for completeness.

We update the layers from bottom to top: first we update the latent or "environment" and increment the global clock (\(z(t+dt)\gets z(t)\) & \(t+dt\gets t\)). Next we update both dendritic compartments of the sensory layer (\(\mathbf{p}_{B}(t+dt)\leftarrow\mathbf{p}_{B}(t)\) & \(\mathbf{p}_{A}(t+dt)\leftarrow\mathbf{p}_{A}(t)\) noting that it makes no difference in which order these updates are done as they are independent. Then we update the somatic compartment of the sensory layer (\(\mathbf{p}(t+dt)\leftarrow\mathbf{p}(t)\)). Next we work upwards to the hidden layer (\(\mathbf{g}_{B}(t+dt)\leftarrow\mathbf{g}_{B}(t)\) & \(\mathbf{g}_{A}(t+dt)\leftarrow\mathbf{g}_{A}(t)\) followed by \(\mathbf{g}(t+dt)\leftarrow\mathbf{g}(t)\)) then, if present, the top-most "conjunctive cells" are updated. This gives the following dendritic update rules which are only slightly - and in the limit \(dt\to 0\), irrelevantly - different from the simplified update rules given in the main text:

\[\mathbf{p}_{B}(t+dt)=\bar{\mathbf{p}}(z(t+dt))\] \[\mathbf{p}_{A}(t+dt)=\sigma_{g_{A}}(\mathbf{w}_{g_{A}}\mathbf{g} (t))\] \[\mathbf{g}_{B}(t+dt)=\sigma_{g_{B}}(\mathbf{w}_{g_{B}}\mathbf{p} (t+dt))\] \[\mathbf{g}_{A}(t+dt)=\sigma_{g_{A}}(\mathbf{w}_{g_{A}}\mathbf{g} (t)) \tag{13}\]bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571268](https://doi.org/10.1101/2023.12.2571268); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under ACC-BY-NC-ND 4.0 International license.

#### 5.1.4 Learning rules

Learning rules are conceptually summarised by the equations given in the main text, Eqn (6). Here we give the _full_ equations which include some adjustments to account for the presence of non-linear activation functions and temporal smoothing of the local prediction error learning signals. In our multilayer network all sets of learnable weights follow an equivalent learning rule. For this reason we choose give it here in its most general form: Consider the synaptic weight \(w_{ij}\) connecting from the soma of presynaptic neuron \(j\) with activation \(f_{j}^{\text{pre}}\) to one of the dendritic compartments of a postsynaptic neuron \(i\) with activation \(f_{C,i}^{\text{post}}=\sigma(V_{C,i}^{\text{post}})\) (this could be the basal or apical compartment, \(C\in\{A,B\}\)). Weights are updated on each timestep by an amount:

\[\delta w_{ij}(t)=\eta\text{PI}_{ij}(t) \tag{14}\]

where PI\({}_{ij}\) is (following terminology used in Urbanczik and Senn [43]) the "plasticity induction" variable which is a low-pass filtered measure of the coincidence between the local prediction error and the synaptic input. The prediction error measures how far the activation of the dendritic compartment, \(f_{C,i}^{\text{post}}\), is from the somatic activation \(f_{i}^{\text{post}}\). In total, PI\({}_{ij}\) is defined by the following dynamics:

\[\tau_{\text{PI}}\frac{d\text{PI}_{ij}}{dt}=-\text{PI}_{ij}+\underbrace{[f_{i} ^{\text{post}}(t)-f_{C,i}^{\text{post}}(t)]}_{\text{postsynaptic prediction error}} \cdot\ \sigma^{\prime}(V_{C,i}^{\text{post}}(t))\ \ \ \cdot\ \underbrace{f_{j}^{\text{pre}}(t)}_{\text{presynaptic input}} \tag{15}\]

If the prediction error and one of the presynaptic inputs are both consistently large (i.e. over a time period \(\mathcal{O}(\tau_{\text{PI}})\)) then the plasticity induction variable will therefore also be large and the weight connecting the pre- and postsynaptic neurons will be strengthened (thus decreasing future prediction errors). \(\tau_{\text{PI}}\) is taken to be the same as used in Urbanczik and Senn [43], 100 ms. Note for fast filtering (\(\tau_{\text{PI}}\to 0\) ms) and linear activation functions this reduces to the simplified formulae given in the main text, Eqn. (6).

#### 5.1.5 Synaptic noise

We add synaptic noise to the dendritic activations. Each dendritic compartment maintains its own independent noise variable, \(n(t)\), which is modelled as an Ornstein-Uhlenbeck process. The benefit of modelling neural noise with an Ornstein-Uhlenbeck process is that it is timestep size independent. The dynamics of the noise variable are given by:

\[n(t+dt)=n(t)+\frac{dt}{\tau}n(t)+\sqrt{\frac{2\sigma^{2}dt}{\tau}}\xi(t) \tag{16}\]

where \(\xi(t)\sim\mathcal{N}(0,1)\) is a white noise process. These dynamics lead to a stationary distribution of \(n(t)\) which is Gaussian with zero mean and variance \(\sigma^{2}\). The decoherence timescale of the noise is \(\tau\). We fix \(\tau=300\) ms and \(\sigma=0.01\) Hz in order that noise is relatively slow and weak. Noise is added at each timestep to the activation of the dendrites, e.g. \(\mathbf{p}_{B}(t)\rightarrow\mathbf{p}_{B}(t)+\mathbf{n}_{B}(t)\) where \(\mathbf{n}_{B}(t)\).

#### 5.1.6 Measuring the prediction error

Figs. 1(b) & 2(d) show the prediction errors of the network layers decreasing throughout training. Here we define how these errors. A consequence of our learning rule is that during wake, the apical dendrites adjust to try minimise the discrepancy between the apical activation and the soma (which, during wake, is equal to by the basal activation). During the sleep phase a short time later the basal dendrites adjust to try minimise the discrepancy between the basal activation and the soma (which, during sleep, is equal to apical activation). If learning is successful we would expect the apical and basal activations to converge, thus we use the following measures of the prediction error to track training performance in both layers of the network:

\[\mathcal{E}_{p}(t) =\frac{1}{N_{p}}\sum_{i}|[\mathbf{p}_{B}(t)]_{i}-[\mathbf{p}_{A}( t)]_{i}|\] \[\mathcal{E}_{g}(t) =\frac{1}{N_{g}}\sum_{i}|[\mathbf{g}_{B}(t)]_{i}-[\mathbf{g}_{A}( t)]_{i}|. \tag{17}\]

These are then smoothed with a decaying exponential kernel of timescale 60 seconds to remove some of the nosie and better display the learning signal.

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571286](https://doi.org/10.1101/2023.12.2571286); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under ACC-BY-NC-ND 4.0 International license.

### Relationship to online Bayesian Inference

Bredenberg et al. [41] derived local synaptic learning rules for a similar hierarchical network performing online latent inference starting from a loss function closely related to the evidence lower bound (ELBO) of variational inference. Here we will not repeat their derivation, instead we intend to highlight their starting point, the most important assumptions they made and the learning rules they derived, finally pointing out how ours differ. The point is to demonstrate that the learning rules we propose are not arbitrary but can actually be derived from a more principled approach to online inference.

Bredenberg et al. [41] consider a network receiving input from a latent variable \(z\). The network has two layers, \(\mathbf{p}_{t}\) and \(\mathbf{g}_{t}\). 3 The network is trained to perform online inference over a sequence of observations from the environment, \(z_{0:T}\). To do this they start from the loss function

Footnote 3: For convenience we have translated their variables into our notation (\(\mathbf{g}\leftrightarrow\mathbf{r},\mathbf{p}\leftrightarrow\mathbf{s}, \theta\leftrightarrow\lambda\), \(\mathbf{w}\leftrightarrow\theta\)) so it is easier to compare.

\[\mathcal{L}=\mathbb{E}_{\theta,\mathbf{z}}\big{[}D_{KL}(\tilde{q}_{w}\parallel \tilde{p}_{w})\big{]} \tag{18}\]

where \(\tilde{q}_{w}\) and \(\tilde{p}_{w}\) are the following probability distributions over the layer variables \(\mathbf{p}_{t}\) and \(\mathbf{g}_{t}\):

\[\tilde{q}_{w} =\prod_{t=0}^{T}\big{(}q(\mathbf{g}_{t}|\mathbf{p}_{t};w_{\text{ inf}})p(\mathbf{p}_{t}|z_{t})\big{)}^{\theta_{t}}p_{m}(\mathbf{g}_{t},\mathbf{p}_{t }|\mathbf{p}_{t-1},\theta_{t};w_{\text{gen}})^{1-\theta_{t}}, \tag{19}\] \[\tilde{p}_{w} =\prod_{t=0}^{T}\underbrace{(p(\mathbf{g}_{t}|\mathbf{p}_{t};w_{ \text{inf}})p(\mathbf{p}_{t}|z_{t}))}_{\text{inference model}}\text{1}^{- \theta_{t}}\underbrace{p_{m}(\mathbf{g}_{t},\mathbf{p}_{t}|\mathbf{p}_{t-1}, \theta_{t};w_{\text{gen}})}_{\text{generative model}}\theta_{t} \tag{20}\]

and \(\theta_{t}\in\{0,1\}\) is a binary variable (in their analysis they fix this to oscillate in fixed symmetric phases, e.g. 000111000111...). The two probability distributions, \(\tilde{q}_{w}\) & \(\tilde{p}_{w}\), which this loss function attempts to make similar to one another, can be interpreted as the probabilites over the layer variables \(\mathbf{p}_{t}\) and \(\mathbf{g}_{t}\) in two noisy neural networks4 connected as we drew in Fig. 1a: the first network alternates between phases of inference, where information flows bottom up from the latents \(z\) to the hidden layer \(\mathbf{g}\), and generation, the opposite (inference-generation-inference-generation...), the second network alternates in exact counterphase (generation-inference-generation-inference...). This loss is a generalisation of the widely used evidence lower bound (ELBO) which corresponds to the case where \(\theta_{t}=1\) for all \(t\). ELBO loss functions seek to make the inference and generative distributions over sensory and hidden variables similar. We will not delve further into the justifications for these types of loss functions other than to state that they are widely used[1].

Footnote 4: Note there isn’t _actually_ two networks being trained. Instead they use a mathematical trick, deriving from the symmetry in the alternating phase of the theta cycle, to do away with the need to sample from both networks meaning they can deriving local learning rules which can train a single network, e.g. \(\tilde{q}_{m}\), on its own. This single network, like ours, contains both inference and generative models, represented by the two terms in equation (19)

One of the key conceptual steps taken by Bredenberg et al. [41] (and now us) is to note that processes of performing inference and generation can locally occur simultaneously as long as they are recieved into distinct dendritic compartments. Which dendrite then gates into the soma (i.e. Eqn 4) then dictates the global state (wake or sleep) of the network. It also means, as they show, that the loss can be approximately optimized using local learning rules by comparing the dendritic compartment activation to that of the soma. The learning rules they derive, again translated into our notation, are as follows (note for simplicity we assume all activations are linear since non-linearities add only one additional multiplicative term into their update equations, see equations (14), (15) and (16) in [41]):

\[\frac{d\mathbf{w}_{g_{R}}}{dt} \propto(1-\theta_{t})(\mathbf{g}_{t}-\mathbf{g}_{B,t})\mathbf{p}_ {t}^{\mathsf{T}} \tag{21}\] \[\frac{d\mathbf{w}_{p_{A}}}{dt} \propto\theta_{t}(\mathbf{p}_{t}-\mathbf{p}_{A,t})\mathbf{g}_{t} ^{\mathsf{T}}\] (22) \[\frac{d\mathbf{w}_{g_{A}}}{dt} \propto\theta_{t}(1-k_{t})(\mathbf{g}_{t}-\mathbf{g}_{A,t-1}) \mathbf{g}_{t-1}^{\mathsf{T}} \tag{23}\]

where \(k_{t}=(1-\delta(\theta_{t}-\theta_{t-1}))\theta_{t}\) is a term which is 1 if and only if \(\theta_{t}=1\) and \(\theta_{t-1}=0\) therefore it briefly turns off learning upon switching from sleep to wake.

Reader may like to compare these learning rules to our own as given in the main text Eqns (6). Our learning rules differ from theirs in the following way:

\[\mathbf{g}_{t}\leftrightarrow\mathbf{r},\mathbf{p}\leftrightarrow\mathbf{s}, \theta\leftrightarrow\lambda,\,\mathbf{w}\leftrightarrow\theta\text{) so it is }\]

\[\mathbf{g}_{t}\leftrightarrow\mathbf{r},\mathbf{p}\leftrightarrow\mathbf{s}, \theta\leftrightarrow\lambda,\,\mathbf{w}\leftrightarrow\theta\text{) so it is }\]

\[\mathbf{g}_{t}\leftrightarrow\mathbf{r},\mathbf{p}\leftrightarrow\mathbf{s}, \theta\leftrightarrow\lambda,\,\mathbf{w}\leftrightarrow\theta\text{) so it is }\]

\[\mathbf{g}_{t}\leftrightarrow\mathbf{r},\mathbf{p}\leftrightarrow\mathbf{s}, \theta\leftrightarrow\lambda,\mathbf{w}\leftrightarrow\theta\text{) so it is }\]

\[\mathbf{g}

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571286](https://doi.org/10.1101/2023.12.2571286); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under ACC-BY-NC-ND 4.0 International license.
* We relax their discrete time assumption, opting for a continuous time formulation (\(\mathbf{p}_{t}\rightarrow\mathbf{p}(t)\) etc.).
* We note that the terms in the equations proportional to \(\theta_{t}\) or \(1-\theta_{t}\) which actively turn on or off learning depending on whether \(\theta_{t}\) = 0 or 1 are unnecessary since the prediction error term natural falls to zero anyway. For example, in Eqn. (22) when \(\theta_{t}=0\) the network is in sleep and so \(\mathbf{p}_{t}=\mathbf{p}_{A,t}\). In this case the prediction error is zero by definition and learning ceases even without the preceeding \(\theta_{t}\) term.
* We disregard the \(1-k_{t}\) term. Empirically this does not seem to damage our model and theoretically its impact should only be small in our continuous time formulation where the network is only switching from sleep to wake for a negligible proportion of the time.
* Upon provisional theoretical and experimental justification we liken \(\theta\) the theta component of the hippocampal local field potential and set it to 5 Hz.

Ultimately these changes are surface level. Our learning rules can - and should - be understood as a close approximation to those derived by Bredenberg et al. [41]. Consequently it is appropriate to consider our hippocampal model as learning to perform approximately optimal online Bayesian inference.

### Experiment 1: An artifical latent learning task

\(N_{z}=5\) independent, autocorrelated, random latent variables are sampled from a Gaussian process with a squared exponential covariance function of width 1 second, samples of these are shown in Fig. 2a and Fig. S2. The sensory layer is large (\(N_{p}=50\)) relative to the compressed hidden layer (\(N_{g}=N_{z}=5\)) and recieves a random mixture of the latents into the basal compartments as described in the text. All activation functions are linear, no layers have biases, all learning rates are set to \(\eta=0.01\), and the discretisation timestep was \(dt=25\) ms. Weights are initialised randomly \(\left[\mathbf{w}_{\mathbf{g}_{\mathbf{g}_{\mathbf{g}_{\mathbf{g}_{\mathbf{g} _{\mathbf{g}_{\mathbf{g}_{\mathbf{g}_{\mathbf{g}_{\mathbf{g}_{\mathbf{g}_{ \mathbf{g}}}}}}}}\left|{\mathbf{w}_{\mathbf{g}_{\mathbf{g}_{\mathbf{g}{\mathbf{ g}}_{\mathbf{g}{\mathbf{g}}_{\mathbf{g}{\mathbf{g}}_{\mathbf{g}}_{\mathbf{g} {\mathbf{g}}}}}}}}\right.}}}}}\right]_{ij}\sim\mathcal{N}(0,1/\sqrt{N_{g}})\), \([\mathbf{w}_{\mathbf{g}_{\mathbf{g}_{\mathbf{g}{\mathbf{g}}}}}]_{ij}\sim \mathcal{N}(0,0.1/\sqrt{N_{g}})\) where the smaller initialisation on the recurrent weights, \(\mathbf{w}_{g_{\mathbf{g}{\mathbf{g}}}}\), was chosen to prevent unstable dynamics.

Before learning - since weights are initialised randomly - basal and apical voltages in the sensory layer are unmatched when tested for a period in wake mode (Fig. S1a). When tested for a period in sleep mode, the small initialisation of the recurrent weights means the hidden layer cannot sustain activity (Fig. S1b, top) which decays and decorrelates rapidly in contrast to the true latents (Fig. S1c). Compare this to after learning where, during wake, basal and apical voltages in the sensory layer are closely matched implying accurate autoencoding through the compressed hidden layer. During sleep, the hidden layer generates sustained activity statistically similar to the true latents (they do not match because during sleep the true latents are not driving the network, even during wake we would only expect our network to represent the true latents in its latent space up to a linear rotation), i.e. its is functioning as a generative model. Note the only source of randomness driving stochasticity and activity in the network is the noise in the dendritic updates themselves.

### Experiment 2: Learnable path integration with a hidden ring attractor

An agent randomly moves around a 1 m 1D circular track. The trajectory, \(x(t)\), is sampled using the RatInABox[55] simulation package. This means that velocity is model as an Ornstein-Uhlenbeck process (see Eqn. (16)) with a decoherence timescale of \(\tau=0.7\) seconds and a standard deviation of \(\sigma=0.5\) ms\({}^{-1}\). There are \(N_{p}=N_{g}=100\) neurons in both layers. The HPC dendritic activation function is linear (\(\sigma_{p_{A}}(x)=x\)) whilst both MEC dendritic compartments have rectified tanh activation functions (\(\sigma_{g_{B}}(x)=\sigma_{g_{A}}(x)=\max(0,\tanh(x))\)). Note the choice of activation function means MEC neurons have firing rate \(\mathcal{O}(1\,\mathrm{Hz})\). All learning rates are set to \(\eta=0.01\), the discretisation timestep was \(dt=25\) ms and only \(\mathbf{p}_{A}\) & \(\mathbf{g}_{B}\) have learnable biases.

We model \(N_{i}=N_{p}=100\) inputs which are tuned to the position of the agent according to the following Gaussian tuning curves (these roughly model place cells):

\[[\phi(t)]_{i}=\exp\bigg{[}-\frac{(x(t)-x_{i})}{2\sigma^{2}}\bigg{]}. \tag{24}\]

where \(x_{i}\) are centres of the Gaussians evenly spaced along the track. These then linearly drive the basal dendritic compartments of the sensory neurons:

\[\mathbf{p}_{B}(t)=\mathbf{B}\phi(x(t)) \tag{25}\]bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571268](https://doi.org/10.1101/2023.12.2571268); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.

where, in the results shown in the main paper, \(\mathbf{B}_{ij}=\delta_{ij}\) is the identity matrix such that each sensory neuron inherits a unimodel-tuning curve from one and only one of the inputs, i.e. what was stated in Eqn. (8). We show in the supplementary figure S2 that this choice is not particularly critical and the network can learn to perform path integration with random sensory drive (\([\mathbf{B}]_{ij}\sim\mathcal{N}(0,1/\sqrt{N_{p}})\)).

Velocity inputs are connected as follows: two neurons encode the rectified leftward and rightward velocity of the agent, normalised by the standard deviation \(\sigma\). Note, this means they have firing rates \(\mathcal{O}(1\text{ Hz})\).

\[v_{L}(t) =\max(0,-\dot{x}(t)/\sigma)\] \[v_{R}(t) =\max(0,\dot{x}(t)/\sigma) \tag{26}\]

Two sets of conjunctive cells (\(N_{g}=100\) in each set) sum inputs from the left and right velocity neurons and the hidden units as follows:

\[[\mathbf{g}_{v_{L}}(t)]_{i} =\sigma_{gv}\big{(}v_{L}(t)-v_{R}(t)+\sum_{j}[\mathbf{w}^{g_{vL}} ]_{ij}[\mathbf{g}(t)]_{j}\big{)}\] \[=\sigma_{gv}\big{(}v_{R}(t)-v_{L}(t)+\sum_{j}[\mathbf{w}^{g_{vR}} ]_{ij}[\mathbf{g}(t)]_{j}\big{)} \tag{27}\]

where \(\sigma_{gv}(x)=\max(0,x-1)\) is a ReLU function thresholded at \(x=1\). In the main paper we set \([\mathbf{w}^{g_{vL}}]_{ij}=[\mathbf{w}^{g_{vR}}]_{ij}=\delta_{ij}\) so each conjunctive cell is connected to one and only one hidden unit (something we relax in Fig. S2c). The consequence of this connectivity is that a \(\mathbf{g}_{vL}\) neuron is above threshold (and therefore active) if and only if the agent is moving to the leftand the hidden unit it is connected to is active. Rightward motion silences \(\mathbf{g}_{vL}\) neurons. Similarly, a \(\mathbf{g}_{vR}\) neurons is active if and only if the agent is moving to the right and the hidden unit it is connected to is active. This conjunctive, logic-AND-gate-like tuning to both MEC and velocity is why these neurons are called "conjunctive" cells.

To order the MEC neurons after learning, and thus reveal the ring attractor, we calculate their receptive fields as a function of agent position, \(\mathbf{g}(x)\), as though the network is in inference mode (so top-down recurrent connections and drive from the conjunctive cells do not play a role). Then we permute the ordering \(i^{\prime}\gets i\) such that the maxima of the receptive fields move from left to right along the track as the neuron count increases, \(\arg\max_{x}[\mathbf{g}(x)]_{j^{\prime}}>\arg\max_{x}[\mathbf{g}(x)]_{i^{\prime }}\forall i^{\prime},j^{\prime}>i^{\prime}\). The effect of this ordering procedure is shown in Fig. S2, panel a (left hand side, top two panels).

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571268](https://doi.org/10.1101/2023.12.2571268); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under ACC-BY-NC-ND 4.0 International license.

Fig S2a repeats the same path integration test as was shown in the main text Fig. 3 except now we additionally visualise the receptive fields of HPC and MEC (after learning) and show timeseries of both HPC and MEC neurons during the test. Once MEC neurons are reordered by their maxima the ring attractor activity bump can be seen moving up at down the manifold of neurons, even after the sensory lesion. Note again how some MEC neurons have "died" and do not engage in the ring attractor dynamics, forcing the ring attractor manifold to live on the remain subset of MEC neurons.

#### 5.4.1 Position decoding

To quantify the performance of path integration we train a decoder to estimate agent position directly from the HPC population vector. The decoder is trained on positon and activity data from the final 10 minutes of training, after learning had plateaued. The decoder we use is a Gaussian process regressor with a squared exponential kernel, the length scale of which is optimised during fitting. The decoder works well as can be seen in the path integration plots where, before the sensory lesion, the decoded position correctly and accurately tracks the true position.

#### 5.4.2 Robustness of path integration to weight initialisations, plasticity lesions and noise

Since a central claim of our work is that the network can learn, _from random initialisations_, the correct connectivity required to perform path integration, it is important to question where and why weights in our model are not randomly initialised and plastic.

Sensory weightsThe weights from the Gaussian tuned inputs to the HPC sensory neurons, \(\mathbf{B}\) in Eqn. (25), must be non-plastic to prevent the network from rapidly converging on a trivial solution where all input weights fall to zero killing all activity in the network and trivially minimising the local prediction errors. They do not, however, need to be the identity function as we chose. Fig. S2 panel b repeats the standard path integration experiment but with a network where \([\mathbf{B}]_{ij}\sim\mathcal{N}(0,1/\sqrt{N_{p}})\), path integration is still learned without any problem. Ultimately this is not particularly surprising since the mapping from the spatially-tuned sensory inputs, \(\phi\), to the ring attractor in the orginal formulation was already mixed once by the randomly initialised weights from HPC to MEC (\(\mathbf{w}_{g_{B}}\)). This just adds one additional layer of mixing.

MEC to conjunctive cellsWe show in Fig. S2 panel c, that path integration is still learned even when the MEC to conjunctive cell weights are initialised randomly, \([\mathbf{w}_{g_{e}\perp}]_{ij}\sim\mathcal{N}(0,1/\sqrt{N_{g}})\), \([\mathbf{w}_{g_{e}\parallel}]_{ij}\sim\mathcal{N}(0,1/\sqrt{N_{g}})\). We leave it to future work to investigate this result more thoroughly but comment that it is a notable relaxation on assumptions made in previous models [35, 27] that fine-tuned connectivity from MEC to the conjunctive cells is assumed a priori for path integration (connectivity which would presumably have to be genetically encoded, which seems unlikely). We suspect part of the reason our path integration is robust with respect to the setting of these weights is down to the ability for MEC to construct its own inputs from HPC. This might means the exact form of the activity bump inside the ring attractor can be tailored to fit the specific connectivity to the conjunctive cells - which is perhaps randomly determined during development - in a particular network.

Plasticity lesionsPath integration, as explored in section 3.2, requires fine tuning the recurrent weights in the hidden layer (\(w_{g_{A}}\)) and consequently fails when this plasticity is turned off (Fig. S3a). Intriguingly however, we find that path integration does not _strictly_ require plasticity between HPC and MEC (as shown in Fig. S3b, echoing results in [35]). However, when such plasticity is removed, the apical input to HPC coming from MEC is unmatched to the sensory input HPC recieves from the environment. As such, any downstream system reading out position from the HPC code would only be able to do so during sleep or wake and not both. This is somewhat restrictive for a system hoping to use the hippocampal formation for online inference and planning. Hence, a primary role of interlayer plasticity between HPC and MEC in our model is to "translate" the environment-agnostic MEC code into the the environment-specific HPC code. This idea is discussed further in section 3.3.

### Experiment 3 details: Remapping

To investigate remapping we first train our network to path integrate as described in the main paper. The only difference is that we fix the weights from HPC to MEC to the identity matrix (\([\mathbf{w}_{g_{B}}]_{ij}=\delta_{ij}\)bioRxiv preprint doi: [https://doi.org/10.11011/2023.12.2571286](https://doi.org/10.11011/2023.12.2571286); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.

Figure S2: Path integration is performed by a ring attractor in MEC revealed once the neurons are reordered by receptive field peak position. The network learns to path integrate robustly, regardless of the choice of random initialisations. **a** The same path integration test as in the main text is performed here: The top three rows show receptive fields (left) and timeseries activity (right) for the MEC (top two) and HPC layers (third) layers. MEC receptive fields and activity at first appears random. It is only after reordering the neurons by the peak position of their receptive fields that we see the ring attractor manifold. The bottom row shows the decoded position (red) and the true position (purple), demonstrating accurate path integration. **b** Like panel a except, instead of unimodel Gaussian inputs, the HPC neurons recieve a random-sum-of-Gaussian inputs. Nonetheless the network still learns to path integrate (right). **c** Like panel a – with HPC neurons returned to their original Gaussian receptive fields – except in this experiment the hidden units (MEC, **g**) are connected to the conjunctive cells randomly, not one-to-one. The network still learns to path integrate.

and \(\eta=0\) on these weights) during this phase of training, this results in MEC neurons with receptive fields equal to those of the HPC neurons (except also passed through a rectified-tanh activation function), Fig 4b left column.

In the second phase we begin by randomly permuting the centres of the Gaussian sensory inputs in Eqn. (24). This "sensory shuffle" simulates the sort of hippocampal remapping event which typically occurs when an agent enters into a new environment. The activations of all neuronal layers are reset to zero. A second phase of learning then begins, this time only the weights from HPC to MEC (\(\mathbf{w}_{g_{B}}\)) and from MEC to HPC (\(\mathbf{w}_{p_{A}}\)) are plastic (\(\eta=0.01\)) while the recurrent weights within MEC and the weights from the conjunctive cells to MEC (collectively, \(\mathbf{w}_{g_{A}}\)) are frozen (\(\eta=0\)).

We found that MEC neurons regroup after the shuffle, reestablishing the pairwise correlational structure they had before remapping with, perhaps, a phase shift (Fig. 4b). Once the ring attractor manifold has reappeared in this way the ability to path integrate returns (Fig. 4c). We find these results are clearest when \(\mathbf{w}_{g_{B}}\) was fixed to the identity during the initial learning phase as desribed above. Although we don't investigate this finding thoroughly we suspect it is because the network has an easier time learning the ring attractor since the MEC inputs are already unimodal. With the identity mapping, a tidy activity bump already on the MEC cells before the rest of the ring attractor connectivity is learned, providing a good starting point. Note this matches the standard set up for studies of path integration in, for example, Vafidis et al. [35]). This, perhaps, leads to a ring attractor which is more deeply embedded into the MEC recurrent connectivity structure and which can therefore more easily reestablish itself after a remapping. Nonetheless we discover that MEC is able to learn a significant portion of the bump attractor structure during the second phase of learning even when this was not the case and \(\mathbf{w}_{g_{B}}\) was randomly initialised (\(\mathbf{w}_{g_{B}}\sim\mathcal{N}(0,1/N_{p})\)) and plastic during the initial learning, this is shown in Fig. S4. Note how, in contrast to the receptive field shown in Fig. 4b, the MEC neurons are now multimodal and additional bands of correlational structure (in addition to a global phase shift) appear after relearning. We leave it to future work to investigate this further.

bioRxiv preprint doi: [https://doi.org/10.1101/2023.12.2571286](https://doi.org/10.1101/2023.12.2571286); this version posted December 13, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/functor, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.