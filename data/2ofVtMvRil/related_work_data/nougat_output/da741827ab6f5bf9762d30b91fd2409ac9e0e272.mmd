World Models and Predictive Coding for Cognitive and Developmental Robotics: Frontiers and Challenges

Tadahiro Taniguchi\({}^{a}\)

Corresponding author. Email: taniguchi@ci.ritsumei.ac.jp

 Shingo Murata\({}^{b}\)

 Masahiro Suzuki\({}^{c}\)

 Dimitri Ognibene\({}^{d,e}\)

 Pablo Lanillos\({}^{f,s}\)

 Emre Ugur\({}^{h}\)

 Lorenzo Jamone\({}^{f}\)

 Tomoaki Nakamura\({}^{f}\)

 Alejandra Ciria\({}^{k}\)

 Bruno Lara\({}^{f}\)

 Giovanni Pezzulo\({}^{m}\)

\({}^{a}\)Department of Information Science and Engineering, Ritsumeikan University,

1-1-1 Noji-Higashi, Kusatsu, Shiga, Japan

\({}^{b}\)Keio University, Japan

\({}^{c}\)The University of Tokyo, Japan

\({}^{d}\)Universita Milano-Bicocca, Italy

\({}^{e}\)University of Essex, UK

\({}^{f}\)Donders Institute for Brain, Cognition and Behaviour, Netherlands

\({}^{g}\)Cajal International Neuroscience Center, Spanish National Research Council, Spain.

\({}^{h}\)Bogazici University, Turkey

\({}^{i}\)Queen Mary University of London, UK

\({}^{j}\)The University of Electro-Communications, Japan

\({}^{k}\)National Autonomous University of Mexico, Mexico

\({}^{l}\)Universidad Autonoma del Estado de Morelos, Mexico

\({}^{m}\)Institute of Cognitive Sciences and Technologies, National Research Council of Italy, Italy

###### Abstract

Creating autonomous robots that can actively explore the environment, acquire knowledge and learn skills continuously is the ultimate achievement envisioned in cognitive and developmental robotics. Importantly, if the aim is to create robots that can continuously develop through interactions with their environment, their learning processes should be based on interactions with their physical and social world in the manner of human learning and cognitive development. Based on this context, in this paper, we focus on the two concepts of _world models_ and _predictive coding_. Recently, world models have attracted renewed attention as a topic of considerable interest in artificial intelligence. Cognitive systems learn world models to better predict future sensory observations and optimize their policies, i.e., controllers. Alternatively, in neuroscience, predictive coding proposes that the brain continuously predicts its inputs and adapts to model its own dynamics and control behavior in its environment. Both ideas may be considered as underpinning the cognitive development of robots and humans capable of continual or lifelong learning. Although many studies have been conducted on predictive coding in cognitive robotics and neurorobotics, the relationship between world model-based approaches in AI and predictive coding in robotics has rarely been discussed. Therefore, in this paper, we clarify the definitions, relationships, and status of current research on these topics, as well as missing pieces of world models and predictive coding in conjunction with crucially related concepts such as the free-energy principle and active inference in the context of cognitive and developmental robotics. Furthermore, we outline the frontiers and challenges involved in world models and predictive coding toward the further integration of AI and robotics, as well as the creation of robots with real cognitive and developmental capabilities in the future.

**Keywords:** World model; cognitive robotics; predictive coding; free-energy principle; active inference; deep generative modelsIntroduction

How can we develop robots that can autonomously explore the environment, acquire knowledge, and learn skills continuously? Creating autonomous cognitive and developmental robots that can co-exist in our society has been considered an ultimate goal of cognitive and developmental robotics and artificial intelligence (AI) since the inception of these fields. Autonomous robots that can develop in the real world and collaborate with us may also be called embodied artificial general intelligence (AGI). The recent success of artificial intelligence depends primarily on large-scale human-annotated data. However, human infants can acquire knowledge and skills from sensorimotor information through physical interactions with their environment and social interactions with others (e.g., their parents or caregivers). Importantly, the aim is to build robots that can continuously develop through embodied interactions, their learning process must be strongly based on their own sensorimotor experiences. This autonomous learning process that occurs throughout development is also referred to as continual or lifelong learning [1, 2, 3], and is considered the foundation for the emergence of both individual and social abilities necessary for robots with adaptive and collaborative capabilities.

Recently, _world models_ have attracted renewed attention in artificial intelligence [4, 5, 6, 7]. Now, the term "world" does not indicate the objective world but rather refers to a world understood from a robot's point of view1. This idea corresponds to that of _Umwelt_ proposed by Uexkull [9]. Umwelt, literally around-world, meaning environment or surroundings, refers to the self-centered world of an organism perceived through its species-specific sensors2. Therefore, notably, the world model is different from the bird's-eye model of the world that was aimed to build in good-old-fashioned AI and criticized later [12]3. A cognitive system learns a world model to predict its future sensory observations better and optimize its policies, also referred to as controllers. Note that although typically the term "world model" is used to denote the spatiotemporal dynamics of the external environment, it could also equally apply to bodily dynamics (including interoceptive signals from inside the body) and the social environment. This world-model view entails previous ideas and results, such as the effect of behavioral feedback on sensory sampling and perceptual learning [14] and the resulting acquisition of self-centered, yet efficient, representations induced by an active perception strategy on the part of an agent [15].

Footnote 1: This viewpoint may be called a robot’s subjective point of view of the world. Philosophically, however, whether robots can have a “subjective” point of view remains controversial [8]. Therefore, we describe this point of view simply as “a robot’s point of view”.

Footnote 2: Importantly, the relationship between Umwelt and world modeling was suggested in semiotics. Sebeok pointed out that the closest equivalent of Umwelt in English is “model” [10]. An Umwelt is created and constructed through a functional cycle, which includes 1) anticipation of a perceptual cue, 2) perception, 3) working out a relation between the perception and action (either simply executing a habit or using representation, or modeling (new), and 4) action (operation) [11].

Footnote 3: Also, notably, the world-model approach is different from behavior-based robotics [13], which does not learn world models.

_Predictive coding_ is another related theory that recently has become more and more influential [16]. It is heavily influenced by Helmholtz's early theories of perception as a process driven by learning, knowledge, and inference [17]. Predictive coding proposes that the brain infers the external causes of sensations by continuously predicting its input through top-down signals and adapts to minimize prediction error [18, 19]. This substantiates the idea that the brain might use an adaptive world model to support perception. The free energy principle (FEP) also proposes a similar vision. It argues that our brain supports both perception (perceptual inference) and action (active inference) using a form of variational Bayesian inference; in particular, using (variational) free energy, it assesses the quality of the prediction and its conformity to prior beliefs [20]. These ideas, which are currently influential in neuroscience and cognitive science, are also used in cognitive and developmental robotics, neurorobotics [21, 22, 23], and artificial intelligence to develop neurodynamics realizing adaptive behaviors and social perception [24].

Although such a learning-driven world model-based approach is promising in cognitive and developmental robotics, the many applications and studies of world models tend to be limited to simulation studies or adopt an offline pretrained world model [25]. Meanwhile, many studies based on predictive coding have been conducted in the field of cognitive robotics and neurorobotics. However, the relationship between the world model-based approach in AI and the predictive coding-based approach in robotics has rarely been discussed in an integrated manner. We believe that clarifying the definition,relationship, current state of the art, notable research gaps in work on world models, predictive coding, free-energy principle, and active inference in the context of cognitive and developmental robotics is important for further progress in this field. Based on the current status, we elucidate the frontiers and challenges toward this holy grail in cognitive and developmental robotics.

In this survey paper, we aim to build bridges and clarify the challenges and frontiers of world models and predictive coding in cognitive robotics. The remainder of this paper is structured as follows. Section 2 provides a working definition of each key concept. Section 3 describes prior works related to the concepts and clarifies state of the art. Section 4 describes some notable challenges. Some additional discussion is provided in Section 5, and we conclude the work in Section 6.

## 2 Working definition

### World model

World models describe the _internal models_ of an agent, which encodes how world states evolve, respond to agents' actions, and relate to a given sensory input [4, 26]. The term _world model_ dates back to the beginnings of artificial intelligence and robotics [27]. Early research in machine learning studied how an agent could independently acquire and adapt a world model to [28, 29]. Currently, it usually refers to predictive models [30], which are mainly encoded using deep neural networks.

In recent years, advancements in the studies on deep neural networks have enabled self-supervised (or unsupervised) learning4 of large-scale world models directly from observations (sensory inputs) [26, 31, 32], and these models have been applied in various areas of artificial intelligence, including

Figure 1: Overview of challenges and relationships between topics described in this survey. A robot, similar to a human, receives _sensations_\(x\), infers _internal states_\(z\), exhibits _actions_\(a\), and affects _causes_\(E\) in the social and physical environment. The initial problem is determining the models and architecture that a robot uses to efficiently and effectively learn _latent representations_. An approach to this problem uses a _neuro-symbolic predictive model_, which combines neural network and symbolic models. The notion of _object affordance_ highlights the importance of object-centric representation learning and the coupling of action and perception. _Social interaction_ with other agents is also an important area of research. Developing artificial intelligence for cognitive and developmental autonomous robots based on knowledge of neuroscience, i.e., _brain-inspirebrotherd models_, is promising. Creating _cognitive architecture_ and developing and sharing software frameworks for this purpose will also be an important frontier.

reinforcement learning (RL). World models allow agents to perform a sample-efficient prediction of the present state of the world and enable the prediction of future states, which further enables efficient planning (equivalent to model-based reinforcement learning or control). A compact internal representation further enables planning in an efficient low-dimensional space.

The key elements of world models are _prediction_ and _inference5_. _Prediction_ is the probabilistic process of generating the _observation_\(x\) given the _state_ (or _representation_) \(z\), whereas _inference_ is the process of obtaining a state representation \(z\) from an observation \(x\) in a probabilistic manner. In real-world settings, observations \(x\) are large (high-dimensional, e.g., images) and provide only partial information about the world (partial observability). At the same time, the latent representation \(z\) is assumed to represent the internal state of the world. In a static case, these can be summarized as the generative processes of probability distributions as follows.

Footnote 5: The use of these terms is sometimes incongruent in the literature on statistics and machine learning; the word _inference_ is also used to describe prediction or substituted by other concepts, such as _encoding_ and _decoding_ in the literature on (variational) autoencoders.

\[\text{Prediction:}\hskip 28.452756ptx \sim p(x|z)\] \[\text{Inference:}\hskip 28.452756ptz \sim q(z|x), \tag{1}\]

where \(p\) is a generative model and \(q\) is an inference (or recognition) model6. These models are considered parameterized in deep neural networks. When these models are trained simultaneously, generative approaches (e.g., variational autoencoders [38]) are employed [26]. There are also cases where only an inference model is trained, in which case a discriminative approach (e.g., contrastive learning [39]) is used7.

Footnote 6: In classical formulations of control theory [33], AI [34], and probabilistic robotics [35], the inference step is performed by exactly inverting the prediction probability \(p\) using the Bayes theorem. However, this poses several computational challenges and is often intractable. For dealing with such complexity, sampling-based approximate inference can be performed using Monte Carlo methods [36, 37]. In the context of world models and predictive coding, variational Bayes approaches are often preferred [20]. Variational Bayes involves the definition of an approximate inference distribution \(\hat{q}\). Amortized inference allows us to approximate the \(q(x)\) using a neural network, which is called an inference network, and obtain an inference model \(q(x|\alpha)\)[38].

Footnote 7: However, recently, studies have also shown that contrastive learning can be interpreted as a generative approach [40]. Therefore, the boundary between generative and discriminative approaches is, to a certain extent, blurred.

In the most common conditions, the state of the environment (and agent body) and the observations evolve over time in response to the agent's actions. In such cases, the state \(z\) is often assumed to satisfy Markov conditions. In turn, due to typical sensory limitations such as limited field of view or occlusions, the environment is assumed to follow a partially observable Markov decision process (POMDP) [34]. That is, when the current internal state of the environment is \(z_{t}\) (where the subscript represents a discrete time step), performing an action \(a_{t}\) causes the internal state to transition to \(z_{t+1}\) and the corresponding \(x_{t+1}\) is observed. In the POMDP case, the prediction and inference models are given as follows.

\[\text{Prediction (transition):}\hskip 28.452756ptz_{t} \sim p(z_{t}|z_{t-1},a_{t-1})\] \[\text{Prediction (generation):}\hskip 28.452756ptx_{t} \sim p(x_{t}|z_{t})\] \[\text{Inference:}\hskip 28.452756ptz_{t} \sim q(z_{t}|x_{1:t},a_{1:t-1}), \tag{2}\]

where \(x_{1:t}\) denotes the set of observations from the first step (\(x_{1}\)) to step \(t\) (\(x_{t}\)). To learn a state space model (SSM) on the time interval 1 to \(T\), a variational approach can be adopted that maximizes the following objective [6, 30, 41]:

\[\log p(x_{1:T}|a_{1:T-1})\] \[\geq\sum_{t=1}^{T}\underbrace{\mathbb{E}_{q(z_{t}|x_{1:t},a_{1:t -1})}[\log p(x_{t}|z_{t})]}_{\text{(Negative) prediction error}}-\underbrace{ \mathbb{E}_{q(z_{t-1}|x_{1:t-1},a_{1:t-2})}[D_{\text{KL}}[q(z_{t}|x_{1:t},a_{1 -1})||p(z_{t}|z_{t-1},a_{t-1})]]}_{\text{Regularization}}\equiv\sum_{t=1}^{T}L_{t}, \tag{3}\]where the first term in Eq. (3) represents the prediction error (or reconstruction error) of the observation, and the second term represents the regularization for the state representation (so that the transition model and the inference model yield the same state representation).

### 2.2 **Predictive coding and the free-energy principle**

The original predictive coding model provided by Rao and Ballard [18] was proposed as a model of visual processing in the brain. The model assumes a hierarchically organized neural network, and top-down and bottom-up interactions at each hierarchical level are considered. In the top-down process, higher levels generate predictions about lower-level neural activities, and the lowest level generates sensory predictions. In the bottom-up process, residual errors between the predictions and actual activities (or sensory inputs) are computed and used to correct the originally generated predictions at each level. Predictive coding models learn spatial and temporal statistical regularities at each level for efficient coding and to reduce the redundancy of the predicted activity of lower levels [42, 43]. The main principle behind this hierarchical predictive coding cortical organization in the brain is prediction error minimization (PEM) [19, 44, 45]. This idea based on the principle of PEM has been extended to various cognitive processes, and this framework is usually referred to as predictive processing [16, 46, 47]. This approach is being recently used also in machine learning to learn robust generative models of data [48].

The principle of PEM can be situated within a more general principle of free-energy minimization because the amount of variational free energy, the core information measure used in the FEP, can be understood, under simplifying assumptions, as the amount of prediction error [19, 49, 45]. The variational free energy \(F_{t}\), which is an upper bound on the surprise \(-\log p(x_{t}|x_{1:t-1},a_{1:t-1})\), is the negative value of the evidence lower bound (ELBO) \(L_{t}\) introduced in Eq. (3) as follows.

\[-L_{t} =F_{t}\] \[=\underbrace{-\mathbb{E}_{q(z_{t}|x_{1:t-1})}[\log p(x_{t}|z_{t} )]}_{Prediction\ error}+\underbrace{\mathbb{E}_{q(z_{t-1}|x_{1:t-1},a_{1:t-2})}[D_ {\text{KL}}[q(z_{t}|x_{1:t},a_{1:t-1})||p(z_{t}|z_{t-1},a_{t-1})]}_{Regularization}\] \[=\underbrace{D_{\text{KL}}[q(z_{t}|x_{1:t},a_{1:t-1})||p(z_{t}|x_{ 1:t},a_{1:t-1})]}_{Divergence}-\underbrace{\log p(x_{t}|x_{1:t-1},a_{1:t-1})} _{Evidence}\] \[\geq\underbrace{-\log p(x_{t}|x_{1:t-1},a_{1:t-1})}_{Surprise}. \tag{4}\]

From the second line of Eq. (4), when observations are assumed to follow a Gaussian distribution with a fixed variance, minimizing the variational free energy is equivalent to minimizing the sum of mean squared errors and a regularization term.

The FEP is a mathematical formulation of how self-organizing systems, such as biological agents, brains, and cells, are able to maintain an equilibrium with their environment by means of minimizing variational free energy, or the surprise associated with sensations8[44, 50, 51]. In the FEP, different cognitive processes such as perception and action can be understood as different ways to minimize the variational free energy in terms of probabilistic inference called active inference [52, 53, 54] as detailed in the next subsection.

Footnote 8: In contrast to states of surprise, free-energy can be measured because it is a function of sensory input and the inferred state [45]

### 2.3 **Active inference and exploration**

Active inference is a normative framework that derives from the FEP and provides a unifying account for perception, control, and learning in terms of minimization of the variational free energy in the past,present, and future. This unification is important in neuroscience as it reflects neural mechanisms and on a computational level because it offers new perspectives and the possibility of sharing algorithmic solutions between all these functions and transforming them into sophisticated robotic behaviors [23]. For example, perception aims to minimize the variational free energy in the past and present by inferring the latent representations of observed sensory inputs9, e.g., when an orange appears in the field of view instead of the apple as currently encoded in the internal representation, the representation state can change toward that of an orange [19]. Conversely, actions try to minimize the variational free energy in the present by actively sampling sensory inputs, e.g., by moving the gaze away from the orange toward an apple. In addition to selecting an action in the present, agents can infer a sequence of future actions (or policy) that elicit the most plausible future states [53, 54, 55, 56] by considering the minimization of _expected_ free energy, as detailed below.

Footnote 9: This perception can be regarded as a variational Bayesian version of the original predictive coding that employs a maximum a posteriori (MAP) estimation [44].

While active inference introduces an important perspective towards an understanding of adaptive and autonomous behaviors, an obvious behavioral imperative, the exploration-exploitation dilemma, seems in conflict with this idea because exploration, i.e., observing an uncertain aspect of the environment, would result in obtaining an unpredictable outcome [57, 58]. Indeed exploration and active perception have a central role in robot control and learning. Several tasks focus on robots' ability to explore an unknown environment [59, 60, 61]. Furthermore, in social contexts, unobservable factors such as others' intentions must be actively considered to allow for efficient human-robot collaboration [62, 63, 64, 65].

However, in [20, 54, 66], the authors showed that active inference can easily support exploratory behaviors and that it can provide an elegant formal solution for the exploration-exploitation dilemma.

In fact, we must consider that planning behaviors for an extended period of time require anticipating future data. More specifically, to infer the best action sequences (policies), one must also predict the future observations they would produce. This is realized in the active inference framework by minimizing the expected free energy over a time interval \(T\). We can express this as the sum of two terms, including **i)** the variational information gain term [62, 67, 68, 69], or _epistemic_ value [54], defined as the expected KL divergence between the distribution of the latent states conditioned on the expected observations \(q(z_{t+1:T}|x_{t+1:T},a_{t:T-1})\) and the prior distribution on the latent states \(q(z_{t+1:T}|a_{t:T-1})\) that represents the reduction in uncertainty on the latent states \(z_{t+1:T}\) provided by the expected observations \(x_{t+1:T}\), and **ii)** the extrinsic or _pragmatic_ value \(\log p(x_{t+1:T}|C)\), where \(C\) denotes the agent's preferences. This results in the following expression10.

Footnote 10: Note that in the literature on active inference, a sequence of actions \(a_{t:T-1}\) is referred to as a policy \(\pi\). This policy is different from a policy in reinforcement learning, where it represents a statistical mapping from states to actions (\(\pi(a_{t}|y_{t})\)). Using this notation of the policy \(\pi\) and mean-field approximation, the general formulation of the expected free energy can be described by the following more practical formulation. \(G(\pi)=-\sum_{t:t+1}^{T}\mathbb{E}_{q(z_{t}|z)}|D_{\mathrm{KL}}[q(z_{t}|x_{t},\pi)||q(z_{t}|\pi)]|-\mathbb{E}_{q(z_{t}|\pi)}|\log p(x_{t}|C)|\), where the time step \(\tau>t\) used here is a future time step.

\[G(a_{t:T-1})=-\underbrace{\mathbb{E}_{q(z_{t+1:T}|a_{t:T-1})}[D_ {\mathrm{KL}}[q(z_{t+1:T}|x_{t+1:T},a_{t:T-1})||q(z_{t+1:T}|a_{t:T-1})]]}_{ Epistemic value}\] \[-\underbrace{\mathbb{E}_{q(x_{t+1:T}|a_{t:T-1})}[\log p(x_{t+1:T} |C)]}_{Pragmatic value}. \tag{5}\]

The epistemic value term favors obtaining observations that disambiguate the world state such as obtaining the address for the best apple shop in town, versus observations that correspond to multiple (aliased) state such as corridors in a mall. Without the factor of variational information gain, asking the address of the shop would not be preferred to any other action that would not immediately result in obtaining an apple. Thus, minimizing expected free energy corresponds to maximizing the sum of epistemic and pragmatic values over an extended period and defines the optimal trade-off between exploration and exploitation. The similarity between the epistemic value term in Eq. (5) and the divergence term in Eq. (4) with an inverted sign may be noted. This is due to the different role that observations play in expected free-energy formulation, where they comprise not observed data but expected observations. Finally, the close connection between variational free energy (Eq. (4)), expected free energy (Eq. (5)), used in this context to define behaviors with exploration capabilities, and ELBO (Eq. (3)), used to model learning processes objectives, shows the versatility of this type of formulation, the extension and refinement of which currently a promising field of research that aims to develop an autonomous system with the ability to efficiently acquire and execute complex skills [69]. For a more advanced and detailed presentation, we refer to [20, 54, 69, 70].

Another important framework that considers behaviors as inference is planning or _control as inference_ (Cal) [71, 72, 73, 74, 75]. The main difference between CaI and active inference is that CaI introduces a binary optimality variable \(O_{t}\) that represents whether an action \(a_{t}\) in state \(z_{t}\) is optimal (or preferred) [75, 76]. If the reward for taking action \(a_{t}\) in state \(z_{t}\) is \(r\left(z_{t},a_{t}\right)\), the conditional distribution of the optimality variable is defined as follows.

\[p\left(O_{t}=1\mid z_{t},a_{t}\right)\equiv\exp\left(r\left(z_{t},a_{t}\right) \right). \tag{6}\]

Thus, unlike active inference, CaI can introduce the value of the reward at each time explicitly and independently of the observation's generative model11.

Footnote 11: Therefore, unlike active inference, CaI does not require the assumption of POMDP.

Cal aims to obtain the optimal policy \(p(a_{t}|z_{t})\) for inference. If the variational inference is chosen as a solution to the intractability of exact inference (as with active inference), we seek the policy that maximizes the following ELBO12.

Footnote 12: Here, MDP is assumed, i.e., state \(z_{t}\) is an observed variable rather than a latent variable; if POMDP is assumed, inference and generative models for observations are added to this ELBO.

\[\log p(O_{1:T})\geq\mathbb{E}_{\prod_{t=1}^{T}p(a_{t}|z_{t})p(z_{t}|z_{t-1},a _{t-1})}\left[\sum_{t=1}^{T}r\left(z_{t},a_{t}\right)+H\left(p\left(a_{t}\mid z _{t}\right)\right)\right], \tag{7}\]

where \(H\) represents the entropy. This corresponds to the entropy-regularized expected reward, and reinforcement learning with this as the objective is called entropy-regularized reinforcement learning [77].

## 3 Prior works

### World models and model-based reinforcement learning in AI and robotics

In this section, we describe world models used in _model-based reinforcement learning_ in the context of artificial intelligence and robotics.

Time-series world models conditioned on behavior have been studied for policy learning for some time. Schmidhuber proposed learning an agent's policy (utility) via an RNN-based world model obtained

Figure 2: Researches of world models. Left: Dreamer [30] and Dreaming [78]. Right: robot control system using NewtonianVAE [79].

by self-supervised learning [28]. Based on this idea, Ha et al. introduced a large-scale world model consisting of a VAE and an RNN that learned directly from observations (time-series images) from the external world [5]. They showed that the policies of agents trained only on this world model, which learns a game environment, can work properly in real game environments. Since this study, research has been conducted on self-supervised learning of models of an environment directly from observations, along with ideas referred to as "world models". However, the authors trained spatial compression (VAE) and temporal transitions (RNN) separately; thus, the perspective of learning state representations was not considered.

Subsequently, VAE-based models that simultaneously learn time transitions and spatial compression have been proposed. Kaiser et al. proposed a VAE-based world model with discrete latent variables designed to predict the next frame and reward from the stacked frames of the previous four steps and the current action and showed that model-based reinforcement learning using this model performed adequately in an Atari video game environment with high sample efficiency [80]. One limitation of this model is that it does not include RNNs and cannot account for long-term prediction. Moreover, the measures are learned from the observation space, so the learned representation is not fully exploited. Ke et al. showed that learning long-term transitions using a stochastic RNN-based world model contributes to high performance on tasks that require long-term prediction [81]. All of these models, however, are autoregressive, requiring the generation of a high-dimensional observation space every step for long-term prediction, and are unable to transition within the latent space.

Recently, models that learn transitions in latent space without requiring autoregressive generation have been widely used. Hafner et al. introduced a recurrent state space model (RSSM) that includes RNNs in SSM and showed that it could be used for long-term prediction and model-based reinforcement learning with higher performance than model-free learning [41].

While PlaNet [41], the first study using RSSM, used an existing model-based planning method (cross-entropy method) for planning in the latent space13, Dreamer [30], a subsequent method, explicitly modeled the policy and value function in neural networks and learned a world model through gradients in an actor-critic framework, resulting in a better performance than PlaNet. This model has been further developed by replacing the latent variables with discrete values, which significantly outperformed model-free performance in Atari game environments (Dreamer V2 [6]), and by using contrastive learning instead of reconstruction, which resulted in higher performance on tasks that were difficult to reconstruct (Dreaming [78], see the left side of Fig. 2). They were also combined and compared (Dreaming V2 [83]).

Footnote 13: PlaNet was extended to be uncertainty-aware on the basis of Bayesian inference [82].

In terms of obtaining a good state representation for control, enforcing explicit constraints on transitions is preferable. For example, NewtonianVAE was able to form PD-controllable state space [84]. However, to develop such a model, what kind of state representation the world model should acquire (as a good representation for control) should be considered, which remains as yet relatively unclear (see section 4.1 for details).

These world models have been shown to be effective in learning using real robots. Okumura et al. successfully applied the NewtonianVAE to a robot and enabled it to perform a precise socket insertion task [79](see the right side of Fig. 2). Wu et al. showed that Dreamer V2 enabled real robots to perform online learning with very high sample efficiency and performance, which includes a pipeline of acquiring data through interaction with the external world, learning a world model, and controlling the robot using the model [7]. However, all of these results are for a single environment and task, and what kinds of world models should be acquired for robot control in diverse environments and tasks remains unclear.

### **Predictive coding and active inference in cognitive and developmental robotics**

In recent years, an increasing body of research has considered predictive coding models for perception and action in robotics. Recent comprehensive reviews on active inference and predictive processing in robotics can be found in [21, 23], respectively. These ideas aim to provide a general mathematical account of behavior. Importantly, they incorporate adaptation and robustness to current methods in cognitive and developmental robotics.

Since the early works of Tani et al. using hierarchically organized RNNs [85], a variety of methods have been proposed to exploit this idea of prediction-error-minimization or propagation. "Higher levels" (internal representation) generate predictions about the dynamics of the "lower levels" up to the sensorimotor level. Prediction errors at the sensorimotor level, given the observations, are then propagated "upwards" in the hierarchy correcting the internal state and thus minimizing the errors. Extensions of Tani's approach allow multiple time scales [86, 87, 88, 89] (see the left side of Fig. 3), stochasticity [90, 91] and stochastic latent representations [92]. In particular, a precision-weighting mechanism for the PEM enabled robots to extract stochastic or fluctuating structures of temporal sensorimotor sequences and utilize the extracted structures for their action generation [91]. Interestingly, this mechanism is related to the precision account in psychiatric disorders [93] (especially autism spectrum disorder [94, 95]) and several works have proposed cognitive robot models based on aberrant-precision to model unusual perception and action [96, 97, 98].

Aside from hierarchical RNNs, active inference controllers for robotic manipulators [100, 101] and humanoid robots [22, 99] have also been developed based on Lanillos's initial work on predictive coding adaptive perception and learning [102, 103] for both low-dimensional and high-dimensional inputs (see the right side of Fig. 3). These methods have widespread applications such as object manipulation [104], imitation [104], language acquisition [105], social interaction [106] and navigation [107].

Cognitive robots benefit from predictive coding mechanisms to infer others' actions [108]. The reuse of common circuits for both movement generation and action estimation seems to be a key principle in the sensorimotor organization. Recently, the authors of [109] proposed deep modality blending networks (DMBN) designed to create a common latent space from the multi-modal experience of a robot by blending multi-modal signals with a stochastic weighting mechanism. Using a state-of-the-art skill-encoding system referred to as Conditional Neural Movement Primitives (CNMPs) [110], they showed that deep learning could facilitate action recognition and produce structures to sustain anatomical (mirror-like) and effect-based imitation capabilities when combined with a novel modality-blending scheme.

Current state-of-the-art research is focusing on scaling active inference in planning tasks [51] with high-dimensional inputs [32, 111] and improving representation learning through multimodal common latent space [109] or introducing structural inductive biases, such as objects [112]. Whilst active inference is a promising framework for robotics [113], current works are still limited to a particular aspect of cognitive and developmental processes. Therefore, in addition to extending the scalability of computational frameworks, continual or lifelong learning for developing abilities from low-level sensorimotor skills to higher-order cognitive functions should also be considered.

Figure 3: Research on predictive coding and active inference in cognitive robotics. Left: adaptation to environmental changes by prediction error minimization [89]. Right: body perception and action by pixel-based deep active inference [99].

## 4 Frontiers and Challenges

### Latent representations for action planning

One of the most important challenges in world-model approaches of any kind is that of efficiently performing planning, in the sense of generating meaningful actions to solve a sequential task [114]. Working in the high-dimensional space of the sensorimotor manifold is very computationally expensive and provides local optima solutions [99]. In fact, current approaches in planning use a compressed encoded representation of the world dynamics, which aids in the process of predicting future states and in action generation [26]. In reinforcement learning, state representation learning is tied to learned tasks to achieve high performance because it depends on the actions needed to obtain the maximum expected reward [30]. However, this sometimes prevents generalization across tasks. Decoupled action-representation world models are an interesting work-around [39]. In deep active inference [23] amortized methods have also been considered [32, 115], in addition to contrastive [116] and iterative amortized inference approaches [112].

However, the key question cannot be narrowed down to that what type of architecture or method should be used. Rather, what type of information should be encoded in the latent representation and how this information is processed must be a key focus so that information is not uncoupled from the sensorimotor process, particularly from motor control, which is a key limitation of existing endeavors in robotics. There has been considerable discussion as to what would comprise an appropriate state representation of a world model; that is, what inductive bias or prior knowledge should be given [117, 118, 119]. Here, we list the properties of this prior knowledge we consider important.

* **Low dimensionality.** Observations obtained from the environment are high-dimensional, and compressing this information into a low-dimensional space is critical for efficient data handling, abstraction, and planning. This approach is the most frequently considered in state representation learning. The challenge is how best to represent observations in a low-dimensional encoding while retaining the necessary task-dependant information. Recent literature focused on generative and discriminative approaches to tackle this.
* **Meaningful abstraction and disentanglement.** Low-dimensional representations should have scene-understanding and task meaning, such as objects [112], locations [120] and temporal events [121]. Representation disentanglement proposes that factors of variation with different semantics should be separated, contributing to the requirements for sufficiency and efficiency in state representation. Object-centric representation learning is related to this hypothesis, [112, 122], in which every observed object is encoded independently.
* **Compositionality.** Although disentanglement aims to separate independent factors, the agent should also acquire their relationships and hierarchy. In the case of object representations, there should also be relations or implication relations among objects. Currently, methods such as those using graph neural networks are being considered, but they do not provide an essential solution. This idea of the compositionality of representation is also relevant to the neuro-symbolic approach.
* **Dynamics prediction.** These three properties are important not only for learning representations in static environments but in dynamic worlds, e.g., they consider a transition model that depends on external factors and agent actions. The best latent representation is one that allows transitions to be easily predictable for given actions. Many recent models use RNNs to learn transitions, which incorporate information on long-term dependence [30, 41]. One way to make transitions more predictable is to incorporate prior knowledge of the physical world (e.g., dynamics following Newton's laws of motion [84]). Furthermore, by learning to separate representations that are not related to control from state representations, representations that are easier to control can be acquired [123].
* **Values are sufficiently encoded.** To perform reinforcement learning on the state representation of the world model, the value of the state representative to the agent must be known. For example, a recurrent state representation learns to predict the reward from the state so that the reward is embedded in the representation [30]. However, because the value of the state changes dependingon the task, it remains unclear whether this hypothesis should be introduced in a world model that should acquire a prediction model that is as task-independent as possible. Alternatively, in active inference approaches, the agent value function cannot be modified, and it is defined by expected free energy. Here, the challenge becomes learning the state preferences and being able to predict the transitions that may yield those preferences.
* **Task-agnostic.** Representations should be informative to solve narrow problems where the agent is trained but also sufficiently general to be reused in tasks with different kinds of variability or new tasks that the agent has never encountered.
* **Fusion of multiple-types multimodal information.** Robots inevitably face a variety of events with their multimodal sensorimotor systems. Observations given to world models are from multiple sources (e.g., social non-social, sensorimotor purely sensorial, and linguistic and non-linguistic). They can have different reliability and volatility and represent various aspects of the world. Therefore, the world models must properly encode the internal representation in a stable and efficient manner.

These are some of the elements that we identified that a latent representation should be fulfilled to provide a smooth connection with real-world interaction and provide power for solving cognitive tasks. Importantly, abstract representation and disentanglement, such as objects or events encoding, may be important to achieve efficient planning, reducing the gap for neuro-symbolic solutions. However, the connection between the low-dimensional (and hierarchical) encoding and the synchronization with the sensorimotor control remains a major challenge.

### **Neuro-symbolic predictive models**

In this section, we provide an overview of state-of-the-art techniques in which symbols and rules are discovered and used by robots through neuro-symbolic approaches. The term _symbols_, here, refers to manipulative discrete representations used in symbolic AI and cognitive science. The neuro-symbolic approach attempts to integrate conventional symbolic and modern neural network-based AIs.

Both biological and artificial agents benefit from predictive coding mechanisms for reasoning, decision-making, and planning. Predictive forward models are used to generate plans that involve a sequence of actions. For example, chimpanzees are known to generate multi-step plans that include stacking a number of boxes on top of each other, grabbing a long stick, climbing on top of a stack of boxes, and using the stick to reach the object that was initially out of reach [124, 125]. While the underlying cognitive mechanisms for high-level planning remain unknown, different specific brain regions have been shown to become active in inductive and deductive reasoning in humans [126] while predicting the effects of actions [127]. In artificial agents, on the other hand, standard search and planning rely heavily on manually coded or learned state transitions and prediction models [128, Ch. 3-6,10-11].

The seminal work of [129] addressed the learning of discrete representations of predictive models, i.e., dynamic Bayesian networks, by discretizing the continuous features of the environment to plan goal-directed arm/hand control. [130] showed the units generated by slow feature analysis with the lowest eigenvalues resemble symbolic representations that highly correlate with high-level features, which might be considered precursors for fully symbolic systems. [131, 132] studied methods to discover useful symbols that can be directly utilized in problem and domain definition language (PDDL) for various agent settings. In simulation and the real world, the discovered symbols were directly used as predicates in the action descriptions to generate deterministic and probabilistic symbolic plans. [133] learned symbols in the ego-centric frame of the agent to transfer the learned symbols into novel settings. [134, 135] discovered symbols in the continuous perceptual space of the robots for PDDL-based manipulation planning via combining several machine learning algorithms such as X-means clustering and support vector machine (SVM) classification. Although the symbols were discovered by the robot without any human intervention, the continuous perceptual features were manually encoded by the authors. Towards an end-to-end framework, [136] used directly raw camera image and pixel values to discover symbols via a novel deep predictive coding neural architecture. In detail, they proposed a deep encoder-decoder network with a binary bottleneck layer designed to take a camera image and an action as input and output the action 

[MISSING_PAGE_FAIL:12]

performed. Therefore, it is not surprising that the computational techniques used for learning affordance models often overlap with those used for learning world models [164]. It is worth noting that, in world model approaches, a robot only receives raw sensory information and needs to extract the relevant semantics from such data flow; therefore, to successfully integrate affordance perception in these systems, the challenges of meaningful abstraction/disentanglement and object-centric representation learning, described in Section 4.1, are particularly relevant.

### Social interaction

Robots' "worlds" do not consist of physical objects alone but also of social entities, i.e., people who give them social guidance and try to cooperate with them. World models should model and predict social dynamics involving people's behaviors, and infer their latent variables, e.g., intentions and emotions, to cooperate with them, that is, to control social phenomena.

Efficient and safe human-robot collaboration and interaction are some of the main research objectives of robotics and have important practical applications [165, 166, 167, 168, 169, 170, 171]. Associating beliefs, intentions, or mental states to other agents, theory of mind, or, in other words, trying to predict the internal state of another agent's world model to understand its activities and context [172], is an essential aspect of human interaction [173, 174, 175] and has attracted attention in robotics [176].

Mutual understanding using a world model in social interaction can play an important role when complex interactions are challenging the perceptual systems of the agents, inducing a mismatch between their interpretation of the current context [177, 178]. It is also crucial when different levels of knowledge and expertise induce different representations of a domain, as well as different points of view, which may induce conflictual interactions [179] or different support strategies [180]. For example, the perspective of an automotive mechanic and that of an ordinary user differ considerably, so collaboration may be difficult if one cannot properly infer the internal state of others. A robots' world model can play a crucial role in its operation and functionality [181].

The recent progress in machine learning methods has resulted in substantial improvement in action recognition methodologies [182, 183, 65, 184]. However, this approach has often focused on shallow and purely perceptual representations of the observed activities resulting in limited flexibility in terms of contexts, tasks, and observed actors demanding a substantial amount of difficult-to-collect data and retraining time to apply the system in relatively similar conditions [185]. Approaches such as goal recognition as planning or inverse planning [186, 187, 188, 166], that, given a model of the environment, understand others' activities by computing plans that would result in the observed actions have shown the flexibility advantage delivered in intention recognition by a world model. Several works have extended this approach. The problem of dealing with behaviors generated under partial observability, which may require inferring both the plan and the beliefs, the mental state [176, 178], of the observed actor, was studied with both classical planning [189] and Bayesian approaches [179, 190]. The impact of missing observations for the observer agent has also been analyzed [189]. A further step has been proposed by active methods for activity recognition [191, 192, 62] that use the same world model both to interpret others' actions as well as selecting actions that would improve the recognition process, e.g., by giving access to the most informative observations [63] and allow the completion of a joint task [180]. While even the initial formulations of this approach were computationally aware [186], their efficiency is often affected by the length of the observed behavior and the environment complexity, resulting in methods that can seldom be applied online on a robot. Several models proposed a pre-compile approach that transformed the world into a form that would allow efficient plan recognition [185]. The adoption of hierarchical world model representations has also been considered to constrain the computational and modeling costs of the process [193, 194, 195]. Precomputed and robust local plans, in the form of the same motor controllers that the robot uses to perform its own actions, have also been adopted to allow active perception for action recognition and prediction on humanoid robots [172, 62]. One of the main issues of the approach, also related to computational efficiency considerations, is relying on specific algorithms for planning that aiming for the optimal plan may misinterpret the bounded rational behaviors that collaborators may perform. This problem was faced by using online Bayesian inference in [196].

The additional flexibility provided by world models in social interaction skills is likely relevant beyond activity recognition. It is easy to imagine that purely supervised models may be limited in terms of perspective-taking and the ability to reason based on the world structure may help to adapt to partners with different sensory systems [176, 197, 198, 199]. Similarly, world models are likely to help with imitation learning by dealing with embodiment mismatch between the observed actor and the learner [200]. Finally, physical cooperation [201, 202] and signaling [203, 204] would also be more flexible when integrating world and partner models in the equation, for example, to account for the trust of the human cooperator towards the robot [205]. Finally, a world model may also be learned through socially rich experiences and sources of information (e.g., imitation [200] or verbal instructions) in addition to the results of autonomous exploration. However, developing a robust, efficient, and flexible enough representation may prove to be one of the main challenges in this effort.

### Brain-inspired world models

In cognitive science, it has long been postulated that the brain learns small-scale models of the world and uses these models for various cognitive functions, such as perception, planning, and imagination [206]. For example, theories of perception-as-inference described perception as an inferential process, which works by "inverting" a generative model of how the percepts are generated [207, 208]. As discussed above, these ideas (and others) have been recently formalized under the label of the _Bayesian Brain_[209] and extended by _Active Inference_ from the domain of perception to other domains, such as action planning and interoception [20].

In parallel, there have been many attempts to describe mathematically and to assess the neuronal underpinnings of world models and of inference processes empirically (e.g., [210]). One question that has received a great deal of attention is how the brain might encode internal world models in the neuronal substrate. Given that the brain models are often assumed to be probabilistic, various formal schemes have been proposed that describe plausible neuronal implementations of probabilistic variables and of Bayesian inference over these variables, such as, for example, probabilistic population codes [211] and sampling schemes [212]. These attempts show that (probabilistic, generative) world models could be at least potentially implemented in neuronal substrate [213, 214] - and even updated after statistical learning [215] - but the specific scheme(s) that the brain might use for this remain to be fully assessed.

Another relevant question is what algorithms the brain might use to perform inference over world models. A strong candidate in neuroscience is _predictive coding_[18, 19]. Several studies have aimed to validate its key empirical predictions, showing that under the appropriate conditions, it is possible to observe predictions [216], prediction errors [217] and other signatures of inference in brain signals [218] and that neural activity in lower visual areas in the absence of bottom-up inputs could be explained by the top-down, feedback dynamics postulated by predictive coding [219]. These and other studies (see [220] for a recent review) lend some support for predictive coding, but the theory remains under development.

At yet another level, one may ask what the systems-level architecture that supports world models and whether different parts of the brain might model different aspects of the world is. Anatomical considerations suggest that the brain is not a monolithic entity but rather is composed of several areas and networks [221]; however, the extent to which these areas or networks are modularized and how they exactly influence each other are heavily discussed [222]. One interesting consideration is that cortical brain areas in humans and monkeys appear to be organized along principal gradients (defined by functional connectivity); in one of these gradients, heteromodal areas (e.g., prefrontal cortex) are placed at the top, and unimodal areas (e.g., primary visual area) at the bottom, recapitulating the structure of a putative hierarchical generative model [223]. Another interesting consideration is that there seems to be a "division of labor" between brain pathways that perform complementary computations, such as the two visual pathways for processing "what" and "where" information [224]. These anatomical and functional separations might be potentially interpreted as useful _factorizations_ of the brain generative models. A whole-brain probabilistic generative model (WB-PGM) approach attempts to build a cognitive architecture for cognitive and developmental robots integrating probabilistic generative model (PGM)-based modules referring comprehensive knowledge of human and animal brain architectures and their anatomy [225].

The above studies indicate that at a general level, both neuroscience and machine learning / AI conceive world models and inference in similar ways. However, at a more detailed level, there might be profound differences between the ways these two disciplines use the same concepts. Predictive coding and other biological schemes proposed in neuroscience exploit top-down dynamics (and recurrences) in ways that are rarely used in machine learning. Furthermore, brain information processing is heavily based on spontaneous brain dynamics, which are largely absent in machine learning systems; see [226, 227, 228] for a detailed discussion of putative computational roles of spontaneous dynamics. Moreover, it is plausible to assume that different parts of the brain might be specialized (or might have different _inductive biases_) to process different statistical regularities, rendering them able to learn and model (for example) slower or faster dynamics of the visual scenes, one's own body, the actions of other agents, or extended temporal events [229]. It is worth highlighting here that, although prediction errors have a central role in learning, there are other forms of statistical learning, such as those based on Hebbian associative learning [230]. It remains to be understood how to best endow our more advanced machine learning systems with the ability of the brain to perform (apparently) specialized computations but also orchestrate them coherently. Finally, it is important to remember that the brain is an evolved system, and our more advanced cognitive abilities are grounded in (the neuronal mechanisms supporting) simpler sensorimotor skills [231, 232]. Trying to develop advanced cognitive systems without the necessary requirements for embodied interaction and "phylogenetic refinement" might lead to solutions that differ completely from how the brain works - or that fail altogether.

### Cognitive architectures

Truly cognitive and developmental robots, i.e., embodied AGI, that behave autonomously and flexibly in the real environment would have a wide range of sensors and exhibit multiple functions. That requires a large-scale world model that deals with multimodal sensory observations and multilayered state representations. Considering the discussion in Section 4.5, such word models may be factorized in a proper manner from engineering and biological viewpoints. To realize embodied AGIs, further frameworks and architectures to factorize a total world model into cognitive modules and to integrate individual cognitive capabilities into a cognitive system are required. The idea is related to _cognitive architectures_, which have been studied in cognitive science, artificial intelligence, and robotics [233, 234].

In cognitive science, cognitive functionalities such as memory, perception, and decision-making are implemented as modules in the cognitive architectures studied, and the specific task can be solved by activating these modules coordinately. ACT-R [235] and Soar [236] are representatives of cognitive architectures. It has been shown that the model implemented by ACT-R can explain the time to solve the task by humans, and activation patterns of the brain can be predicted by activation patterns of the modules [237]. Furthermore, Soar has been used for controlling robots [238] and learning games [239]. However, complex machine learning methods that have rapidly advanced in a decade are not introduced yet. Sigma [240, 241] is a newer cognitive architecture that introduces the generative flow graph, a generalized probabilistic graphical model. Therefore, the model can be implemented using probabilistic programming techniques [242, 243, 244]. Furthermore, the concept of the standard model of the mind is discussed through a synthesis across these three cognitive architectures [222]. Particularly, cognitive architectures based on first principles, e.g., with a general computation scheme, such as free energy minimization [19], are especially attractive. The architecture for social cognition has also been proposed [245]. The authors point out that these architectures explained above are incomplete in dealing with the social aspect of cognition and describe the elements of architecture for social cognition. Clarion [246] is another cognitive architecture based on dual process theory [247]. In this architecture, each subsystem is composed of explicit and implicit processes, and it is shown that the interaction between implicit-explicit processes can explain psychological phenomena.

In robotics, several types of cognitive architecture have been proposed. One of them is ArmarX [248], which has three layers, including a middleware layer, a robot framework layer, and an application layer. This three-layered structure simplifies the development robotics software easier. (Neuro-)Serket[249, 250] is another approach to integrating cognitive modules14. In (Neuro-)Serket, modules are described by the (deep) PGM and trained mutually by exchanging messages between modules. To make it easy to develop large-scale models, the modules in Neuro-Serket are weakly connected through the Serket interface. (Neuro-)SERKET is closely related to the world model-based approach because SERKET requires each module to be a PGM, i.e., a model based on prediction and inference as Eq. (1), and integrate modules into a large PGM. This architecture does not provide any restrictions regarding the functionalities of modules. Therefore, it has high flexibility but brings high dimensional design space at the same time. To reduce the large degree of freedom in the design space, a brain-inspired approach, WBA-PGM, was proposed [225]. In this approach, a cognitive model was constructed by connecting PGM-based modules utilizing knowledge from neuroscience. By referring to the brain studies, WBA-PGM constrains the function of modules and their connection and reduces the design space of the cognitive model.

Footnote 14: Neuro-SERKET is an updated version of SERKET.

There are two crucial requirements for cognitive architecture for cognitive and developmental robots, which can be used along with the approach based on world models and predictive coding. The first is the engineering aspect which is seen in (Neuro-)Serket and ArmarX. The scale of cognitive models that enables the robots to behave flexibly in the real environment is very large, and many modules must be connected and work collaboratively. Furthermore, the model needs to introduce machine learning techniques that are not only existing as well as those will be developed in rapid progress. The development of such a model would require a massive engineering effort, and this is considered a notable obstacle to realizing such robots. Therefore, architecture is needed to simplify development. Another requirement is that of the scientific aspect seen in ACT-R, Soar, WBA-PGM, and Clarion. Developing AGI, which is human-like intelligence, referring to the knowledge regarding humans obtained in cognitive science and neuroscience, can accelerate its development. However, meeting these two aspects completely is very challenging. All machine learning techniques and module connections might necessarily be not reasonable from the point of view of cognitive science and neuroscience. On the other hand, entire humans are not understood yet. Therefore, finding common ground between engineering and science aspects and developing a novel cognitive architecture is a current challenge. Developing a large-scale cognitive architecture and overcoming the problems described in the previous subsections is also a challenge.

## 5 Discussion

As we described, world models and predictive coding are promising approaches in cognitive and developmental robotics. Before closing this paper, we will mention some remaining issues which have not been addressed in the main body sufficiently.

**Language and world models**: Umwelts, i.e., worlds from first-person views, of biological systems are not monolithic but have some sort of structure. Notably, language and symbolic systems have syntactic structures. The interaction between high-level cognitive capabilities, e.g., language and reasoning, and low-level cognitive capabilities, e.g., perception and action, is essential in world modeling. Recently, large-scale language models (LLMs) have been replacing many natural language processing methods [251, 252], including reasoning tasks, which have been conducted solely by symbolic AI by the end of 2010s [253, 254]. Recently, the use of LLMs in robotics has been attempted, e.g., [255]. It is clear that language learning and understanding by robots is itself a frontier [256]. To leverage the symbolic knowledge in LLMs, integration of LLMs and world models will be an important challenge.

This shift from models of artificial symbols in conventional AI to models of natural language, i.e., a human symbol system, is resonating with the discussion in symbol emergence in cognitive and developmental systems [257, 258, 259]. An important topic is then considering not only the integration of human language into robots' world models in a top-down manner but also the bottom-up formation of symbol systems, including language in relation to world models.

**Policy representations**: How should the policies of robots be represented? Conventionally, policiesare described as feedback controllers \(\pi(z_{t},a_{t})=p(a_{t}|z_{t})\) in reinforcement and imitation learning. Even though one direction of world model approaches is to explore task agnostic representations (Section 4.1), the decomposition of world modeling and policy learning can be controversial. In a conventional approach of world models, policies (\(\pi=p(a_{t}|z_{t})\) or \(a_{t:T}\)) and world models (\(p(z_{t+1}|z_{t},a_{t})\) and \(p(o_{t}|z_{t})\)) are decoupled. In contrast, a series of studies about predictive coding in neuro-robotics have been intentionally entangling policies and world models and making robots directly learn \(p(o_{t+1},a_{t+1}|o_{12},a_{12})\) and exhibiting many successful results in robotics, e.g.,[24, 260, 261]. As the notion of affordance also suggests, actions and perceptions are not independent and entangled, generally. The question "to what extent should we decouple world model and policy representations?" should be investigated.

**From artificial cognition to human cognition**: Cognitive and developmental robotics are also constructive approaches to human developmental cognition. Not only learning from neuro-, cognitive and developmental sciences but also provide with scientific feedback to them is also an important mission. Building a virtuous circle between studies on human and artificial studies is a challenge.

The constructive approach may give us a novel approach to scientific and philosophical hard problems like self-awareness [262] and consciousness. The relationship between the multimodal world model and global workspace theory was suggested [263]. Extending the discussion between world models and consciousness using robots may be an exciting challenge. Moreover, the relationship between predictive coding and emotion is worth exploring to build emotional robots and understand the emotions of biological systems [264, 265, 266].

**Software frameworks for implementation**: To accelerate the studies on world models and predictive coding in robotics, the development framework for cognitive and developmental robotics is crucially important. In robotics, not only AI "software" frameworks but also middle-ware are important. Recently, ROS has been widely used in the robotics community for bridging hardware and AI software layers. Developing and sharing such software frameworks as a community will be important, e.g., [267]. Moreover, the world model involves many types of knowledge, and the knowledge can be used for achieving multiple functions via active inference. The software framework should allow the world model to efficiently organize the knowledge and to perform (cross-modal) active inference. A great initiative is the discrete state-space active inference python library [268]. However, it can only be used for toy examples due to scalability issues and to be useful in cognitive and developmental robotics. It needs further development. For instance, the support for high-dimensional input observations and the possibility of combining discrete and continuous action and state representations are something that has been addressed in robotic approaches [23].

**Data-efficient and autonomous learning**: A generalist agent called GATO was developed based on Transformers and shown to be able to solve various tasks with one neural network [269]. Although the approach is superficially different, the approach is really related to the world models and the predictive coding approach. However, the learning system is hugely data-hungry. It is very questionable if the model can be regarded as a model of human intelligence. Moreover, to train the generalist agent, researchers need to prepare a large dataset and simulation environment. Human children can autonomously explore their environment and acquire data through active exploration. Moreover, they use heuristics and biases in their developmental process. Learning and considering the human developmental process will give us the inspiration to build real generalist agents. Developing a data-efficient autonomous learning architecture with world models and predictive coding at its core is the key to a truly cognitive and developmental system.

**Emergence of behaviors**: Should an agent have a completely internal model of its world? Lastly, we raise fundamental speculation about the world model-based approach. Behaviors are not externalization of internally designed trajectories but something to emerge through the interaction between the body and the environment. For example, it has been proven by passive walking machines that the behavior of walking emerges only from the interaction between the body and the environment, without any computation by the brain [270]. About three decades ago, Brooks famously advocated the physical grounding hypothesis together with subsumption architecture, saying the world is its own best model [271]. The robots behaved smoothly and flexibly without any explicit world models. This is also referred to as morphological computation, which means the body itself implicitly processes information dynamically [272, 273]. Softrobotics emphasizes these points nowadays. Combining the viewpoints of the emergence of behaviors with complex physical dynamics and the world model-based approach is another important challenge.

## 6 Conclusion

In this survey paper, we have aimed to clarify the frontiers and challenges of world models and predictive coding in cognitive and developmental robotics. Creating an autonomous robot that can actively explore the real environment, acquire knowledge, and learn skills continuously is the ultimate goal of cognitive and developmental robotics. To make the robot continuously develop through active exploration, the robot's learning process should be based on sensorimotor information obtained through physical and social interactions with the physical and social environment. Following the motivation, this paper reviewed studies related to world models and predictive coding in cognitive and developmental robotics and related AI studies. We clarified the definition of world model and predictive coding in robotics, in conjunction with those of FEP and active inference, and discussed the relationship between them. We also introduced state-of-the-art and research gaps of studies on world models and predictive in robotics. We described six frontiers and challenges, i.e., latent representations for action planning, neuro-symbolic predictive models, affordance perception, social interaction, brain-inspired world models, and cognitive architecture. Through the survey and clarification of challenges, we provided future directions for developing cognitive and developmental robots based on world models and predictive coding.

## Acknowledgements

This work was partially supported by JST Moonshot R&D, Grant Number JPMJMS2033 and JPMJMS2011, JST PRESTO Grant Number JPMJPR22C9, the BAGEP Award of the Science Academy, TUBITAK ARDEB 1001 program (project number: 120E274), the European Union's Horizon 2020 Framework Programme for Research and Innovation under Specific Grant Agreements No. 945539 (Human Brain Project SGA3), No. 952215 (TAILOR), and No. 824153 (POTION), the European Research Council under the Grant Agreement No. 820213 (ThinkAhead), the Human Brain Project Specific Grant Agreement 3 grant (ID 643945539, for the SPIKEFERENCE project), Deepself project under the Priority Programme "The Active Self" (SPP 2134), the project 'COURAGE - A social media companion safeguarding and educating students' (No. 95563 and No. 9B145), and the Volkswagen Foundation inside the initiative Artificial Intelligence and the Society of the Future.

## References

* [1] Lungarella M, Metta G, Pfeifer R, Sandini G. Developmental robotics: a survey. Connection science. 2003; 15(4):151-190.
* [2] Lesort T, Lomonaco V, Stoian A, Maltoni D, Filliat D, Diaz-Rodriguez N. Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges. Information fusion. 2020;58:52-68.
* [3] De Lange M, Aljundi R, Masana M, Parisot S, Jia X, Leonardis A, Slabaugh G, Tuytelaars T. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence. 2021;44(7):3366-3385.
* [4] Friston K, Moran RJ, Nagai Y, Taniguchi T, Gomi H, Tenenbaum J. World model learning and inference. Neural Networks. 2021;144:573-590.
* [5] Ha D, Schmidhuber J. World models. arXiv preprint arXiv:180310122. 2018;.
* [6] Hafner D, Lillicrap T, Norouzi M, Ba J. Mastering Atari with discrete world models. arXiv preprint arXiv:201002193. 2020;.
* [7] Wu P, Escontrela A, Hafner D, Goldberg K, Abbeel P. Daydreamer: World models for physical robot learning. arXiv preprint arXiv:220614176. 2022;.

* [8] Kiverstein J. Could a robot have a subjective point of view? Journal of Consciousness Studies. 2007; 14(7):127-139.
* [9] Von Uexkull J. A scroll through the worlds of animals and men: A picture book of invisible worlds. Semiotica. 1992;89(4):319-391.
* [10] Sebeok TA. Biosemiotics: Its roots, proliferation, and prospects. Semiotica. 2001;:61-78.
* [11] Kull K. Umwelt and modelling. In: The routledge companion to semiotics. Routledge. 2009. p. 65-78.
* [12] Brooks R. Intelligence without representation. Artificial Intelligence. 1991;47(1-3):139-159.
* [13] Arkin RC, Arkin RC, et al.. Behavior-based robotics. MIT press. 1998.
* [14] Verschure PF, Voegtlin T, Douglas RJ. Environmentally mediated synergy between perception and behaviour in mobile robots. Nature. 2003;425(6958):620-624.
* [15] Ognibene D, Baldassare G. Ecological active vision: four bioinspired principles to integrate bottom-up and adaptive top-down attention tested with a simple camera-arm robot. IEEE transactions on autonomous mental development. 2014;7(1):3-25.
* [16] Clark A. Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behavioral and Brain Sciences. 2013;36(3):181-204.
* [17] Helmholtz Hv. Handbuch der physiologischen optik, vol. III. Allgemeine Encyklopadie der Physik, Leipzig: Leopold Voss. 1867;.
* [18] Rao RP, Ballard DH. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience. 1999;2(1):79-87.
* [19] Fritson K. A theory of cortical responses. Philosophical transactions of the Royal Society B: Biological sciences. 2005;360(1456):815-836.
* [20] Parr T, Pezzulo G, Fritson KJ. Active inference: the free energy principle in mind, brain, and behavior. MIT Press. 2022.
* [21] Ciria A, Schillaci G, Pezzulo G, Hafner VV, Lara B. Predictive processing in cognitive robotics: a review. Neural Computation. 2021;33(5):1402-1432.
* [22] Oliver G, Lanillos P, Cheng G. An empirical study of active inference on a humanoid robot. IEEE Transactions on Cognitive and Developmental Systems. 2021;.
* [23] Lanillos P, Meo C, Pezzato C, Meera AA, Baioumy M, Ohata W, Tschantz A, Millidge B, Wisse M, Buckley CL, et al.. Active inference in robotics and artificial agents: Survey and challenges. arXiv preprint arXiv:211201871. 2021;.
* [24] Tani J. Exploring robotic minds: actions, symbols, and consciousness as self-organizing dynamic phenomena. Oxford University Press. 2016.
* [25] Ibarz J, Tan J, Finn C, Kalakrishnan M, Pastor P, Levine S. How to train your robot with deep reinforcement learning: lessons we have learned. The International Journal of Robotics Research. 2021;40(4-5):698-721.
* [26] Ha D, Schmidhuber J. Recurrent world models facilitate policy evolution. Advances in neural information processing systems. 2018;31.
* [27] Nilsson NJ, et al.. Shakey the robot. 1984. Tech Rep.
* [28] Schmidhuber J. Making the world differentiable: On using self-supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments. Inst. fur Informatik. 1990.
* [29] Sutton RS. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In: Machine learning proceedings 1990. Elsevier. 1990. p. 216-224.
* [30] Hafner D, Lillicrap T, Ba J, Norouzi M. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:191201603. 2019;.
* [31] Levine S, Finn C, Darrell T, Abbeel P. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research. 2016;17(1):1334-1373.
* [32] van der Hintst O, Lanillos P. Deep active inference for partially observable mdps. In: Verbelen T, Lanillos P, Buckley CL, De Boom C, editors. Active inference. Cham: Springer International Publishing. 2020. p. 61-71.
* [33] Kalman RE. A new approach to linear filtering and prediction problems. Journal of Basic Engineering (Transactions of the American Society of Mechanical Engineers). 1960;.35-45.
* [34] Cassandra AR, Kaelbling LP, Littman ML. Acting optimally in partially observable stochastic domains. In: Aaai. Vol. 94. 1994. p. 1023-1028.
* [35] Thrun S, Burgard W, Fox D. Probabilistic robotics. MIT Press. 2005.
* [36] Chen Z, et al.. Bayesian filtering: From kalman filters to particle filters, and beyond. Statistics. 2003; 182(1):1-69.
* [37] Thrun S. Particle filters in robotics. In: Proceedings of the eighteenth conference on uncertainty in artificial intelligence. 2002. p. 511-518.

* [38] Kingma DP, Welling M. Auto-encoding variational bayes. arXiv preprint arXiv:13126114. 2013;.
* [39] Laskin M, Srinivas A, Abbeel P. CURL: Contrastive unsupervised representations for reinforcement learning. In: International Conference on Machine Learning (ICML). 2020. p. 5639-5650.
* [40] Nakamura H, Okada M, Taniguchi T. Self-supervised representation learning as multimodal variational inference. arXiv preprint arXiv:220311437. 2022;.
* [41] Hafner D, Lillicrap T, Fischer I, Villegas R, Ha D, Lee H, Davidson J. Learning latent dynamics for planning from pixels. In: International conference on machine learning (ICML). 2019. p. 2555-2565.
* [42] Huang Y, Rao RP. Predictive coding. Wiley Interdisciplinary Reviews: Cognitive Science. 2011;2(5):580-593.
* [43] Hogendoorn H, Burkitt AN. Predictive coding with neural transmission delays: a real-time temporal alignment hypothesis. Eneuro. 2019;6(2).
* [44] Friston K, Kiebel S. Predictive coding under the free-energy principle. Philosophical Transactions of the Royal Society B: Biological Sciences. 2009;364(1521):1211-1221.
* [45] Friston K. The free-energy principle: a unified brain theory? Nature Reviews Neuroscience. 2010;11(2):127-138.
* [46] Clark A. Surfing uncertainty: Prediction, action, and the embodied mind. Oxford University Press. 2015.
* [47] Hohwy J. New directions in predictive processing. Mind & Language. 2020;35(2):209-223.
* [48] Ororbia A, Kifer D. The neural coding framework for learning generative models. Nature communications. 2022;13(1):1-14.
* [49] Friston K, Kilner J, Harrison L. A free energy principle for the brain. Journal of Physiology-Paris. 2006;100(1-3):70-87.
* [50] Friston K, Mattout J, Kilner J. Action understanding and active inference. Biological cybernetics. 2011;104(1):137-160.
* [51] Buckley C, Kim C, McGregor S, Seth A. The free energy principle for action and perception: A mathematical review. Journal of Mathematical Psychology. 2017;81:55-79.
* [52] Friston KJ, Daunizeau J, Kilner J, Kiebel SJ. Action and behavior: a free-energy formulation. Biological Cybernetics. 2010;102(3):227-260.
* [53] Friston K, Adams RA, Perninet L, Breakspear M. Perceptions as Hypotheses: Saccades as Experiments. Frontiers in Psychology. 2012;3(May):151.
* [54] Friston K, Rigoli F, Ognibene D, Mathys C, Fitzgerald T, Pezzulo G. Active inference and epistemic value. Cognitive neuroscience. 2015;6(4):187-214.
* [55] Kruglanski A, Jasko K, Friston K. All thinking is "wishful"thinking. Trends in Cognitive Sciences. 2020;.
* [56] Friston K, Samothrakis S, Montague R. Active inference and agency: optimal control without cost functions. Biological Cybernetics. 2012;106(8-9):523-541.
* [57] Friston K, Thornton C, Clark A. Free-energy minimization and the dark-room problem. Frontiers in psychology. 2012;.130.
* [58] Sun Z, Firestone C. The dark room problem. Trends in Cognitive Sciences. 2020;24(5):346-348.
* [59] Chaplot DS, Gandhi D, Gupta S, Gupta A, Salakhutdinov R. Learning to explore using active neural slam. In: International Conference on Learning Representations (ICLR). 2020.
* [60] Ramakrishnan SK, Jayaraman D, Grauman K. Emergence of exploratory look-around behaviors through active observation completion. Science Robotics. 2019;4(30):eaaw6326.
* [61] Ammirato P, Poirson P, Park E, Kosecka J, Berg AC. A dataset for developing and benchmarking active vision. In: IEEE International Conference on Robotics and Automation (ICRA). 2017. p. 1378-1385.
* [62] Ognibene D, Demiris Y. Towards active event recognition in: II. Jcai. 2013. p. 2495-2501.
* [63] Lee K, Ognibene D, Chang HJ, Kim TK, Demiris Y. Stare: Spatio-temporal attention relocation for multiple structured activities detection. IEEE Transactions on Image Processing. 2015;24(12):5916-5927.
* [64] Donnarumma F, Costantini M, Ambrosini E, Friston K, Pezzulo G. Action perception as hypothesis testing. Cortex. 2017;89:45-60.
* [65] Wang B, Huang L, Hoai M. Active vision for early recognition of human actions. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020.
* [66] Schwartenbeck P, Passecker J, Hauser TU, FitzGerald TH, Kronbichler M, Friston KJ. Computational mechanisms of curiosity and goal-directed exploration. Elife. 2019;8.
* [67] Denzler J, Brown C. Information theoretic sensor data selection for active object recognition and state estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2002;24(2):145-157.
* [68] Sommerlade E, Reid I. Information-theoretic active scene exploration. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2008. p. 1-7.
* [69] Hafner D, Ortega PA, Ba J, Parr T, Friston K, Heess N. Action and perception as divergence minimization.

arXiv preprint arXiv:200901791. 2020;.
* [70] Parr T, Friston KJ. Generalised free energy and active inference. Biological cybernetics. 2019;113(5):495-513.
* [71] Attias H. Planning by probabilistic inference. In: International workshop on artificial intelligence and statistics. 2003. p. 9-16.
* [72] Toussaint M. Robot trajectory optimization using approximate inference. In: International conference on machine learning (ICML). 2009. p. 1049-1056.
* [73] Kappen HJ, Gomez V, Opper M. Optimal control as a graphical model inference problem. Machine learning. 2012;87(2):159-182.
* [74] Botvinick M, Toussaint M. Planning as inference. Trends in cognitive sciences. 2012;16(10):485-488.
* [75] Millidge B, Tschantz A, Seth AK, Buckley CL. On the relationship between active inference and control as inference. In: International workshop on active inference. Springer. 2020. p. 3-11.
* [76] Van de Cruys S, Friston K, Clark A. Controlled optimism: Reply to sun and frestone on the dark room problem. Trends in Cognitive Sciences. 2020;24(9):1-2.
* [77] Haarnoja T, Zhou A, Abbeel P, Levine S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In: International Conference on Machine Learning (ICML). 2018. p. 1861-1870.
* [78] Okada M, Taniguchi T. Dreaming: Model-based reinforcement learning by latent imagination without reconstruction. In: IEEE International Conference on Robotics and Automation (ICRA). 2021. p. 4209-4215.
* [79] Okumura R, Nishio N, Taniguchi T. Tactile-sensitive NewtonianVAE for high-accuracy industrial connector-socket insertion. In: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022.
* [80] Kaiser L, Babaeizadeh M, Milos P, Osinski B, Campbell RH, Czechowski K, Erhan D, Finn C, Kozakowski P, Levine S, et al.. Model-based reinforcement learning for atari. arXiv preprint arXiv:190300374. 2019;.
* [81] Ke NR, Singh A, Touati A, Goyal A, Bengio Y, Parikh D, Batra D. Learning dynamics model in reinforcement learning by incorporating the long term future. arXiv preprint arXiv:190301599. 2019;.
* [82] Okada M, Kosaka N, Taniguchi T. Planet of the Bayesians: Reconsidering and improving deep planning network by incorporating bayesian inference. In: Ieee/rsj international conference on intelligent robots and systems (IROS). 2020. p. 5611-5618.
* [83] Okada M, Taniguchi T. DreamingV2: Reinforcement learning with discrete world models without reconstruction. In: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022.
* [84] Jaques M, Burke M, Hospedales TM. NewtonianVAE: Proportional control and goal identification from pixels via physical latent spaces. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021. p. 4454-4463.
* [85] Tani J. Learning to generate articulated behavior through the bottom-up and the top-down interaction processes. Neural Networks. 2003;16(1):11-23.
* [86] Yamashita Y, Tani J. Emergence of functional hierarchy in a multiple timescale neural network model: A humanoid robot experiment. PLoS Computational Biology. 2008;4(11):e1000220.
* [87] Nishimoto R, Tani J. Development of hierarchical structures for actions and motor imagery: A constructivist view from synthetic neuro-robotics study. Psychological Research. 2009;73(4):545-558.
* [88] Namikawa J, Nishimoto R, Tani J. A neurodynamic account of spontaneous behaviour. PLoS Computational Biology. 2011;7(10):e1002221.
* [89] Yamashita Y, Tani J. Spontaneous prediction error generation in schizophrenia. PloS One. 2012;7(5):e37843.
* [90] Murata S, Namikawa J, Arie H, Sugano S, Tani J. Learning to Reproduce Fluctuating Time Series by Inferring Their Time-Dependent Stochastic Properties: Application in Robot Learning Via Tutoring. IEEE Transactions on Autonomous Mental Development. 2013;5(4):298-310.
* [91] Murata S, Yamashita Y, Arie H, Ogata T, Sugano S, Tani J. Learning to Perceive the World as Probabilistic or Deterministic via Interaction With Others: A Neuro-Robotics Experiment. IEEE Transactions on Neural Networks and Learning Systems. 2017;28(4):830-848.
* [92] Ahmadi A, Tani J. A Novel Predictive-Coding-Inspired Variational RNN Model for Online Prediction and Recognition. Neural Computation. 2019;31(11):2025-2074.
* [93] Lanillos P, Oliva D, Philippsen A, Yamashita Y, Nagai Y, Cheng G. A review on neural network models of schizophrenia and autism spectrum disorder. Neural Networks. 2020;122:338-363.
* [94] Van de Cruys S, Evers K, Van der Hallen R, Van Eylen L, Boets B, De-Wit L, Wagemans J. Precise minds in uncertain worlds: predictive coding in autism. Psychological review. 2014;121(4):649-75.
* [95] Lawson RP, Rees G, Friston KJ. An aberrant precision account of autism. Frontiers in Human Neuroscience. 2014;8(May):1-10.
* [96] Idei H, Murata S, Chen Y, Yamashita Y, Tani J, Ogata T. A Neurorobotics Simulation of Autistic BehaviorInduced by Unusual Sensory Precision. Computational Psychiatry. 2018;2:164-182.
* [97] Idei H, Murata S, Yamashita Y, Ogata T. Homogeneous Intrinsic Neuronal Excitability Induces Overfitting to Sensory Noise: A Robot Model of Neurodevelopmental Disorder. Frontiers in Psychiatry. 2020;11(August):1-15.
* [98] Idei H, Murata S, Yamashita Y, Ogata T. Paradoxical sensory reactivity induced by functional disconnection in a robot model of neurodevelopmental disorder. Neural Networks. 2021;138:150-163.
* [99] Sancaktar C, van Gerven M, Lanillos P. End-to-end pixel-based deep active inference for body perception and action. In: Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob). 2020.
* [100] Pezzato C, Baioumy M, Corbato CH, Hawes N, Wisse M, Ferrari R. Active inference for fault tolerant control of robot manipulators with sensory faults. In: International workshop on active inference. Springer. 2020. p. 20-27.
* [101] Meo C, Lanillos P. Multimodal vae active inference controller. In: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. 2021.
* [102] Lanillos P, Cheng G. Active inference with function learning for robot body perception. International Workshop on Continual Unsupervised Sensorimotor Learning, IEEE Developmental Learning and Epigenetic Robotics (ICDL-Epirob). 2018;.
* [103] Lanillos P, Cheng G. Adaptive robot body learning and estimation through predictive coding. In: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. 2018. p. 4083-4090.
* [104] Ito M, Noda K, Hoshino Y, Tani J. Dynamic and interactive generation of object handling behaviors by a small humanoid robot using a dynamic neural network model. Neural Networks. 2006;19(3):323-337.
* [105] Sugita Y, Tani J. Learning Semantic Combinatoriality from the Interaction between Linguistic and Behavioral Processes. Adaptive Behavior. 2005;13(1):33-52.
* [106] Chen Y, Murata S, Arie H, Ogata T, Tani J, Sugano S. Emergence of interactive behaviors between two robots by prediction error minimization mechanism. In: Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob). 2016. p. 302-307.
* [107] Catal O, Verbehen T, Van de Maele T, Dhoedt B, Safron A. Robot navigation as hierarchical active inference. Neural Networks. 2021;142:192-204.
* [108] Imre M, Oztop E, Nagai Y, Ugur E. Affordance-based altruistic robotic architecture for human-robot collaboration. Adaptive Behavior. 2019;27(4):223-241.
* [109] Seker MY, Ahmetoglu A, Nagai Y, Asada M, Oztop E, Ugur E. Imitation and mirror systems in robots through deep modality blending networks. Neural Networks. 2022;146:22-35.
* [110] Seker MY, Imre M, Piater J, Ugur E. Conditional neural movement primitives. In: Robotics Science and Systems (RSS). 2019.
* [111] Tschantz A, Baltieri M, Seth AK, Buckley CL. Scaling active inference. In: international joint conference on neural networks (IJCNN). 2020. p. 1-8.
* [112] van Bergen RS, Lanillos PL. Object-based active inference. arXiv preprint arXiv:220901258. 2022;.
* [113] Da Costa L, Lanillos P, Sajid N, Friston K, Khan S. How active inference could help revolutionise robotics. Entropy. 2022;224(3):361.
* [114] Valenzo D, Ciria A, Schillaci G, Lara B. Grounding context in embodied cognitive robotics. Frontiers in Neurorobotics. 2022;16.
* [115] Fountas Z, Sajid N, Mediano P, Friston K. Deep active inference agents using monte-carlo methods. Advances in neural information processing systems. 2020;33:11662-11675.
* [116] Mazzaglia P, Verbehen T, Dhoedt B. Contrastive active inference. Advances in Neural Information Processing Systems. 2021;34:13870-13882.
* [117] Bohmer W, Springenberg JT, Boedecker J, Riedmiller M, Obermayer K. Autonomous learning of state representations for control: An emerging field aims to autonomously learn state representations for reinforcement learning agents from their real-world sensor observations. KI-Kunstliche Intelligenz. 2015;29(4):353-362.
* [118] Achille A, Soatto S. A separation principle for control in the age of deep learning. Annual Review of Control, Robotics, and Autonomous Systems. 2018;1:287-307.
* [119] Lesort T, Diaz-Rodriguez N, Goudou JF, Filliat D. State representation learning for control: An overview. Neural Networks. 2018;108:379-392.
* [120] Stoianov I, Maisto D, Pezzulo G. The hippocampal formation as a hierarchical generative model supporting generative replay and continual learning. Progress in Neurobiology. 2022;217:102329.
* [121] Gumbsch C, Butz MV, Martius G. Sparsely changing latent states for prediction and planning in partially observable domains. Advances in Neural Information Processing Systems. 2021;34:17518-17531.
* [122] Greff K, Kaufman RL, Kabra R, Watters N, Burgess C, Zoran D, Matthey L, Botvinick M, Lerchner A,Multi-object representation learning with iterative variational inference. In: International Conference on Machine Learning (ICML). 2019. p. 2424-2433.
* [123] Wang T, Du SS, Torralba A, Isola P, Zhang A, Tian Y. Denoised MDPs: Learning world models better than the world itself. arXiv preprint arXiv:220615477. 2022;.
* [124] Kohler W, Winter E. The mentality of apes. International library of psychology, philosophy, and scientific method. K. Paul, Trench, Trubner & Company, Limited. 1925.
* [125] Steedman M. Plans, affordances, and combinatory grammar. Linguistics and Philosophy. 2002;25(5-6):723-753.
* [126] Goel V, Gold B, Kapur S, Houle S. Neuroanatomical correlates of human reasoning. Journal of cognitive neuroscience. 1998;10(3):293-302.
* [127] Al N, Nolen-Hoeksema S. Atkinson and higard's introduction to psychology. Cengage Learning. 2014.
* [128] Russell SJ, Norvig P. Artificial intelligence: a modern approach. Pearson Education Limited,. 2016.
* [129] Mugan J, Kuipers B. Autonomous learning of high-level states and actions in continuous environments. IEEE Transactions on Autonomous Mental Development. 2012;4(1):70-86.
* [130] Ahmetoglu A, Ugur E, Asada M, Oztop E. High-level features for resource economy and fast learning in skill transfer. Advanced Robotics. 2022;36(5-6):291-303.
* [131] Konidaris G, Kaelbling LP, Lozano-Perez T. Constructing symbolic representations for high-level planning. In: AAAI Conference on Artificial Intelligence (AAAI). 2014.
* [132] Konidaris G, Kaelbling L, Lozano-Perez T. Symbol acquisition for probabilistic high-level planning. In: International Joint Conference on Artificial Intelligence (IJCAI). 2015.
* [133] James S, Rosman B, Konidaris G. Learning portable representations for high-level planning. arXiv preprint arXiv:190512006. 2019;.
* [134] Ugur E, Piater J. Bottom-up learning of object categories, action effects and logical rules: From continuous manipulative exploration to symbolic planning. In: IEEE International Conference on Robotics and Automation (ICRA). 2015. p. 2627-2633.
* [135] Ugur E, Piater J. Refining discovered symbols with multi-step interaction experience. In: 2015 ieee-ras 15th international conference on humanoid robots (humanoids). 2015. p. 1007-1012.
* [136] Ahmetoglu A, Seker MY, Piater J, Oztop E, Ugur E. Deepsym: Deep symbol generation and rule learning from unsupervised continuous robot interaction for planning. Journal of Artificial Intelligence Research. 2022;.
* [137] Sahin E, Cakmak M, Dogar MR, Ugur E, Ucoluk G. To afford or not to afford: A new formalization of affordances toward affordance-based robot control. Adaptive Behavior. 2007;15(4):447-472.
* [138] Ahmetoglu A, Oztop E, Ugur E. Learning multi-object symbols for manipulation with attentive deep effect predictors. arXiv preprint arXiv:220801021. 2022;.
* [139] Asai M, Fukunaga A. Classical planning in deep latent space: Bridging the subsymbolic-symbolic boundary. arXiv preprint arXiv:170500154. 2017;.
* [140] Asai M, Muise C. Learning neural-symbolic descriptive planning models via cube-space priors: The voyage home (to strips). arXiv preprint arXiv:200412850. 2020;.
* [141] Gibson JJ. The ecological approach to visual perception. Boston: Houghton Mifflin. 1979.
* [142] Zech P, Haller S, Lakani SR, Ridge B, Ugur E, Piater J. Computational models of affordance in robotics: a taxonomy and systematic classification. Adaptive Behavior. 2017;25(5):235-271.
* [143] Jamone L, Ugur E, Cangelosi A, Fadiga L, Bernardino A, Piater J, Santos-Victor J. Affordances in psychology, neuroscience and robotics: a survey. IEEE Transactions on Cognitive and Developmental Systems. 2016;10(1):4-25.
* [144] Renaudo E, Zech P, Chatila R, Khamassi M. Computational models of affordance for robotics. Frontiers in Neurorobotics. 2022;16:1045355.
* [145] Gibson JJ. The senses considered as perceptual systems. Boston: Houghton Mifflin. 1966.
* [146] Gibson JJ. The theory of affordances. Perceiving, Acting, and Knowing: Toward an ecological psychology. Eds. Lawrence Erlbaum Associates. 1977.
* [147] Norman DA. Affordance, conventions, and design. Interactions. 1999;6(3):38-42.
* [148] Gibson EJ. An odyssey in learning and perception. MIT Press. 1994.
* [149] Gibson EJ. Perceptual learning in development: Some basic concepts. Ecological Psychology. 2000;12(4):295-302.
* [150] Gibson EJ. The world is so full of a number of things: On specification and perceptual learning. Ecological Psychology. 2003;15(4):283-288.
* [151] Sahin E, Cakmak M, Dogar MR, Ugur E, Ucoluk G. To afford or not to afford: A new formalization of affordances toward affordance-based robot control. Adaptive Behavior. 2007;15(4):447-472.

* [152] Montesano L, Lopes M, Bernardino A, Santos-Victor J. Learning object affordances: From sensory-motor coordination to imitation. IEEE Transactions on Robotics. 2008;24(1):15-26.
* [153] Ugur E, Sahin E, Oztop E. Unsupervised learning of object affordances for planning in a mobile manipulation platform. In: IEEE International Conference on Robotics and Automation (ICRA). 2011. p. 4312-4317.
* [154] Krueger N, Geib C, Piater J, Petrick R, Steedman M, Worgotter F, Ude A, Asfour T, Kraft D, Omr cpen D, Agostini A, Dillmann R. Object-action complexes: Grounded abstractions of sensory-motor processes. Robotics and Autonomous Systems. 2011;59(10):740-757.
* [155] Szedmak S, Ugur E, Piater J. Knowledge propagation and relation learning for predicting action effects. In: Ieee/rsj international conference on intelligent robots and systems (IROS). 2014. p. 623-629.
* [156] Goncalves A, Abrantes J, Saponaro G, Jamone L, Bernardino A. Learning intermediate object affordances: Towards the development of a tool concept. In: 4th international conference on development and learning and on epigenetic robotics. 2014. p. 482-488.
* [157] Debban A, Jamone L, Kampff AR, Santos-Victor J. Denoising auto-encoders for learning of objects and tools affordances in continuous space. In: IEEE International Conference on Robotics and Automation (ICRA). 2016. p. 4866-4871.
* [158] Debban A, Jamone L, Kampff AR, Santos-Victor J. A deep probabilistic framework for heterogeneous self-supervised learning of affordances. In: IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids). 2017. p. 476-483.
* [159] Ugur E, Piater J. Emergent structuring of interdependent affordance learning tasks using intrinsic motivation and empirical feature selection. IEEE Transactions on Cognitive and Developmental Systems. 2017;9(4):328-340.
* [160] Sarathy V, Scheutz M. A logic-based computational framework for inferring cognitive affordances. IEEE Transactions on Cognitive and Developmental Systems. 2018;10(1):26-43.
* [161] Stramandinoli F, Tikhanoff V, Pattacini U, Nori F. Heteroscedastic regression and active learning for modeling affordances in humanoids. IEEE Transactions on Cognitive and Developmental Systems. 2018;10(2):455-468.
* [162] Tekden AE, Erdem A, Erdem E, Imre M, Seker MY, Ugur E. Belief regulated dual propagation nets for learning action effects on groups of articulated objects. In: IEEE International Conference on Robotics and Automation (ICRA). 2020. p. 10556-10562.
* [163] Antunes A, Jamone L, Saponaro G, Bernardino A, Ventura R. From human instructions to robot actions: Formulation of goals, affordances and probabilistic planning. In: Ieee iera. 2016.
* [164] Debban A, Zhang S, Cauli N, Jamone L, Santos-Victor J. Learning deep features for robotic inference from physical interactions. IEEE Transactions on Cognitive and Developmental Systems. 2022;:1-1.
* [165] Shen Z, Wu Y. Investigation of practical use of humanoid robots in elderly care centres. In: Proceedings of the fourth international conference on human agent interaction. 2016. p. 63-66.
* [166] Albrecht SV, Stone P. Autonomous agents modelling other agents: A comprehensive survey and open problems. Artificial Intelligence. 2018;258:66-95.
* [167] El Zaatari S, Marei M, Li W, Usman Z. Cobot programming for collaborative industrial tasks: An overview. Robotics and Autonomous Systems. 2019;116:162-180.
* [168] Hentout A, Aouache M, Maoudj A, Akli I. Human-robot interaction in industrial collaborative robotics: a literature review of the decade 2008-2017. Advanced Robotics. 2019;33(15-16):764-799.
* [169] Magrini E, Ferraguti F, Ronga AJ, Pini F, De Luca A, Leali F. Human-robot coexistence and interaction in open industrial cells. Robotics and Computer-Integrated Manufacturing. 2020;61:101846.
* [170] Ognibene D, Foulsham T, Marchegiani L, Farinella GM. Active vision and perception in human-robot collaboration. Frontiers in Neurorobotics. 2022;16.
* [171] Semeraro F, Griffiths A, Cangelosi A. Human-robot collaboration and machine learning: A systematic review of recent research. Robotics and Computer-Integrated Manufacturing. 2023;79:102432.
* [172] Ognibene D, Chinellato E, Sarabia M, Demiris Y. Contextual action recognition and target localization with an active allocation of attention on a humanoid robot. Bioinspiration & biomimetics. 2013;8(3):035002.
* [173] Fotopoulou A, Tsakiris M. Mentalizing homeostasis: The social origins of interoceptive inference. Neurosproschanalysis. 2017;19(1):3-28.
* [174] Veissiere SP, Constant A, Ramstead MJ, Friston KJ, Kirmayer LJ. Thinking through other minds: A variational approach to cognition and culture. Behavioral and brain sciences. 2020;43.
* [175] Saxe R. Uniquely human social cognition. Current opinion in neurobiology. 2006;16(2):235-239.
* [176] Bianco F, Ognibene D. Functional advantages of an adaptive theory of mind for robotics: a review of current architectures. Computer Science and Electronic Engineering (CEEC). 2019;:139-143.
* [177] Baker CL, Jara-Ettinger J, Saxe R, Tenenbaum JB. Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. Nature Human Behaviour. 2017;1(4):1-10.
* [178] Bianco F, Ognibene D. Robot learning theory of mind through self-observation: Exploiting the intentions-beliefs synergy. arXiv preprint arXiv:221009435. 2022;.
* [179] Bianchi F, Marelli M, Nicoli P, Palmonari M. Sweat: Scoring polarization of topics across different corpora. arXiv preprint arXiv:210907231. 2021;.
* [180] Ognibene D, Mirante L, Marchegiani L. Proactive intention recognition for joint human-robot search and rescue missions through monte-carlo planning in POMDP environments. In: International conference on social robotics. Springer. 2019. p. 332-343.
* [181] Heinze C. Modelling intention recognition for intelligent agent systems. DEFENCE SCIENCE AND TECHNOLOGY ORGANISATION. 2004. Tech Rep. Available from: [http://www.dsto.defence.gov.au/corporate/reports/DSTO-RR-0286.pdf](http://www.dsto.defence.gov.au/corporate/reports/DSTO-RR-0286.pdf).
* [182] Roggen D, Calatroni A, Rossi M, Holleczek T, Forster K, Troster G, Lukowicz P, Bannach D, Pirkl G, Ferscha A, et al. Collecting complex activity datasets in highly rich networked sensor environments. In: International conference on networked sensing systems (INSS). 2010. p. 233-240.
* [183] Zeng M, Nguyen LT, Yu B, Mengshoel OJ, Zhu J, Wu P, Zhang J. Convolutional neural networks for human activity recognition using mobile sensors. In: 6th international conference on mobile computing, applications and services. IEEE. 2014. p. 197-205.
* [184] Yang J, Nguyen MN, San PP, Li XL, Krishnaswamy S. Deep convolutional neural networks on multichannel time series for human activity recognition. In: Twenty-fourth international joint conference on artificial intelligence. 2015.
* [185] Lee SU, Hofmann A, Williams B. A model-based human activity recognition for human-robot collaboration. In: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. 2019. p. 736-743.
* [186] Ramirez M, Geffner H. Plan recognition as planning. In: International joint conference on artificial intelligence (IJCAI). 2009.
* [187] Baker CL, Saxe R, Tenenbaum JB. Action understanding as inverse planning. Cognition. 2009;113(3):329-349.
* [188] Sohrabi S, Riabov AV, Udrea O. Plan recognition as planning revisited. In: International joint conference on artificial intelligence (IJCAI). New York, NY. 2016. p. 3258-3264.
* [189] Ramirez M, Geffner H. Goal recognition over POMDPs: Inferring the intention of a pomdp agent. In: International joint conference on artificial intelligence (IJCAI). 2011.
* [190] Baker C, Saxe R, Tenenbaum J. Bayesian theory of mind: Modeling joint belief-desire attribution. In: Proceedings of the annual meeting of the cognitive science society. Vol. 33. 2011.
* [191] Shvo M, McIlraith SA. Active goal recognition. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. 2020. p. 9957-9966.
* [192] Amato C, Baisero A. Active goal recognition. arXiv preprint arXiv:190911173. 2019;.
* [193] Demiris Y, Simmons G. Perceiving the unusual: Temporal properties of hierarchical motor representations for action perception. Neural Networks. 2006;19(3):272-284.
* [194] Cardona-Rivera RE, Young RM. Toward combining domain theory and recipes in plan recognition. In: Workshops at the thirty-first aaai conference on artificial intelligence. 2017.
* [195] Proietti R, Pezzulo G, Tessari A. An active inference model of hierarchical action understanding, learning and imitation. PsyArXiv. 2021;.
* [196] Zhi-Xuan T, Mann J, Silver T, Tenenbaum J, Mansinghka V. Online bayesian goal inference for boundedly rational planning agents. In: Larochelle H, Ranzato M, Hadsell R, Balcan M, Lin H, editors. Advances in neural information processing systems. Vol. 33. Curran Associates, Inc.. 2020. p. 19238-19250. Available from: [https://proceedings.neurips.cc/paper/2020/file/df3aebc649fe9e3b674eeb79a04da224e-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/df3aebc649fe9e3b674eeb79a04da224e-Paper.pdf).
* [197] Johnson M, Demiris Y. Perceptual perspective taking and action recognition. International Journal of Advanced Robotic Systems. 2005;2(4):32.
* [198] Pandey AK, Ali M, Alami R. Towards a task-aware proactive sociable robot based on multi-state perspective-taking. International Journal of Social Robotics. 2013;5(2):215-236.
* [199] Fischer T, Demiris Y. Computational modeling of embodied visual perspective taking. IEEE Transactions on Cognitive and Developmental Systems. 2019;12(4):723-732.
* [200] Torabi F, Warnell G, Stone P. Recent advances in imitation learning from observation. arXiv e-prints. 2019; :arXiv-1905.
* [201] Mortl A, Lawitzky M, Kucukyilmaz A, Sezgin M, Basdogan C, Hirche S. The role of roles: Physical cooperation between humans and robots. The International Journal of Robotics Research. 2012;31(13):1656-1674.

* [202] Li Y, Tee KP, Yan R, Chan WL, Wu Y. A framework of human-robot coordination based on game theory and policy iteration. IEEE Transactions on Robotics. 2016;32(6):1408-1418.
* [203] Pezzulo G, Donnarumma F, Dindo H, D'Ausilio A, Konvalinka I, Castelfranchi C. The body talks: Sensorimotor communication and its brain and kinematic signatures. Physics of life reviews. 2019;28:1-21.
* [204] Ognihene D, Giglia G, Marchegiani L, Rudrauf D. Implicit perception simplicity and explicit perception complexity in sensorimotor comunication. Physics of life reviews. 2019;28:36-38.
* [205] Kok BC, Soh H. Trust in robots: Challenges and opportunities. Current Robotics Reports. 2020;1(4):297-309.
* [206] Craik K. The nature of explanation. Cambridge: Cambridge University Press. 1943.
* [207] von Helmholtz H. Handbuch der physiologischen optik. Leipzig: L. Voss. 1867.
* [208] Gregory RL. Knowledge in perception and illusion. Philosophical Transactions of the Royal Society B: Biological Sciences. 1997;352:1121-1128.
* [209] Doya K, Ishii S, Pouget A, Rao RPN, editors. Bayesian brain: Probabilistic approaches to neural coding. 1st ed. The MIT Press. 2007.
* [210] Shiffrin RM, Bassett DS, Kriegeskorte N, Tenenbaum JB. The brain produces mind by modeling. Proceedings of the National Academy of Sciences. 2020;117(47):29299-29301.
* [211] Ma WJ, Beck JM, Latham PE, Pouget A. Bayesian inference with probabilistic population codes. Nat Neurosci. 2006;9(11):1432-1438. Available from: [http://dx.doi.org/10.1038/nn1790](http://dx.doi.org/10.1038/nn1790).
* [212] Buesing L, Bill J, Nessler B, Maass W. Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons. PLoS Comput Biol. 2011;7(1):e1002211.
* [213] Sebastian S, Seemiller ES, Geisler WS. Local reliability weighting explains identification of partially masked objects in natural images. Proceedings of the National Academy of Sciences. 2020;117(47):29363-29370.
* [214] Lynn CW, Bassett DS. How humans learn and represent networks. Proceedings of the National Academy of Sciences. 2020;117(47):29407-29415.
* [215] Berkes P, Orban G, Lengyel M, Fiser J. Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment. Science. 2011;331(6013):83-87.
* [216] Ekman M, Kok P, de Lange FP. Time-compressed preplay of anticipated events in human primary visual cortex. Nature Communications. 2017;8(1):1-9.
* [217] Daw ND, Gershman SJ, Seymour B, Dayan P, Dolan RJ. Model-based influences on humans' choices and striatal prediction errors. Neuron. 2011;69(6):1204-1215.
* [218] Gomez CM, Arjona A, Donnarumma F, Maisto D, Rodriguez Martinez EI, Pezzulo G. Tracking the time course of bayesian inference with event related potentials: a study using the central cue posner paradigm. Frontiers in Psychology. 2019;10:1424.
* [219] Muckli L, De Martino F, Vizioli L, Petro LS, Smith FW, Ugurbil K, Goebel R, Yacoub E. Contextual feedback to superficial layers of v1. Current Biology. 2015;25(20):2690-2695.
* [220] Walsh KS, McGovern DP, Clark A, O'Connell RG. Evaluating the neurophysiological evidence for predictive processing as a model of perception. Annals of the new York Academy of Sciences. 2020;1464(1):242-268.
* [221] Bullmore E, Sporns O. Complex brain networks: graph theoretical analysis of structural and functional systems. Nature reviews neuroscience. 2009;10(3):186-198.
* [222] Laird JE, Lebiere C, Rosenbloom PS. A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. AI Magazine. 2017;38(4):13-26.
* [223] Margules DS, Ghosh SS, Goulas A, Falkiewicz M, Huntenburg JM, Langs G, Bezgin G, Eickhoff SB, Castellanos FX, Petrides M, et al.. Situating the default-mode network along a principal gradient of macroscale cortical organization. Proceedings of the National Academy of Sciences. 2016;113(44):12574-12579.
* [224] Ungerleider L, Haxby J. "what" and "where" in the human brain. Current Opinion in Neurobiology. 1994;4(2):157-65.
* [225] Taniguchi T, Yamakawa H, Nagai T, Doya K, Sakagami M, Suzuki M, Nakamura T, Taniguchi A. A whole brain probabilistic generative model: Toward realizing cognitive architectures for developmental robots. Neural Networks. 2022;150:293-312.
* [226] Singer W. Recurrent dynamics in the cerebral cortex: Integration of sensory evidence with stored knowledge. Proceedings of the National Academy of Sciences. 2021;118(33):e2101043118.
* [227] Pezzulo G, Zorzi M, Corbetta M. The secret life of predictive brains: what's spontaneous activity for? Trends in Cognitive Sciences. 2021;.

* [228] Gyorgy Buzsaki M. The brain from inside out. Oxford University Press. 2019.
* [229] Pezzulo G, Kemere C, van der Meer M. Internally generated hippocampal sequences as a vantage point to probe future-oriented cognition. Annals of the New York Academy of Sciences. 2017;1396:144-165.
* [230] Nazli I, Ferrari A, Huber-Huber C, de Lange FP. Statistical learning is not error-driven. bioRxiv. 2022;.
* [231] Pezzulo G, Cisek P. Navigating the affordance landscape: Feedback control as a process model of behavior and cognition. Trends in Cognitive Sciences. 2016;20(6):414-424.
* [232] Cisek P. Resynthesizing behavior through phylogenetic refinement. Attention, Perception, & Psychophysics. 2019;81(7):2265-2287.
* [233] Lieto A, Bhatt M, Oltramari A, Vernon D. The role of cognitive architectures in general artificial intelligence. 2018.
* [234] Kotseruba I, Tsotsos JK. 40 years of cognitive architectures: core cognitive abilities and practical applications. Artificial Intelligence Review. 2020;53(1):17-94.
* [235] Anderson JR. How can the human mind occur in the physical universe? Oxford University Press. 2009.
* [236] Laird JE. Extending the soar cognitive architecture. Frontiers in Artificial Intelligence and Applications. 2008;171:224.
* [237] Anderson JR. Human symbol manipulation within an integrated cognitive architecture. In: Cognitive science. Routledge. 2005. p. 313-341.
* [238] Puigbo JY, Pumarola A, Tellez RA. Controlling a general purpose service robot by means of a cognitive architecture. In: C eur workshop proceedings. 2013. p. 45-55.
* [239] Mohan S, Laird JE. Learning to play mario. Tech Rep CCA-TR-2009-03. 2009;.
* [240] Rosenbloom PS, Demski A, Ustun V. The sigma cognitive architecture and system: Towards functionally elegant grand unification. Journal of Artificial General Intelligence. 2016;7(1):1-103.
* [241] Damgaard MR, Pedersen R, Bak T. Toward an idiomatic framework for cognitive robotics. Patterns. 2022;3(7):100533.
* [242] Bingham E, Chen JP,ankowiak M, Obermeyer F, Pradhan N, Karaletsos T, Singh R, Szerlip P, Horsfall P, Goodman ND. Pyro: Deep universal probabilistic programming. The Journal of Machine Learning Research. 2019;20(1):973-978.
* [243] Paige B, van de Meent JW, Desmaison A, Goodman N, Kohli P, Wood F, Torr P, et al.. Learning disentangled representations with semi-supervised deep generative models. Advances in neural information processing systems. 2017;30.
* [244] Tran D, Kucukelbir A, Dieng AB, Rudolph M, Liang D, Blei DM. Edward: A library for probabilistic modeling, inference, and criticism. arXiv preprint arXiv:161009787. 2016;.
* [245] Sandini G, Mohan V, Sciutti A, Morasso P. Social cognition for human-robot symbiosis--challenges and building blocks. Frontiers in neurorobotics. 2018;12:34.
* [246] Sun R. Anatomy of the Mind: Exploring Psychological Mechanisms and Processes with the Clarion Cognitive Architecture. Oxford University Press. 2016.
* [247] Kahneman D. A perspective on judgment and choice: mapping bounded rationality. American psychologist. 2003;58(9):697.
* [248] Vahrenkamp N, Wachter M, Krohnert M, Welke K, Asfour T. The robot software framework armax. ith Information Technology. 2015;57(2):99-111.
* [249] Nakamura T, Nagai T, Taniguchi T. Serket: An architecture for connecting stochastic models to realize a large-scale cognitive model. Frontiers in Neurorobotics. 2018;12:1-16.
* [250] Taniguchi T, Nakamura T, Suzuki M, Kuniyasu R, Hayashi K, Taniguchi A, Horii T, Nagai T. Neuro-serket: development of integrative cognitive system through the composition of deep probabilistic generative models. New Generation Computing. 2020;:1-26.
* [251] Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:181004805. 2019;.
* [252] Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, et al.. Language models are few-shot learners. Advances in neural information processing systems. 2020;33:1877-1901.
* [253] Wang X, Wei J, Schuurmans D, Le Q, Chi E, Zhou D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:220311171. 2022;.
* [254] Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:220511916. 2022;.
* [255] Ahn M, Brohan A, Brown N, Chebotar Y, Cortes O, David B, Finn C, Fu C, Gopalakrishnan K, Hausman K, Herzog A, Ho D, Hsu J, Ibarz J, Ichter B, Irpan A, Jang E, Ruano RJ, Jeffrey K, Jesmonth S, Joshi N, Julian R, Kalashnikov D, Kuang Y, Lee KH, Levine S, Lu Y, Luu L, Parada C, Pastor P, Quiambao J, RaoK, Rettinghouse J, Reyes D, Sermanet P, Sievers N, Tan C, Toshev A, Vanhoucke V, Xia F, Xiao T, Xu P, Xu S, Yan M, Zeng A. Do as i can and not as is say: Grounding language in robotic affordances. In: arxiv preprint arxiv:2204.01691. 2022.
* [256] Tangiuchi T, Mochihashi D, Nagai T, Uchida S, Inoue N, Kobayashi I, Nakamura T, Hagiwara Y, Iwahashi N, Inamura T. Survey on frontiers of language and robotics. Advanced Robotics. 2019;33(15-16):700-730.
* [257] Steels L. The symbol grounding problem has been solved, so that's next? In: Symbols and embodiment: Debates on meaning and cognition. Oxford University Press. 2008. p. 223-244.
* [258] Taniguchi T, Ugur E, Hoffmann M, Jamone L, Nagai T, Rosman B, Matsuka T, Iwahashi N, Oztop E, Piater J, et al.. Symbol emergence in cognitive developmental systems: a survey. IEEE transactions on Cognitive and Developmental Systems. 2018;11(4):494-516.
* [259] Taniguchi T, Nagai T, Nakamura T, Iwahashi N, Ogata T, Asoh H. Symbol emergence in robotics: A survey. Advanced Robotics. 2016;30(11-12):706-728.
* [260] Jung M, Matsumoto T, Tani J. Goal-directed behavior under variational predictive coding: Dynamic organization of visual attention and working memory. In: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2019. p. 1040-1047.
* [261] Ito H, Yamamoto K, Mori H, Ogata T. Efficient multitask learning with an embodied predictive model for door opening and entry with whole-body control. Science Robotics. 2022;7(65):eaax8177.
* [262] Hoffmann M, Wang S, Outrata V, Alzueta E, Lanillos P. Robot in the mirror: toward an embodied computational model of mirror self-recognition. KI-Kunstliche Intelligenz. 2021;35(1):37-51.
* [263] Juliani A, Aruklumaran K, Sasai S, Kanai R. On the link between conscious function and general intelligence in humans and machines. arXiv preprint arXiv:220405133. 2022;.
* [264] Seth AK. Interoceptive inference, emotion, and the embodied self. Trends in cognitive sciences. 2013;17(11):565-573.
* [265] Bauermeister J, Lanillos P. The role of valence and meta-awareness in mirror self-recognition using hierarchical active inference. arXiv preprint arXiv:220813213. 2022;.
* [266] Hieda C, Nagai T. Survey and perspective on social emotions in robotics. Advanced Robotics. 2022;36(1-2):17-32.
* [267] El Hafi L, Zheng Y, Shirouzu H, Nakamura T, Taniguchi T. Serket-SDE: A Containerized Software Development Environment for the Symbol Emergence in Robotics Toolkit. In: IEEE/SICE International Symposium on System Integration (SII). 2023.
* [268] Heins C, Millidge B, Demekas D, Klein B, Friston K, Couzin I, Tschantz A. pymdp: A python library for active inference in discrete state spaces. arXiv preprint arXiv:220103904. 2022;.
* [269] Reed S, Zolna K, Parisotto E, Colmenarejo SG, Novikov A, Barth-Maron G, Gimenez M, Sulsky Y, Kay J, Springenberg JT, et al. A generalist agent. arXiv preprint arXiv:220506175. 2022;.
* [270] McGeer T, et al. Passive dynamic walking. Int J Robotics Res. 1990;9(2):62-82.
* [271] Brooks RA. Elephants don't play chess. Robotics and autonomous systems. 1990;6(1-2):3-15.
* [272] Pfeifer R, Lungarella M, Iida F. Self-organization, embodiment, and biologically inspired robotics. science. 2007;318(5853):1088-1093.
* [273] Pfeifer R, Gomez G. Morphological computation-connecting brain, body, and environment. In: Creating brain-like intelligence. Springer. 2009. p. 66-83.