{
  "submission_id": "2ofVtMvRil",
  "source_paper": {
    "paper_id": "2ofVtMvRil",
    "title": "Under review as a conference paper at ICLR 2025 LEARNING GRID CELLS BY PREDICTIVE CODING",
    "abstract": "Grid cells in the medial entorhinal cortex (MEC) of the mammalian brain exhibit a strikingly regular hexagonal firing field over space. These cells are learned after birth and are thought to support spatial navigation but also more abstract computations. Although various computational models, including those based on artificial neural networks, have been proposed to explain the formation of grid cells, the process through which the MEC circuit learns to develop grid cells remains unclear. In this study, we argue that predictive coding, a biologically plausible plasticity rule known to learn visual representations, can also train neural networks to develop hexagonal grid representations from spatial inputs. We demonstrate that grid cells emerge robustly through predictive coding in both static and dynamic environments, and we develop an understanding of this grid cell learning capability by analytically comparing predictive coding with existing models. Our work therefore offers a novel and biologically plausible perspective on the learning mechanisms underlying grid cells. Moreover, it extends the predictive coding theory to the hippocampal formation, suggesting a unified learning algorithm for diverse cortical representations.",
    "publication_date": "2024-10-01",
    "venue": "",
    "year": "2024",
    "citation_count": 0,
    "authors": "",
    "novel": null,
    "cited_paper": false
  },
  "cited_papers": [
    {
      "paper_id": "5e2fdf42d985b3eb9eeb8387e1f7d093ade81525",
      "title": "Vector-based navigation using grid-like representations in artificial agents",
      "abstract": null,
      "publication_date": "2018-05-09",
      "venue": "Nature",
      "year": 2018,
      "citation_count": 612,
      "authors": [
        "Andrea Banino",
        "C. Barry",
        "Benigno Uria",
        "C. Blundell",
        "T. Lillicrap",
        "Piotr Wojciech Mirowski",
        "A. Pritzel",
        "M. Chadwick",
        "T. Degris",
        "Joseph Modayil",
        "Greg Wayne",
        "Hubert Soyer",
        "Fabio Viola",
        "Brian Zhang",
        "Ross Goroshin",
        "Neil C. Rabinowitz",
        "Razvan Pascanu",
        "Charlie Beattie",
        "Stig Petersen",
        "Amir Sadik",
        "Stephen Gaffney",
        "Helen King",
        "K. Kavukcuoglu",
        "D. Hassabis",
        "R. Hadsell",
        "D. Kumaran"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "72823437d781996a412428b097b4b562fe846716",
      "title": "Prediction and memory: A predictive coding account",
      "abstract": null,
      "publication_date": "2020-05-21",
      "venue": "Progress in neurobiology",
      "year": 2020,
      "citation_count": 144,
      "authors": [
        "Helen C. Barron",
        "R. Auksztulewicz",
        "Karl J. Friston"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "858549b00245aadc92f91a2540f01398f5f389ae",
      "title": "A solution to the learning dilemma for recurrent networks of spiking neurons",
      "abstract": "Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method\u2013called e-prop\u2013approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence. Bellec et al. present a mathematically founded approximation for gradient descent training of recurrent neural networks without backwards propagation in time. This enables biologically plausible training of spike-based neural network models with working memory and supports on-chip training of neuromorphic hardware.",
      "publication_date": "2019-08-19",
      "venue": "Nature Communications",
      "year": 2019,
      "citation_count": 466,
      "authors": [
        "G. Bellec",
        "Franz Scherr",
        "Anand Subramoney",
        "Elias Hajek",
        "Darjan Salaj",
        "R. Legenstein",
        "W. Maass"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "57070167d647bfa23bc5c5895623af13576db66b",
      "title": "A tutorial on the free-energy framework for modelling perception and learning",
      "abstract": null,
      "publication_date": "2017-02-01",
      "venue": "Journal of Mathematical Psychology",
      "year": 2017,
      "citation_count": 269,
      "authors": [
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "198acc8504895b1ba3211ea3f1e8fe0cf106b753",
      "title": "Impression learning: Online representation learning with synaptic plasticity",
      "abstract": null,
      "publication_date": null,
      "venue": "Neural Information Processing Systems",
      "year": 2021,
      "citation_count": 9,
      "authors": [
        "C. Bredenberg",
        "Benjamin Lyo",
        "Eero P. Simoncelli",
        "Cristina Savin"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "b820ad4a35a6587b44ab03c0e70672a2ed5e9c5f",
      "title": "Accurate Path Integration in Continuous Attractor Network Models of Grid Cells",
      "abstract": "Grid cells in the rat entorhinal cortex display strikingly regular firing responses to the animal's position in 2-D space and have been hypothesized to form the neural substrate for dead-reckoning. However, errors accumulate rapidly when velocity inputs are integrated in existing models of grid cell activity. To produce grid-cell-like responses, these models would require frequent resets triggered by external sensory cues. Such inadequacies, shared by various models, cast doubt on the dead-reckoning potential of the grid cell system. Here we focus on the question of accurate path integration, specifically in continuous attractor models of grid cell activity. We show, in contrast to previous models, that continuous attractor models can generate regular triangular grid responses, based on inputs that encode only the rat's velocity and heading direction. We consider the role of the network boundary in the integration performance of the network and show that both periodic and aperiodic networks are capable of accurate path integration, despite important differences in their attractor manifolds. We quantify the rate at which errors in the velocity integration accumulate as a function of network size and intrinsic noise within the network. With a plausible range of parameters and the inclusion of spike variability, our model networks can accurately integrate velocity inputs over a maximum of \u223c10\u2013100 meters and \u223c1\u201310 minutes. These findings form a proof-of-concept that continuous attractor dynamics may underlie velocity integration in the dorsolateral medial entorhinal cortex. The simulations also generate pertinent upper bounds on the accuracy of integration that may be achieved by continuous attractor dynamics in the grid cell network. We suggest experiments to test the continuous attractor model and differentiate it from models in which single cells establish their responses independently of each other.",
      "publication_date": "2008-11-12",
      "venue": "PLoS Comput. Biol.",
      "year": 2008,
      "citation_count": 703,
      "authors": [
        "Y. Burak",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "fad77fa5f1b84eac354b6d6f7e011259092ce87d",
      "title": "What do grid cells contribute to place cell firing?",
      "abstract": null,
      "publication_date": "2014-03-01",
      "venue": "Trends in Neurosciences",
      "year": 2014,
      "citation_count": 151,
      "authors": [
        "D. Bush",
        "C. Barry",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "554a5385b955e3be4a0621b643b476c0a5514d46",
      "title": "What Does the Anatomical Organization of the Entorhinal Cortex Tell Us?",
      "abstract": "The entorhinal cortex is commonly perceived as a major input and output structure of the hippocampal formation, entertaining the role of the nodal point of cortico-hippocampal circuits. Superficial layers receive convergent cortical information, which is relayed to structures in the hippocampus, and hippocampal output reaches deep layers of entorhinal cortex, that project back to the cortex. The finding of the grid cells in all layers and reports on interactions between deep and superficial layers indicate that this rather simplistic perception may be at fault. Therefore, an integrative approach on the entorhinal cortex, that takes into account recent additions to our knowledge database on entorhinal connectivity, is timely. We argue that layers in entorhinal cortex show different functional characteristics most likely not on the basis of strikingly different inputs or outputs, but much more likely on the basis of differences in intrinsic organization, combined with very specific sets of inputs. Here, we aim to summarize recent anatomical data supporting the notion that the traditional description of the entorhinal cortex as a layered input-output structure for the hippocampal formation does not give the deserved credit to what this structure might be contributing to the overall functions of cortico-hippocampal networks.",
      "publication_date": "2008-08-28",
      "venue": "Journal of Neural Transplantation and Plasticity",
      "year": 2008,
      "citation_count": 407,
      "authors": [
        "Cathrin B. Canto",
        "F. Wouterlood",
        "M. Witter"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a538579ac50d659ac0bca9824d6446e741c586b3",
      "title": "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization",
      "abstract": "Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits.",
      "publication_date": "2018-02-15",
      "venue": "International Conference on Learning Representations",
      "year": 2018,
      "citation_count": 216,
      "authors": [
        "Christopher J. Cueva",
        "Xue-Xin Wei"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2495e0b51367beb828d68abbd00999b4bcdf3c44",
      "title": "Evidence for grid cells in a human memory network",
      "abstract": null,
      "publication_date": "2010-02-04",
      "venue": "Nature",
      "year": 2010,
      "citation_count": 706,
      "authors": [
        "Christian F. Doeller",
        "C. Barry",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2bf3295a0cec7163c063c0d3967cc0935396bb3e",
      "title": "Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis",
      "abstract": "Many recent models study the downstream projection from grid cells to place cells, while recent data have pointed out the importance of the feedback projection. We thus asked how grid cells are affected by the nature of the input from the place cells. We propose a single-layer neural network with feedforward weights connecting place-like input cells to grid cell outputs. Place-to-grid weights are learned via a generalized Hebbian rule. The architecture of this network highly resembles neural networks used to perform Principal Component Analysis (PCA). Both numerical results and analytic considerations indicate that if the components of the feedforward neural network are non-negative, the output converges to a hexagonal lattice. Without the non-negativity constraint, the output converges to a square lattice. Consistent with experiments, grid spacing ratio between the first two consecutive modules is \u22121.4. Our results express a possible linkage between place cell to grid cell interactions and PCA. DOI: http://dx.doi.org/10.7554/eLife.10094.001",
      "publication_date": "2016-03-08",
      "venue": "eLife",
      "year": 2016,
      "citation_count": 147,
      "authors": [
        "Yedidyah Dordek",
        "Daniel Soudry",
        "R. Meir",
        "D. Derdikman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "bc7417738a7f7186ce9b06933072c4ddd39ce4b1",
      "title": "Actionable Neural Representations: Grid Cells from Minimal Constraints",
      "abstract": "To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won't have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an `actionable representation'. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode.",
      "publication_date": "2022-09-30",
      "venue": "International Conference on Learning Representations",
      "year": 2022,
      "citation_count": 23,
      "authors": [
        "W. Dorrell",
        "P. Latham",
        "T. Behrens",
        "James C. R. Whittington"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "28c3c3d07d27b854babfac7b109ba15fca09437c",
      "title": "Evidence for area CA1 as a match/mismatch detector: A high\u2010resolution fMRI study of the human hippocampus",
      "abstract": null,
      "publication_date": "2012-03-01",
      "venue": "Hippocampus",
      "year": 2012,
      "citation_count": 238,
      "authors": [
        "Katherine Duncan",
        "Nicholas A. Ketz",
        "S. Inati",
        "L. Davachi"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8fcbc38e7196b0dc8748f04cd6101e71f92c158e",
      "title": "A theory of cortical responses",
      "abstract": null,
      "publication_date": "2005-04-29",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences",
      "year": 2005,
      "citation_count": 3972,
      "authors": [
        "Karl J. Friston"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "f4649234d52d1848495dcebd9f0cc9042e636bff",
      "title": "A Spin Glass Model of Path Integration in Rat Medial Entorhinal Cortex",
      "abstract": "Electrophysiological recording studies in the dorsocaudal region of medial entorhinal cortex (dMEC) of the rat reveal cells whose spatial firing fields show a remarkably regular hexagonal grid pattern (Fyhn et al., 2004; Hafting et al., 2005). We describe a symmetric, locally connected neural network, or spin glass model, that spontaneously produces a hexagonal grid of activity bumps on a two-dimensional sheet of units. The spatial firing fields of the simulated cells closely resemble those of dMEC cells. A collection of grids with different scales and/or orientations forms a basis set for encoding position. Simulations show that the animal\u2019s location can easily be determined from the population activity pattern. Introducing an asymmetry in the model allows the activity bumps to be shifted in any direction, at a rate proportional to velocity, to achieve path integration. Furthermore, information about the structure of the environment can be superimposed on the spatial position signal by modulation of the bump activity levels without significantly interfering with the hexagonal periodicity of firing fields. Our results support the conjecture of Hafting et al. (2005) that an attractor network in dMEC may be the source of path integration information afferent to hippocampus.",
      "publication_date": "2006-04-19",
      "venue": "Journal of Neuroscience",
      "year": 2006,
      "citation_count": 617,
      "authors": [
        "Mark C. Fuhs",
        "D. Touretzky"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "84c888d94d93137b767ec982c58e6829b10ed88a",
      "title": "Grid cells in mice",
      "abstract": null,
      "publication_date": "2008-12-01",
      "venue": "Hippocampus",
      "year": 2008,
      "citation_count": 174,
      "authors": [
        "M. Fyhn",
        "T. Hafting",
        "M. Witter",
        "E. Moser",
        "M. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "cd98854d3482e276b7124712c02e20ed48f56a0f",
      "title": "A generative model of the hippocampal formation trained with theta driven local learning rules",
      "abstract": "Advances in generative models have recently revolutionised machine learning. Meanwhile, in neuroscience, generative models have long been thought fundamental to animal intelligence. Understanding the biological mechanisms that support these processes promises to shed light on the relationship between biological and artificial intelligence. In animals, the hippocampal formation is thought to learn and use a generative model to support its role in spatial and non-spatial memory. Here we introduce a biologically plausible model of the hippocampal formation tantamount to a Helmholtz machine that we apply to a temporal stream of inputs. A novel component of our model is that fast theta-band oscillations (5-10 Hz) gate the direction of information flow throughout the network, training it akin to a high-frequency wake-sleep algorithm. Our model accurately infers the latent state of high-dimensional sensory environments and generates realistic sensory predictions. Furthermore, it can learn to path integrate by developing a ring attractor connectivity structure matching previous theoretical proposals and flexibly transfer this structure between environments. Whereas many models trade-off biological plausibility with generality, our model captures a variety of hippocampal cognitive functions under one biologically plausible local learning rule.",
      "publication_date": "2023-12-13",
      "venue": "bioRxiv",
      "year": 2023,
      "citation_count": 9,
      "authors": [
        "Tom M George",
        "C. Barry",
        "K. Stachenfeld",
        "C. Clopath",
        "T. Fukai"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "57eb4d347212a9123ff2cfc17839ccd231f6b4b9",
      "title": "Computational Models of Grid Cells",
      "abstract": null,
      "publication_date": "2011-08-25",
      "venue": "Neuron",
      "year": 2011,
      "citation_count": 218,
      "authors": [
        "Lisa M. Giocomo",
        "M. Moser",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "b0004097eac896ee4bf8b45ba3a7d9f101782542",
      "title": "Microstructure of a spatial map in the entorhinal cortex",
      "abstract": null,
      "publication_date": "2005-08-11",
      "venue": "Nature",
      "year": 2005,
      "citation_count": 3671,
      "authors": [
        "T. Hafting",
        "M. Fyhn",
        "S. Molden",
        "M. Moser",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "14a90fb87242fd7033eca54671f9f1f7c09c05e1",
      "title": "Grid cell firing may arise from interference of theta frequency membrane potential oscillations in single neurons",
      "abstract": null,
      "publication_date": "2007-12-01",
      "venue": "Hippocampus",
      "year": 2007,
      "citation_count": 265,
      "authors": [
        "M. Hasselmo",
        "Lisa M. Giocomo",
        "Eric A. Zilli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "252b2d7ab977ed1d1dfc659294318bb2394410d7",
      "title": "Basket-like interneurones in layer II of the entorhinal cortex exhibit a powerful NMDA-mediated synaptic excitation",
      "abstract": null,
      "publication_date": "1993-01-04",
      "venue": "Neuroscience Letters",
      "year": 1993,
      "citation_count": 185,
      "authors": [
        "R. Jones",
        "E. H. B\u00fchl"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1c9852b8cbeb0668cfd5e3aeef01f9ce9a123843",
      "title": "The emergence of grid cells: Intelligent design or just adaptation?",
      "abstract": null,
      "publication_date": "2008-12-01",
      "venue": "Hippocampus",
      "year": 2008,
      "citation_count": 245,
      "authors": [
        "E. Kropff",
        "A. Treves"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6ea99443ff4d599ee67b42a5afd5c7bf6729c2c5",
      "title": "Neural Representations of Location Composed of Spatially Periodic Bands",
      "abstract": null,
      "publication_date": "2012-08-17",
      "venue": "Science",
      "year": 2012,
      "citation_count": 175,
      "authors": [
        "J. Krupic",
        "N. Burgess",
        "J. O\u2019Keefe"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "ae47b8bef62e47285f5acc7d81ea3e265171e2ef",
      "title": "The contributions of entorhinal cortex and hippocampus to error driven learning",
      "abstract": "Computational models proposed that the medial temporal lobe (MTL) contributes importantly to error-driven learning, though little direct in-vivo evidence for this hypothesis exists. To test this, we recorded in the entorhinal cortex (EC) and hippocampus (HPC) as macaques performed an associative learning task using an error-driven learning strategy, defined as better performance after error relative to correct trials. Error-detection signals were more prominent in the EC relative to HPC. Early in learning hippocampal but not EC neurons signaled error-driven learning by increasing their population stimulus-selectivity following error trials. This same pattern was not seen in another task where error-driven learning was not used. After learning, different populations of cells in both the EC and HPC signaled long-term memory of newly learned associations with enhanced stimulus-selective responses. These results suggest prominent but differential contributions of EC and HPC to learning from errors and a particularly important role of the EC in error-detection. Ku et al. recorded in the entorhinal cortex (EC) and hippocampus (HPC) of macaques during associative learning tasks in order to test the computational model prediction that they contribute to error-driven learning. They demonstrate that the EC and HPC have prominent but differential contributions to learning from errors, with the EC having a particularly prominent role in error-detection.",
      "publication_date": "2020-09-29",
      "venue": "Communications Biology",
      "year": 2020,
      "citation_count": 7,
      "authors": [
        "S-P Ku",
        "E. Hargreaves",
        "S. Wirth",
        "W. Suzuki"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3edd190cc7d541f6ae410e5be3d82868532a39f6",
      "title": "Development of the Spatial Representation System in the Rat",
      "abstract": null,
      "publication_date": "2010-06-18",
      "venue": "Science",
      "year": 2010,
      "citation_count": 600,
      "authors": [
        "R. Langston",
        "J. Ainge",
        "J. J. Couey",
        "Cathrin B. Canto",
        "T. Bjerknes",
        "M. Witter",
        "E. Moser",
        "M. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "42ce761c85bdb0d422917b03751ab9cbc72a3417",
      "title": "Backpropagation through time and the brain",
      "abstract": null,
      "publication_date": "2019-03-07",
      "venue": "Current Opinion in Neurobiology",
      "year": 2019,
      "citation_count": 127,
      "authors": [
        "T. Lillicrap",
        "Adam Santoro"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "c04713059cfdbff7f3cce0b10a5fd6fc7cdee43e",
      "title": "Relating Hippocampal Circuitry to Function Recall of Memory Sequences by Reciprocal Dentate\u2013CA3 Interactions",
      "abstract": null,
      "publication_date": "1999-02-01",
      "venue": "Neuron",
      "year": 1999,
      "citation_count": 632,
      "authors": [
        "J. Lisman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "998628588f7850d533a172c883872057a9198d82",
      "title": "Predictive Coding: a Theoretical and Experimental Review",
      "abstract": "Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.",
      "publication_date": "2021-07-27",
      "venue": "arXiv.org",
      "year": 2021,
      "citation_count": 127,
      "authors": [
        "Beren Millidge",
        "A. Seth",
        "C. Buckley"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2c0203ff41fbe8f3fc2a0e706ecb3ecf806f2108",
      "title": "Predictive Coding Approximates Backprop Along Arbitrary Computation Graphs",
      "abstract": "Abstract Backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. Recently it has been shown that backprop in multilayer perceptrons (MLPs) can be approximated using predictive coding, a biologically plausible process theory of cortical computation that relies solely on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs but in the concept of automatic differentiation, which allows for the optimization of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice, rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding convolutional neural networks, recurrent neural networks, and the more complex long short-term memory, which include a nonlayer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks while using only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry and may also contribute to the development of completely distributed neuromorphic architectures.",
      "publication_date": "2020-06-07",
      "venue": "Neural Computation",
      "year": 2020,
      "citation_count": 119,
      "authors": [
        "Beren Millidge",
        "Alexander Tschantz",
        "C. Buckley"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "870d2b9c71c4448498a47870f2622f0a2db44b20",
      "title": "Predictive coding networks for temporal prediction",
      "abstract": "One of the key problems the brain faces is inferring the state of the world from a sequence of dynamically changing stimuli, and it is not yet clear how the sensory system achieves this task. A well-established computational framework for describing perceptual processes in the brain is provided by the theory of predictive coding. Although the original proposals of predictive coding have discussed temporal prediction, later work developing this theory mostly focused on static stimuli, and key questions on neural implementation and computational properties of temporal predictive coding networks remain open. Here, we address these questions and present a formulation of the temporal predictive coding model that can be naturally implemented in recurrent networks, in which activity dynamics rely only on local inputs to the neurons, and learning only utilises local Hebbian plasticity. Additionally, we show that temporal predictive coding networks can approximate the performance of the Kalman filter in predicting behaviour of linear systems, and behave as a variant of a Kalman filter which does not track its own subjective posterior variance. Importantly, temporal predictive coding networks can achieve similar accuracy as the Kalman filter without performing complex mathematical operations, but just employing simple computations that can be implemented by biological networks. Moreover, when trained with natural dynamic inputs, we found that temporal predictive coding can produce Gabor-like, motion-sensitive receptive fields resembling those observed in real neurons in visual areas. In addition, we demonstrate how the model can be effectively generalized to nonlinear systems. Overall, models presented in this paper show how biologically plausible circuits can predict future stimuli and may guide research on understanding specific neural circuits in brain areas involved in temporal prediction. Author summary While significant advances have been made in the neuroscience of how the brain processes static stimuli, the time dimension has often been relatively neglected. However, time is crucial since the stimuli perceived by our senses typically dynamically vary in time, and the cortex needs to make sense of these changing inputs. This paper describes a computational model of cortical networks processing temporal stimuli. This model is able to infer and track the state of the environment based on noisy inputs, and predict future sensory stimuli. By ensuring that these predictions match the incoming stimuli, the model is able to learn the structure and statistics of its temporal inputs and produces responses of neurons resembling those in the brain. The model may help in further understanding neural circuits in sensory cortical areas.",
      "publication_date": "2024-03-09",
      "venue": "bioRxiv",
      "year": 2024,
      "citation_count": 20,
      "authors": [
        "Beren Millidge",
        "Mufeng Tang",
        "Mahyar Osanlouy",
        "N. Harper",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "0c48611df1f03ca7cec7f98a1ffaadf95dec1260",
      "title": "Grids from bands, or bands from grids? An examination of the effects of single unit contamination on grid cell firing fields.",
      "abstract": "Neural recording technology is improving rapidly, allowing for the detection of spikes from hundreds of cells simultaneously. The limiting step in multielectrode electrophysiology continues to be single cell isolation. However, this step is crucial to the interpretation of data from putative single neurons. We present here, in simulation, an illustration of possibly erroneous conclusions that may be reached when poorly isolated single cell data are analyzed. Grid cells are neurons recorded in rodents, and bats, that spike in equally spaced locations in a hexagonal pattern. One theory states that grid firing patterns arise from a combination of band firing patterns. However, we show here that summing the grid firing patterns of two poorly resolved neurons can result in spurious band-like patterns. Thus, evidence of neurons spiking in band patterns must undergo extreme scrutiny before it is accepted. Toward this aim, we discuss single cell isolation methods and metrics.",
      "publication_date": "2016-02-01",
      "venue": "Journal of Neurophysiology",
      "year": 2016,
      "citation_count": 21,
      "authors": [
        "Zaneta Navratilova",
        "Zaneta Navratilova",
        "Keith B. Godfrey",
        "B. McNaughton",
        "B. McNaughton"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6a77b5266b492c69dec62163cd692f86b9b72860",
      "title": "Emergent elasticity in the neural code for space",
      "abstract": "Significance We develop a theoretical model, grounded in known properties of neural dynamics and synaptic plasticity, that can fuse information gathered from the past history of velocity and sequence of encountered landmarks during exploratory behavior, to construct a self-consistent internal representation of space. Moreover, through model reduction techniques, we obtain conceptual insights into how consistent internal spatial representations naturally emerge through an elastic relaxation process in an effective spring\u2013particle system. We verify several experimentally testable predictions of our model involving the spatial behavior of grid cells in the medial entorhinal cortex, as well as suggest additional experiments. Upon encountering a novel environment, an animal must construct a consistent environmental map, as well as an internal estimate of its position within that map, by combining information from two distinct sources: self-motion cues and sensory landmark cues. How do known aspects of neural circuit dynamics and synaptic plasticity conspire to accomplish this feat? Here we show analytically how a neural attractor model that combines path integration of self-motion cues with Hebbian plasticity in synaptic weights from landmark cells can self-organize a consistent map of space as the animal explores an environment. Intriguingly, the emergence of this map can be understood as an elastic relaxation process between landmark cells mediated by the attractor network. Moreover, our model makes several experimentally testable predictions, including (i) systematic path-dependent shifts in the firing fields of grid cells toward the most recently encountered landmark, even in a fully learned environment; (ii) systematic deformations in the firing fields of grid cells in irregular environments, akin to elastic deformations of solids forced into irregular containers; and (iii) the creation of topological defects in grid cell firing patterns through specific environmental manipulations. Taken together, our results conceptually link known aspects of neurons and synapses to an emergent solution of a fundamental computational problem in navigation, while providing a unified account of disparate experimental observations.",
      "publication_date": "2018-05-21",
      "venue": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": 2018,
      "citation_count": 70,
      "authors": [
        "Samuel A. Ocko",
        "Kiah Hardcastle",
        "Lisa M. Giocomo",
        "S. Ganguli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6aa68970ec381a78a75fc30555884d4e8f1cb672",
      "title": "Place units in the hippocampus of the freely moving rat",
      "abstract": null,
      "publication_date": "1976-12-31",
      "venue": "Experimental Neurology",
      "year": 1976,
      "citation_count": 1807,
      "authors": [
        "J. O\u2019Keefe"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "de76cd6c61199445e2688402cabbb3119dadc877",
      "title": "Dual phase and rate coding in hippocampal place cells: Theoretical significance and relationship to entorhinal grid cells",
      "abstract": null,
      "publication_date": null,
      "venue": "Hippocampus",
      "year": 2005,
      "citation_count": 521,
      "authors": [
        "J. O\u2019Keefe",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
      "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
      "abstract": null,
      "publication_date": "1996-06-13",
      "venue": "Nature",
      "year": 1996,
      "citation_count": 6169,
      "authors": [
        "B. Olshausen",
        "D. Field"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "c1585013b1dbb7341a8a6821159d8d7681e81503",
      "title": "Predictive grid coding in the medial entorhinal cortex",
      "abstract": "The entorhinal cortex represents allocentric spatial geometry and egocentric speed and heading information required for spatial navigation. However, it remains unclear whether it contributes to the prediction of an animal\u2019s future location. We discovered grid cells in the medial entorhinal cortex (MEC) that have grid fields representing future locations during goal-directed behavior. These predictive grid cells represented prospective spatial information by shifting their grid fields against the direction of travel. Predictive grid cells discharged at the trough phases of the hippocampal CA1 theta oscillation and, together with other types of grid cells, organized sequences of the trajectory from the current to future positions across each theta cycle. Our results suggest that the MEC provides a predictive map that supports forward planning in spatial navigation. Editor\u2019s summary Grid cells in the entorhinal cortex create a coordinate system of the environment for spatial navigation. However, it is not clear whether the entorhinal grid system is also involved in predicting where an animal will be in the next moment. Ouchi and Fujisawa performed high-density neuronal recordings in the entorhinal cortex and hippocampal area CA1 of rats during goal-directed behavior in an open field. They observed neurons in layer 3 of the medial entorhinal cortex that explicitly encode a grid representation of an animal\u2019s future projected location, not the current position, and named them \u201c\u201cpredictive grid cells.\u201d Neuronal assemblies in the medial entorhinal cortex thus organize a predictive cognitive map. \u2014Peter Stern",
      "publication_date": "2024-08-16",
      "venue": "Science",
      "year": 2024,
      "citation_count": 7,
      "authors": [
        "Ayako Ouchi",
        "Shigeyoshi Fujisawa"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a424ec3b8846f57b8ffdb566d272e28d5a525909",
      "title": "Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects.",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 1999,
      "citation_count": 4448,
      "authors": [
        "Rajesh P. N. Rao",
        "D. Ballard"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "abb5aa3ff57e043bbaf59239ab27150a2545aee5",
      "title": "Associative Memories via Predictive Coding",
      "abstract": "Associative memories in the brain receive and store patterns of activity registered by the sensory neurons, and are able to retrieve them when necessary. Due to their importance in human intelligence, computational models of associative memories have been developed for several decades now. In this paper, we present a novel neural model for realizing associative memories, which is based on a hierarchical generative network that receives external stimuli via sensory neurons. It is trained using predictive coding, an error-based learning algorithm inspired by information processing in the cortex. To test the model's capabilities, we perform multiple retrieval experiments from both corrupted and incomplete data points. In an extensive comparison, we show that this new model outperforms in retrieval accuracy and robustness popular associative memory models, such as autoencoders trained via backpropagation, and modern Hopfield networks. In particular, in completing partial data points, our model achieves remarkable results on natural image datasets, such as ImageNet, with a surprisingly high accuracy, even when only a tiny fraction of pixels of the original images is presented. Our model provides a plausible framework to study learning and retrieval of memories in the brain, as it closely mimics the behavior of the hippocampus as a memory index and generative model.",
      "publication_date": "2021-09-16",
      "venue": "Neural Information Processing Systems",
      "year": 2021,
      "citation_count": 64,
      "authors": [
        "Tommaso Salvatori",
        "Yuhang Song",
        "Yujian Hong",
        "Simon Frieder",
        "Lei Sha",
        "Zhenghua Xu",
        "R. Bogacz",
        "Thomas Lukasiewicz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "e83816c0e7ed01ee48d579568ba97db72931d979",
      "title": "Learning on Arbitrary Graph Topologies via Predictive Coding",
      "abstract": "Training with backpropagation (BP) in standard deep learning consists of two main steps: a forward pass that maps a data point to its prediction, and a backward pass that propagates the error of this prediction back through the network. This process is highly effective when the goal is to minimize a specific objective function. However, it does not allow training on networks with cyclic or backward connections. This is an obstacle to reaching brain-like capabilities, as the highly complex heterarchical structure of the neural connections in the neocortex are potentially fundamental for its effectiveness. In this paper, we show how predictive coding (PC), a theory of information processing in the cortex, can be used to perform inference and learning on arbitrary graph topologies. We experimentally show how this formulation, called PC graphs, can be used to flexibly perform different tasks with the same network by simply stimulating specific neurons. This enables the model to be queried on stimuli with different structures, such as partial images, images with labels, or images without labels. We conclude by investigating how the topology of the graph influences the final performance, and comparing against simple baselines trained with BP.",
      "publication_date": "2022-01-31",
      "venue": "Neural Information Processing Systems",
      "year": 2022,
      "citation_count": 34,
      "authors": [
        "Tommaso Salvatori",
        "Luca Pinchetti",
        "Beren Millidge",
        "Yuhang Song",
        "R. Bogacz",
        "Thomas Lukasiewicz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "709b4bfc5198336ba5d70da987889a157f695c1e",
      "title": "Optimal unsupervised learning in a single-layer linear feedforward neural network",
      "abstract": null,
      "publication_date": null,
      "venue": "Neural Networks",
      "year": 1989,
      "citation_count": 1603,
      "authors": [
        "T. Sanger"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "4b5daaa4edd3caadfb1fa214f44140877828051f",
      "title": "Conjunctive Representation of Position, Direction, and Velocity in Entorhinal Cortex",
      "abstract": null,
      "publication_date": "2006-05-05",
      "venue": "Science",
      "year": 2006,
      "citation_count": 609,
      "authors": [
        "Francesca Sargolini",
        "M. Fyhn",
        "T. Hafting",
        "Bruce L. McNaughton",
        "M. Witter",
        "M. Moser",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "25b422756cbddb3193002c9a49c844641fff2127",
      "title": "No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit",
      "abstract": "Research in Neuroscience, as in many scientific disciplines, is undergoing a renaissance based on deep learning. Unique to Neuroscience, deep learning models can be used not only as a tool but interpreted as models of the brain. The central claims of recent deep learning-based models of brain circuits are that they make novel predictions about neural phenomena or shed light on the fundamental functions being optimized. We show, through the case-study of grid cells in the entorhinal-hippocampal circuit, that one may get neither. We begin by reviewing the principles of grid cell mechanism and function obtained from first-principles modeling efforts, then rigorously examine the claims of deep learning models of grid cells. Using large-scale architectural and hyperparameter sweeps and theory-driven experimentation, we demonstrate that the results of such models may be more strongly driven by particular, non-fundamental, and post-hoc implementation choices than fundamental truths about neural circuits or the loss function(s) they might optimize. We discuss why these models cannot be expected to produce accurate models of the brain without the addition of substantial amounts of inductive bias, an informal No Free Lunch result for Neuroscience. Based on first principles work, we provide hypotheses for what additional loss functions will produce grid cells more robustly. In conclusion, circumspection and transparency, together with biological knowledge, are warranted in building and interpreting deep learning models in Neuroscience.",
      "publication_date": "2023-08-15",
      "venue": "bioRxiv",
      "year": 2023,
      "citation_count": 57,
      "authors": [
        "Rylan Schaeffer",
        "Mikail Khona",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "27bc20a95969cffa29065fcc1beb63ee2386b852",
      "title": "Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells",
      "abstract": "To solve the spatial problems of mapping, localization and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learnt this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, there is no satisfactory synthesis of core principles that lead to multi-modular grid cells in deep recurrent neural networks. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: coding theory, dynamical systems, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, without access to supervised position information or engineering of particular readout representations as needed in previous approaches. We show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks and emergent representations generalize well outside their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.",
      "publication_date": "2023-11-04",
      "venue": "Neural Information Processing Systems",
      "year": 2023,
      "citation_count": 20,
      "authors": [
        "Rylan Schaeffer",
        "Mikail Khona",
        "Tzuhsuan Ma",
        "Cristobal Eyzaguirre",
        "Sanmi Koyejo",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "f24588ae91d1015ef9e85004624db6434448403a",
      "title": "Biological Softmax: Demonstrated in Modern Hopfield Networks",
      "abstract": null,
      "publication_date": null,
      "venue": "Annual Meeting of the Cognitive Science Society",
      "year": 2022,
      "citation_count": 3,
      "authors": [
        "Mallory A. Snow",
        "Jeff Orchard"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "35ce44e06802d9f778eb02bf03a6fc5aa657bd56",
      "title": "Inferring neural activity before plasticity as a foundation for learning beyond backpropagation",
      "abstract": "This paper introduces \u2018prospective configuration\u2019, a new principle for learning in neural networks, which differs from backpropagation and is more efficient in learning and more consistent with data on neural activity and behavior. For both humans and machines, the essence of learning is to pinpoint which components in its information processing pipeline are responsible for an error in its output, a challenge that is known as \u2018credit assignment\u2019. It has long been assumed that credit assignment is best solved by backpropagation, which is also the foundation of modern machine learning. Here, we set out a fundamentally different principle on credit assignment called \u2018prospective configuration\u2019. In prospective configuration, the network first infers the pattern of neural activity that should result from learning, and then the synaptic weights are modified to consolidate the change in neural activity. We demonstrate that this distinct mechanism, in contrast to backpropagation, (1) underlies learning in a well-established family of models of cortical circuits, (2) enables learning that is more efficient and effective in many contexts faced by biological organisms and (3) reproduces surprising patterns of neural activity and behavior observed in diverse human and rat learning experiments.",
      "publication_date": "2024-01-03",
      "venue": "Nature Neuroscience",
      "year": 2024,
      "citation_count": 16,
      "authors": [
        "Yuhang Song",
        "Beren Millidge",
        "Tommaso Salvatori",
        "Thomas Lukasiewicz",
        "Zhenghua Xu",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "791686d4fa91082e225797a13060adeeb0fddf13",
      "title": "A unified theory for the computational and mechanistic origins of grid cells",
      "abstract": "The discovery of entorhinal grid cells has generated considerable interest in how and why hexagonal firing fields might mechanistically emerge in a generic manner from neural circuits, and what their computational significance might be. Here we forge an intimate link between the computational problem of path-integration and the existence of hexagonal grids, by demonstrating that such grids arise generically in biologically plausible neural networks trained to path integrate. Moreover, we develop a unifying theory for why hexagonal grids are so ubiquitous in path-integrator circuits. Such trained networks also yield powerful mechanistic hypotheses, exhibiting realistic levels of biological variability not captured by hand-designed models. We furthermore develop methods to analyze the connectome and activity maps of our trained networks to elucidate fundamental mechanisms underlying path integration. These methods provide an instructive roadmap to go from connectomic and physiological measurements to conceptual understanding in a manner that might be generalizable to other settings.",
      "publication_date": "2020-12-30",
      "venue": "Neuron",
      "year": 2020,
      "citation_count": 83,
      "authors": [
        "Ben Sorscher",
        "Gabriel C. Mel",
        "Samuel A. Ocko",
        "Lisa M. Giocomo",
        "S. Ganguli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1fe70364ca7c7cf3b5008969518fd3257e62a232",
      "title": "The hippocampus as a predictive map",
      "abstract": "A cognitive map has long been the dominant metaphor for hippocampal function, embracing the idea that place cells encode a geometric representation of space. However, evidence for predictive coding, reward sensitivity and policy dependence in place cells suggests that the representation is not purely spatial. We approach this puzzle from a reinforcement learning perspective: what kind of spatial representation is most useful for maximizing future reward? We show that the answer takes the form of a predictive representation. This representation captures many aspects of place cell responses that fall outside the traditional view of a cognitive map. Furthermore, we argue that entorhinal grid cells encode a low-dimensionality basis set for the predictive representation, useful for suppressing noise in predictions and extracting multiscale structure for hierarchical planning.",
      "publication_date": "2017-06-07",
      "venue": "Nature Neuroscience",
      "year": 2017,
      "citation_count": 757,
      "authors": [
        "Kimberly L. Stachenfeld",
        "M. Botvinick",
        "S. Gershman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "26fc224a118e20e5d6bc98b57711001abc20ec00",
      "title": "Recurrent predictive coding models for associative memory employing covariance learning",
      "abstract": "The computational principles adopted by the hippocampus in associative memory (AM) tasks have been one of the mostly studied topics in computational and theoretical neuroscience. Classical models of the hippocampal network assume that AM is performed via a form of covariance learning, where associations between memorized items are represented by entries in the learned covariance matrix encoded in the recurrent connections in the hippocampal subfield CA3. On the other hand, it has been recently proposed that AM in the hippocampus is achieved through predictive coding. Hierarchical predictive coding models following this theory perform AM, but fail to capture the recurrent hippocampal structure that encodes the covariance in the classical models. Such a dichotomy pose potential difficulties for developing a unitary theory of how memory is formed and recalled in the hippocampus. Earlier predictive coding models that learn the covariance information of inputs explicitly seem to be a solution to this dichotomy. Here, we show that although these models can perform AM, they do it in an implausible and numerically unstable way. Instead, we propose alternatives to these earlier covariance-learning predictive coding networks, which learn the covariance information implicitly and plausibly, and can use dendritic structures to encode prediction errors. We show analytically that our proposed models are perfectly equivalent to the earlier predictive coding model learning covariance explicitly, and encounter no numerical issues when performing AM tasks in practice. We further show that our models can be combined with hierarchical predictive coding networks to model the hippocampo-neocortical interactions. Our models provide a biologically plausible approach to modelling the hippocampal network, pointing to a potential computational mechanism employed by the hippocampus during memory formation and recall, which unifies predictive coding and covariance learning based on the recurrent network structure. Author summary The hippocampus and adjacent cortical areas have long been considered essential for the formation of associative memories. Earlier theoretical works have assumed that the hippocampus stores in its recurrent connections statistical regularities embedded in the sensory inputs. On the other hand, it has been recently suggested that the hippocampus retrieves memory by generating predictions of ongoing sensory inputs. Computational models have thus been proposed to account for this predictive nature of the hippocampal network using predictive coding, a general theory of information processing in the cortex. However, these hierarchical predictive coding models of the hippocampus did not describe how it stores the statistical regularities that play a key role for associative memory in the classical hippocampal models, hindering a unified understanding of the underlying computational principles employed by the hippocampus. To address this dichotomy, here we present a family of predictive coding models that also learn the statistical information needed for associative memory. Our models can stably perform associative memory tasks in a biologically plausible manner, even with large structured data such as natural scenes. Our work provides a possible mechanism of how the recurrent hippocampal network may employ various computational principles concurrently to perform associative memory.",
      "publication_date": "2022-11-09",
      "venue": "bioRxiv",
      "year": 2022,
      "citation_count": 25,
      "authors": [
        "Mufeng Tang",
        "Tommaso Salvatori",
        "Beren Millidge",
        "Yuhang Song",
        "Thomas Lukasiewicz",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8a8f7953894fef09d32c5f35fd79cfbff025e56f",
      "title": "Sequential Memory with Temporal Predictive Coding",
      "abstract": "Forming accurate memory of sequential stimuli is a fundamental function of biological agents. However, the computational mechanism underlying sequential memory in the brain remains unclear. Inspired by neuroscience theories and recent successes in applying predictive coding (PC) to static memory tasks, in this work we propose a novel PC-based model for sequential memory, called temporal predictive coding (tPC). We show that our tPC models can memorize and retrieve sequential inputs accurately with a biologically plausible neural implementation. Importantly, our analytical study reveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN) with an implicit statistical whitening process, which leads to more stable performance in sequential memory tasks of structured inputs. Moreover, we find that tPC exhibits properties consistent with behavioral observations and theories in neuroscience, thereby strengthening its biological relevance. Our work establishes a possible computational mechanism underlying sequential memory in the brain that can also be theoretically interpreted using existing memory model frameworks.",
      "publication_date": "2023-05-19",
      "venue": "Neural Information Processing Systems",
      "year": 2023,
      "citation_count": 19,
      "authors": [
        "Mufeng Tang",
        "Helen C. Barron",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "138a75cab26100df70bdb66dd1a28e68612db8ff",
      "title": "Learning place cells, grid cells and invariances with excitatory and inhibitory plasticity",
      "abstract": "Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction, and the underlying circuit mechanisms are not yet resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, place cells are typically invariant to head direction. We propose that all observed spatial tuning patterns \u2013 in both their selectivity and their invariance \u2013 arise from the same mechanism: Excitatory and inhibitory synaptic plasticity driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. Our proposed model is robust to changes in parameters, develops patterns on behavioral timescales and makes distinctive experimental predictions.",
      "publication_date": "2018-02-21",
      "venue": "eLife",
      "year": 2018,
      "citation_count": 50,
      "authors": [
        "Simon Nikolaus Weber",
        "Henning Sprekeler"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "c124a6aec4b1833e4e86092e20a782183349d57e",
      "title": "An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity",
      "abstract": "Abstract To efficiently learn from feedback, cortical networks need to update synaptic weights on multiple levels of cortical hierarchy. An effective and well-known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm. However, in this algorithm, the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons. Several models have been proposed that approximate the backpropagation algorithm with local synaptic plasticity, but these models require complex external control over the network or relatively complex plasticity rules. Here we show that a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple local Hebbian plasticity. Furthermore, for certain parameters, the weight change in the predictive coding model converges to that of the backpropagation algorithm. This suggests that it is possible for cortical networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output.",
      "publication_date": "2017-03-23",
      "venue": "Neural Computation",
      "year": 2017,
      "citation_count": 284,
      "authors": [
        "James C. R. Whittington",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "f509100a8b6ec4036798fe857ea7ca75572b8278",
      "title": "The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation",
      "abstract": null,
      "publication_date": "2019-09-16",
      "venue": "Cell",
      "year": 2019,
      "citation_count": 447,
      "authors": [
        "James C. R. Whittington",
        "Timothy H. Muller",
        "Shirley Mark",
        "Guifen Chen",
        "C. Barry",
        "N. Burgess",
        "T. Behrens"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "bf2f79d0f4accdf9b42dd7be901b77c6bd5c88ed",
      "title": "A Model of Grid Cell Development through Spatial Exploration and Spike Time-Dependent Plasticity",
      "abstract": null,
      "publication_date": "2014-07-16",
      "venue": "Neuron",
      "year": 2014,
      "citation_count": 93,
      "authors": [
        "John Widloski",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2ae5a5507253aa3cada113d41d35fada1e84555f",
      "title": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories",
      "abstract": null,
      "publication_date": "1990-12-01",
      "venue": "Neural Computation",
      "year": 1990,
      "citation_count": 718,
      "authors": [
        "Ronald J. Williams",
        "Jing Peng"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "4935965339f2ae6990cc72721a2bed33c6c4121a",
      "title": "Development of the Hippocampal Cognitive Map in Preweanling Rats",
      "abstract": null,
      "publication_date": "2010-06-18",
      "venue": "Science",
      "year": 2010,
      "citation_count": 565,
      "authors": [
        "Thomas J. Wills",
        "F. Cacucci",
        "N. Burgess",
        "J. O\u2019Keefe"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a8c9ad94386d3a08e11cf653bca6da51ce8d1d79",
      "title": "Passive Transport Disrupts Grid Signals in the Parahippocampal Cortex",
      "abstract": null,
      "publication_date": "2015-10-05",
      "venue": "Current Biology",
      "year": 2015,
      "citation_count": 75,
      "authors": [
        "Shawn S. Winter",
        "Max L Mehlman",
        "B. J. Clark",
        "J. Taube"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d157e8ada2d3cd7c634f08e293f6b4ce0bc13e34",
      "title": "Trial Outcome and Associative Learning Signals in the Monkey Hippocampus",
      "abstract": null,
      "publication_date": "2009-03-01",
      "venue": "Neuron",
      "year": 2009,
      "citation_count": 73,
      "authors": [
        "S. Wirth",
        "E. Av\u015far",
        "Cindy C. Chiu",
        "V. Sharma",
        "Anne C. Smith",
        "E. Brown",
        "W. Suzuki"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2e108d408a92abbbfa3cabffc3c8a3b0a62c6004",
      "title": "Spatial representation and the architecture of the entorhinal cortex",
      "abstract": null,
      "publication_date": "2006-12-01",
      "venue": "Trends in Neurosciences",
      "year": 2006,
      "citation_count": 228,
      "authors": [
        "M. Witter",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d7c42c3a58f05ba4c1aaf3754691ff6fb44f43ba",
      "title": "Grid cells without theta oscillations in the entorhinal cortex of bats",
      "abstract": null,
      "publication_date": "2011-11-03",
      "venue": "Nature",
      "year": 2011,
      "citation_count": 383,
      "authors": [
        "M. Yartsev",
        "M. Witter",
        "N. Ulanovsky"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "b9ea31c9c8719a504dd349feffbd79c7c8d18e23",
      "title": "Coupled Noisy Spiking Neurons as Velocity-Controlled Oscillators in a Model of Grid Cell Spatial Firing",
      "abstract": "One of the two primary classes of models of grid cell spatial firing uses interference between oscillators at dynamically modulated frequencies. Generally, these models are presented in terms of idealized oscillators (modeled as sinusoids), which differ from biological oscillators in multiple important ways. Here we show that two more realistic, noisy neural models (Izhikevich's simple model and a biophysical model of an entorhinal cortex stellate cell) can be successfully used as oscillators in a model of this type. When additive noise is included in the models such that uncoupled or sparsely coupled cells show realistic interspike interval variance, both synaptic and gap-junction coupling can synchronize networks of cells to produce comparatively less variable network-level oscillations. We show that the frequency of these oscillatory networks can be controlled sufficiently well to produce stable grid cell spatial firing on the order of at least 2\u20135 min, despite the high noise level. Our results suggest that the basic principles of oscillatory interference models work with more realistic models of noisy neurons. Nevertheless, a number of simplifications were still made and future work should examine increasingly realistic models.",
      "publication_date": "2010-10-13",
      "venue": "Journal of Neuroscience",
      "year": 2010,
      "citation_count": 108,
      "authors": [
        "Eric A. Zilli",
        "M. Hasselmo"
      ],
      "novel": null,
      "cited_paper": true
    }
  ],
  "query_papers": [
    {
      "paper_id": "b227f3e4c0dc96e5ac5426b85485a70f2175a205",
      "title": "Representation Learning with Contrastive Predictive Coding",
      "abstract": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
      "publication_date": "2018-07-10",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "870d2b9c71c4448498a47870f2622f0a2db44b20",
      "title": "Predictive coding networks for temporal prediction",
      "abstract": "One of the key problems the brain faces is inferring the state of the world from a sequence of dynamically changing stimuli, and it is not yet clear how the sensory system achieves this task. A well-established computational framework for describing perceptual processes in the brain is provided by the theory of predictive coding. Although the original proposals of predictive coding have discussed temporal prediction, later work developing this theory mostly focused on static stimuli, and key questions on neural implementation and computational properties of temporal predictive coding networks remain open. Here, we address these questions and present a formulation of the temporal predictive coding model that can be naturally implemented in recurrent networks, in which activity dynamics rely only on local inputs to the neurons, and learning only utilises local Hebbian plasticity. Additionally, we show that temporal predictive coding networks can approximate the performance of the Kalman filter in predicting behaviour of linear systems, and behave as a variant of a Kalman filter which does not track its own subjective posterior variance. Importantly, temporal predictive coding networks can achieve similar accuracy as the Kalman filter without performing complex mathematical operations, but just employing simple computations that can be implemented by biological networks. Moreover, when trained with natural dynamic inputs, we found that temporal predictive coding can produce Gabor-like, motion-sensitive receptive fields resembling those observed in real neurons in visual areas. In addition, we demonstrate how the model can be effectively generalized to nonlinear systems. Overall, models presented in this paper show how biologically plausible circuits can predict future stimuli and may guide research on understanding specific neural circuits in brain areas involved in temporal prediction. Author summary While significant advances have been made in the neuroscience of how the brain processes static stimuli, the time dimension has often been relatively neglected. However, time is crucial since the stimuli perceived by our senses typically dynamically vary in time, and the cortex needs to make sense of these changing inputs. This paper describes a computational model of cortical networks processing temporal stimuli. This model is able to infer and track the state of the environment based on noisy inputs, and predict future sensory stimuli. By ensuring that these predictions match the incoming stimuli, the model is able to learn the structure and statistics of its temporal inputs and produces responses of neurons resembling those in the brain. The model may help in further understanding neural circuits in sensory cortical areas.",
      "publication_date": "2024-03-09",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "a424ec3b8846f57b8ffdb566d272e28d5a525909",
      "title": "Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects.",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 1999,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "66c286df54551baba7351a1ed44019367e5aa7ea",
      "title": "Evidence of a predictive coding hierarchy in the human brain listening to speech",
      "abstract": "Current machine learning language algorithms make adjacent word-level predictions. In this work, Caucheteux et al. show that the human brain probably uses long-range and hierarchical predictions, taking into account up to eight possible words into the future. Considerable progress has recently been made in natural language processing: deep learning algorithms are increasingly able to generate, summarize, translate and classify texts. Yet, these language models still fail to match the language abilities of humans. Predictive coding theory offers a tentative explanation to this discrepancy: while language models are optimized to predict nearby words, the human brain would continuously predict a hierarchy of representations that spans multiple timescales. To test this hypothesis, we analysed the functional magnetic resonance imaging brain signals of 304 participants listening to short stories. First, we confirmed that the activations of modern language models linearly map onto the brain responses to speech. Second, we showed that enhancing these algorithms with predictions that span multiple timescales improves this brain mapping. Finally, we showed that these predictions are organized hierarchically: frontoparietal cortices predict higher-level, longer-range and more contextual representations than temporal cortices. Overall, these results strengthen the role of hierarchical predictive coding in language processing and illustrate how the synergy between neuroscience and artificial intelligence can unravel the computational bases of human cognition.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1cae417456711c4da184f5efcd1b7464a7a0661a",
      "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding",
      "abstract": "Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.",
      "publication_date": "2019-05-22",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "da741827ab6f5bf9762d30b91fd2409ac9e0e272",
      "title": "World models and predictive coding for cognitive and developmental robotics: frontiers and challenges",
      "abstract": "ABSTRACT Creating autonomous robots that can actively explore the environment, acquire knowledge and learn skills continuously is the ultimate achievement envisioned in cognitive and developmental robotics. Importantly, if the aim is to create robots that can continuously develop through interactions with their environment, their learning processes should be based on interactions with their physical and social world in the manner of human learning and cognitive development. Based on this context, in this paper, we focus on the two concepts of world models and predictive coding. Recently, world models have attracted renewed attention as a topic of considerable interest in artificial intelligence. Cognitive systems learn world models to better predict future sensory observations and optimize their policies, i.e. controllers. Alternatively, in neuroscience, predictive coding proposes that the brain continuously predicts its inputs and adapts to model its own dynamics and control behavior in its environment. Both ideas may be considered as underpinning the cognitive development of robots and humans capable of continual or lifelong learning. Although many studies have been conducted on predictive coding in cognitive robotics and neurorobotics, the relationship between world model-based approaches in AI and predictive coding in robotics has rarely been discussed. Therefore, in this paper, we clarify the definitions, relationships, and status of current research on these topics, as well as missing pieces of world models and predictive coding in conjunction with crucially related concepts such as the free-energy principle and active inference in the context of cognitive and developmental robotics. Furthermore, we outline the frontiers and challenges involved in world models and predictive coding toward the further integration of AI and robotics, as well as the creation of robots with real cognitive and developmental capabilities in the future. GRAPHICAL ABSTRACT",
      "publication_date": "2023-01-14",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "36ec1bf8a3a21321b7887caaeee9c6ee6cfc0db8",
      "title": "A predictive coding model of the N400",
      "abstract": "The N400 event-related component has been widely used to investigate the neural mechanisms underlying real-time language comprehension. However, despite decades of research, there is still no unifying theory that can explain both its temporal dynamics and functional properties. In this work, we show that predictive coding \u2013 a biologically plausible algorithm for approximating Bayesian inference \u2013 offers a promising framework for characterizing the N400. Using an implemented predictive coding computational model, we demonstrate how the N400 can be formalized as the lexico-semantic prediction error produced as the brain infers meaning from linguistic form of incoming words. We show that the magnitude of lexico-semantic prediction error mirrors the functional sensitivity of the N400 to various lexical variables, priming, contextual effects, as well as their higher-order interactions. We further show that the dynamics of the predictive coding algorithm provide a natural explanation for the temporal dynamics of the N400, and a biologically plausible link to neural activity. Together, these findings directly situate the N400 within the broader context of predictive coding research, and suggest that the brain may use the same computational mechanism for inference across linguistic and non-linguistic domains.",
      "publication_date": "2023-04-11",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "8a8f7953894fef09d32c5f35fd79cfbff025e56f",
      "title": "Sequential Memory with Temporal Predictive Coding",
      "abstract": "Forming accurate memory of sequential stimuli is a fundamental function of biological agents. However, the computational mechanism underlying sequential memory in the brain remains unclear. Inspired by neuroscience theories and recent successes in applying predictive coding (PC) to static memory tasks, in this work we propose a novel PC-based model for sequential memory, called temporal predictive coding (tPC). We show that our tPC models can memorize and retrieve sequential inputs accurately with a biologically plausible neural implementation. Importantly, our analytical study reveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN) with an implicit statistical whitening process, which leads to more stable performance in sequential memory tasks of structured inputs. Moreover, we find that tPC exhibits properties consistent with behavioral observations and theories in neuroscience, thereby strengthening its biological relevance. Our work establishes a possible computational mechanism underlying sequential memory in the brain that can also be theoretically interpreted using existing memory model frameworks.",
      "publication_date": "2023-05-19",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9f6eb784ffc460cba56077b4b8739d4eb9115f18",
      "title": "StockFormer: Learning Hybrid Trading Machines with Predictive Coding",
      "abstract": "Typical RL-for-finance solutions directly optimize trading policies over the noisy market data, such as stock prices and trading volumes, without explicitly considering the future trends and correlations of different investment assets as we humans do. In this paper, we present StockFormer, a hybrid trading machine that integrates the forward modeling capabilities of predictive coding with the advantages of RL agents in policy flexibility. The predictive coding part consists of three Transformer branches with modified structures, which respectively extract effective latent states of long-/short-term future dynamics and asset relations. The RL agent adaptively fuses these states and then executes an actor-critic algorithm in the unified state space. The entire model is jointly trained by propagating the critic's gradients back to the predictive coding module. StockFormer significantly outperforms existing approaches across three publicly available financial datasets in terms of portfolio returns and Sharpe ratios.",
      "publication_date": "2023-08-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9480a64d9520f4b8857ed291fd42db11e46594a7",
      "title": "Contrastive Difference Predictive Coding",
      "abstract": "Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \\times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20 \\times$ more sample efficient than the successor representation and $1500 \\times$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.",
      "publication_date": "2023-10-31",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "202c79bbb45ab6524141feacc81caacc4ba00401",
      "title": "Memory-augmented Dense Predictive Coding for Video Representation Learning",
      "abstract": "The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.",
      "publication_date": "2020-08-03",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ad367b44f3434b9ba6b46b41ab083210f6827a9f",
      "title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning",
      "abstract": "While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (\"PredNet\") architecture that is inspired by the concept of \"predictive coding\" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.",
      "publication_date": "2016-05-25",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "46e2af3d7e8dd50675b0062fe9e4fce5cd5a0508",
      "title": "Predictive and Adaptive Deep Coding for Wireless Image Transmission in Semantic Communication",
      "abstract": "Semantic communication is a newly emerged communication paradigm that exploits deep learning (DL) models to realize communication processes like source coding and channel coding. Recent advances have demonstrated that DL-based joint source-channel coding (DeepJSCC) can achieve exciting data compression and noise-resiliency performances for wireless image transmission tasks, especially in environments with low channel signal-to-noises (SNRs). However, existing DeepJSCC-based semantic communication frameworks still cannot achieve adaptive code rates for different channel SNRs and image contents, which reduces its flexibility and bandwidth efficiency. In this paper, we propose a predictive and adaptive deep coding (PADC) framework for realizing flexible code rate optimization with a given target transmission quality requirement. PADC is realized by a variable code length enabled DeepJSCC (DeepJSCC-V) model for realizing flexible code length adjustment, an Oracle Network (OraNet) model for predicting peak-signal-to-noise (PSNR) value for an image transmission task according to its contents, channel signal to noise ratio (SNR) and the compression ratio (CR) value, and a CR optimizer aims at finding the minimal data-level or instance-level CR with a PSNR quality constraint. By using the above three modules, PADC can transmit the image data with minimal CR, which greatly increases bandwidth efficiency. Simulation results demonstrate that the proposed DeepJSCC-V model can achieve similar PSNR performances compared with the state-of-the-art Attention-based DeepJSCC (ADJSCC) model, and the proposed OraNet model is able to predict high-quality PSNR values with an average error lower than 0.5dB. Results also demonstrate that the proposed PADC can use nearly minimal bandwidth consumption for wireless image transmission tasks with different channel SNR and image contents, at the same time guaranteeing the PSNR constraint for each image data.",
      "publication_date": "2023-08-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "23e04389f8728a5736382d3662341a1a2a25e171",
      "title": "Predictive Coding or Just Feature Discovery? An Alternative Account of Why Language Models Fit Brain Data",
      "abstract": "Abstract Many recent studies have shown that representations drawn from neural network language models are extremely effective at predicting brain responses to natural language. But why do these models work so well? One proposed explanation is that language models and brains are similar because they have the same objective: to predict upcoming words before they are perceived. This explanation is attractive because it lends support to the popular theory of predictive coding. We provide several analyses that cast doubt on this claim. First, we show that the ability to predict future words does not uniquely (or even best) explain why some representations are a better match to the brain than others. Second, we show that within a language model, representations that are best at predicting future words are strictly worse brain models than other representations. Finally, we argue in favor of an alternative explanation for the success of language models in neuroscience: These models are effective at predicting brain responses because they generally capture a wide variety of linguistic phenomena.",
      "publication_date": "2022-11-03",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6bcdf260d7927d8fe4ff030c20ee1db974d0c969",
      "title": "Predictive Coding: Towards a Future of Deep Learning beyond Backpropagation?",
      "abstract": "The backpropagation of error algorithm (BP) used to train deep neural networks has been fundamental to the successes of deep learning. However, it requires sequential backwards updates and non-local computations which make it challenging to parallelize at scale and is unlike how learning works in the brain. Neuroscience-inspired learning algorithms, however, such as \\emph{predictive coding} which utilize local learning have the potential to overcome these limitations and advance beyond deep learning technologies in the future. While predictive coding originated in theoretical neuroscience as a model of information processing in the cortex, recent work has developed the idea into a general-purpose algorithm able to train neural networks using only local computations. In this survey, we review works that have contributed to this perspective and demonstrate the close connection between predictive coding and backpropagation in terms of generalization quality, as well as works that highlight the multiple advantages of using predictive coding models over backprop-trained neural networks. Specifically, we show the substantially greater flexibility of predictive coding networks against equivalent deep neural networks, which can function as classifiers, generators, and associative memories simultaneously, and can be defined on arbitrary graph topologies. Finally, we review direct benchmarks of predictive coding networks on machine learning classification tasks, as well as its close connections to control theory and applications in robotics.",
      "publication_date": "2022-02-18",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "34a9b0c7ddc9012b8657c4bc1de5f14d45e646b4",
      "title": "Hybrid predictive coding: Inferring, fast and slow",
      "abstract": "Predictive coding is an influential model of cortical neural activity. It proposes that perceptual beliefs are furnished by sequentially minimising \u201cprediction errors\u201d\u2014the differences between predicted and observed data. Implicit in this proposal is the idea that successful perception requires multiple cycles of neural activity. This is at odds with evidence that several aspects of visual perception\u2014including complex forms of object recognition\u2014arise from an initial \u201cfeedforward sweep\u201d that occurs on fast timescales which preclude substantial recurrent activity. Here, we propose that the feedforward sweep can be understood as performing amortized inference (applying a learned function that maps directly from data to beliefs) and recurrent processing can be understood as performing iterative inference (sequentially updating neural activity in order to improve the accuracy of beliefs). We propose a hybrid predictive coding network that combines both iterative and amortized inference in a principled manner by describing both in terms of a dual optimization of a single objective function. We show that the resulting scheme can be implemented in a biologically plausible neural architecture that approximates Bayesian inference utilising local Hebbian update rules. We demonstrate that our hybrid predictive coding model combines the benefits of both amortized and iterative inference\u2014obtaining rapid and computationally cheap perceptual inference for familiar data while maintaining the context-sensitivity, precision, and sample efficiency of iterative inference schemes. Moreover, we show how our model is inherently sensitive to its uncertainty and adaptively balances iterative and amortized inference to obtain accurate beliefs using minimum computational expense. Hybrid predictive coding offers a new perspective on the functional relevance of the feedforward and recurrent activity observed during visual perception and offers novel insights into distinct aspects of visual phenomenology.",
      "publication_date": "2022-04-05",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "998628588f7850d533a172c883872057a9198d82",
      "title": "Predictive Coding: a Theoretical and Experimental Review",
      "abstract": "Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.",
      "publication_date": "2021-07-27",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e83816c0e7ed01ee48d579568ba97db72931d979",
      "title": "Learning on Arbitrary Graph Topologies via Predictive Coding",
      "abstract": "Training with backpropagation (BP) in standard deep learning consists of two main steps: a forward pass that maps a data point to its prediction, and a backward pass that propagates the error of this prediction back through the network. This process is highly effective when the goal is to minimize a specific objective function. However, it does not allow training on networks with cyclic or backward connections. This is an obstacle to reaching brain-like capabilities, as the highly complex heterarchical structure of the neural connections in the neocortex are potentially fundamental for its effectiveness. In this paper, we show how predictive coding (PC), a theory of information processing in the cortex, can be used to perform inference and learning on arbitrary graph topologies. We experimentally show how this formulation, called PC graphs, can be used to flexibly perform different tasks with the same network by simply stimulating specific neurons. This enables the model to be queried on stimuli with different structures, such as partial images, images with labels, or images without labels. We conclude by investigating how the topology of the graph influences the final performance, and comparing against simple baselines trained with BP.",
      "publication_date": "2022-01-31",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "26fc224a118e20e5d6bc98b57711001abc20ec00",
      "title": "Recurrent predictive coding models for associative memory employing covariance learning",
      "abstract": "The computational principles adopted by the hippocampus in associative memory (AM) tasks have been one of the mostly studied topics in computational and theoretical neuroscience. Classical models of the hippocampal network assume that AM is performed via a form of covariance learning, where associations between memorized items are represented by entries in the learned covariance matrix encoded in the recurrent connections in the hippocampal subfield CA3. On the other hand, it has been recently proposed that AM in the hippocampus is achieved through predictive coding. Hierarchical predictive coding models following this theory perform AM, but fail to capture the recurrent hippocampal structure that encodes the covariance in the classical models. Such a dichotomy pose potential difficulties for developing a unitary theory of how memory is formed and recalled in the hippocampus. Earlier predictive coding models that learn the covariance information of inputs explicitly seem to be a solution to this dichotomy. Here, we show that although these models can perform AM, they do it in an implausible and numerically unstable way. Instead, we propose alternatives to these earlier covariance-learning predictive coding networks, which learn the covariance information implicitly and plausibly, and can use dendritic structures to encode prediction errors. We show analytically that our proposed models are perfectly equivalent to the earlier predictive coding model learning covariance explicitly, and encounter no numerical issues when performing AM tasks in practice. We further show that our models can be combined with hierarchical predictive coding networks to model the hippocampo-neocortical interactions. Our models provide a biologically plausible approach to modelling the hippocampal network, pointing to a potential computational mechanism employed by the hippocampus during memory formation and recall, which unifies predictive coding and covariance learning based on the recurrent network structure. Author summary The hippocampus and adjacent cortical areas have long been considered essential for the formation of associative memories. Earlier theoretical works have assumed that the hippocampus stores in its recurrent connections statistical regularities embedded in the sensory inputs. On the other hand, it has been recently suggested that the hippocampus retrieves memory by generating predictions of ongoing sensory inputs. Computational models have thus been proposed to account for this predictive nature of the hippocampal network using predictive coding, a general theory of information processing in the cortex. However, these hierarchical predictive coding models of the hippocampus did not describe how it stores the statistical regularities that play a key role for associative memory in the classical hippocampal models, hindering a unified understanding of the underlying computational principles employed by the hippocampus. To address this dichotomy, here we present a family of predictive coding models that also learn the statistical information needed for associative memory. Our models can stably perform associative memory tasks in a biologically plausible manner, even with large structured data such as natural scenes. Our work provides a possible mechanism of how the recurrent hippocampal network may employ various computational principles concurrently to perform associative memory.",
      "publication_date": "2022-11-09",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0174d263d3a77bf03fce831a9a5ce2678e1959f0",
      "title": "Video Representation Learning by Dense Predictive Coding",
      "abstract": "The objective of this paper is self-supervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We make three contributions: First, we introduce the Dense Predictive Coding (DPC) framework for self-supervised representation learning on videos. This learns a dense encoding of spatio-temporal blocks by recurrently predicting future representations; Second, we propose a curriculum training scheme to predict further into the future with progressively less temporal context. This encourages the model to only encode slowly varying spatial-temporal signals, therefore leading to semantic representations; Third, we evaluate the approach by first training the DPC model on the Kinetics-400 dataset with self-supervised learning, and then finetuning the representation on a downstream task, i.e. action recognition. With single stream (RGB only), DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet.",
      "publication_date": "2019-09-10",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5374052df7a9a69499586b5acfc433f8a8e20e66",
      "title": "On Conformal Isometry of Grid Cells: Learning Distance-Preserving Position Embedding",
      "abstract": "This paper investigates the conformal isometry hypothesis as a potential explanation for the hexagonal periodic patterns in grid cell response maps. We posit that grid cell activities form a high-dimensional vector in neural space, encoding the agent's position in 2D physical space. As the agent moves, this vector rotates within a 2D manifold in the neural space, driven by a recurrent neural network. The conformal hypothesis proposes that this neural manifold is a conformal isometric embedding of 2D physical space, where local physical distance is preserved by the embedding up to a scaling factor (or unit of metric). Such distance-preserving position embedding is indispensable for path planning in navigation, especially planning local straight path segments. We conduct numerical experiments to show that this hypothesis leads to the hexagonal grid firing patterns by learning maximally distance-preserving position embedding, agnostic to the choice of the recurrent neural network. Furthermore, we present a theoretical explanation of why hexagon periodic patterns emerge by minimizing our loss function by showing that hexagon flat torus is maximally distance preserving.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "06b4bd4553b95699375832c872853e1b3d223aa7",
      "title": "Learning grid cells by predictive coding",
      "abstract": "Grid cells in the medial entorhinal cortex (MEC) of the mammalian brain exhibit a strikingly regular hexagonal firing field over space. These cells are learned after birth and are thought to support spatial navigation but also more abstract computations. Although various computational models, including those based on artificial neural networks, have been proposed to explain the formation of grid cells, the process through which the MEC circuit ${\\it learns}$ to develop grid cells remains unclear. In this study, we argue that predictive coding, a biologically plausible plasticity rule known to learn visual representations, can also train neural networks to develop hexagonal grid representations from spatial inputs. We demonstrate that grid cells emerge robustly through predictive coding in both static and dynamic environments, and we develop an understanding of this grid cell learning capability by analytically comparing predictive coding with existing models. Our work therefore offers a novel and biologically plausible perspective on the learning mechanisms underlying grid cells. Moreover, it extends the predictive coding theory to the hippocampal formation, suggesting a unified learning algorithm for diverse cortical representations.",
      "publication_date": "2024-10-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "27bc20a95969cffa29065fcc1beb63ee2386b852",
      "title": "Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells",
      "abstract": "To solve the spatial problems of mapping, localization and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learnt this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, there is no satisfactory synthesis of core principles that lead to multi-modular grid cells in deep recurrent neural networks. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: coding theory, dynamical systems, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, without access to supervised position information or engineering of particular readout representations as needed in previous approaches. We show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks and emergent representations generalize well outside their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.",
      "publication_date": "2023-11-04",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e96016d5e3c6492230aa172efcf733596dc64b6e",
      "title": "Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells",
      "abstract": "Unsupervised text encoding models have recently fueled substantial progress in Natural Language Processing (NLP). The key idea is to use neural networks to convert words in texts to vector space representations (embeddings) based on word positions in a sentence and their contexts. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as Points of Interest (POIs) into models. A general space encoding method is valuable for a multitude of tasks such asPOI search, land use classification, point-based spatial interpolation and locationaware image classification. However, no such general model exists to date beyond simply applying discretizing or feed forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for encoding space and are critical for recognizing places and for path-integration. Inspired by this research, wepropose a representation learning model called Space2vec to encode the absolutepositions and spatial relationships of places. We conduct experiments on realworld geographic data and predict types of POIs at given positions based on their1) locations and 2) nearby POIs. Results show that because of its multi-scale representations Space2vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed forward nets, and tile embedding approaches.",
      "publication_date": "2020-02-16",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2510c7bcbd6278b50dc73e02d127546ef70141bd",
      "title": "Learning an efficient place cell map from grid cells using non-negative sparse coding",
      "abstract": "Experimental studies of grid cells in the Medial Entorhinal Cortex (MEC) have shown that they are selective to an array of spatial locations in the environment that form a hexagonal grid. However, in a small environment, place cells in the hippocampus are only selective to a single-location of the environment while granule cells in the dentate gyrus of the hippocampus have multiple discrete firing locations, but lack spatial periodicity. Given the anatomical connection from MEC to the hippocampus, previous feedforward models of grid-to-place have been proposed. Here, we propose a unified learning model that can describe the spatial tuning properties of both hippocampal place cells and dentate gyrus granule cells based on non-negative sparse coding. Sparse coding plays an important role in many cortical areas and is proposed here to have a key role in the navigational system of the brain in the hippocampus. Our results show that the hexagonal patterns of grid cells with various orientations, grid spacings and phases are necessary for model cells to learn a single spatial field that efficiently tile the entire spatial environment. However, if there is a lack of diversity in any grid parameters or a lack of cells in the network, this will lead to the emergence of place cells that have multiple firing locations. More surprisingly, the model shows that place cells can also emerge even when non-negative sparse coding is used with weakly-tuned MEC cells, instead of MEC grid cells, as the input to place cells. This work suggests that sparse coding may be one of the underlying organizing principles for the navigational system of the brain.",
      "publication_date": "2020-08-13",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2abc9d603a87b300a251a3796a12b8a2d21746df",
      "title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion",
      "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multiplication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.",
      "publication_date": "2018-09-27",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d09c79f6b7420c8c8a28ecab0836ec9bacb8f8cf",
      "title": "A non-spatial account of place and grid cells based on clustering models of concept learning",
      "abstract": "One view is that conceptual knowledge is organized using the circuitry in the medial temporal lobe (MTL) that supports spatial processing and navigation. In contrast, we find that a domain-general learning algorithm explains key findings in both spatial and conceptual domains. When the clustering model is applied to spatial navigation tasks, so-called place and grid cell-like representations emerge because of the relatively uniform distribution of possible inputs in these tasks. The same mechanism applied to conceptual tasks, where the overall space can be higher-dimensional and sampling sparser, leading to representations more aligned with human conceptual knowledge. Although the types of memory supported by the MTL are superficially dissimilar, the information processing steps appear shared. Our account suggests that the MTL uses a general-purpose algorithm to learn and organize context-relevant information in a useful format, rather than relying on navigation-specific neural circuitry. Spatial maps in the medial temporal lobe (MTL) have been proposed to map abstract conceptual knowledge. Rather than grounding abstract knowledge in a spatial map, the authors propose a general-purpose clustering algorithm that explains how both spatial (including place and grid cells) and higher-dimensional conceptual representations arise during learning.",
      "publication_date": "2018-09-19",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "138a75cab26100df70bdb66dd1a28e68612db8ff",
      "title": "Learning place cells, grid cells and invariances with excitatory and inhibitory plasticity",
      "abstract": "Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction, and the underlying circuit mechanisms are not yet resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, place cells are typically invariant to head direction. We propose that all observed spatial tuning patterns \u2013 in both their selectivity and their invariance \u2013 arise from the same mechanism: Excitatory and inhibitory synaptic plasticity driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. Our proposed model is robust to changes in parameters, develops patterns on behavioral timescales and makes distinctive experimental predictions.",
      "publication_date": "2018-02-21",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4af94a163a695f14597ac101a9562a68772010ae",
      "title": "Probabilistic Learning by Rodent Grid Cells",
      "abstract": "Mounting evidence shows mammalian brains are probabilistic computers, but the specific cells involved remain elusive. Parallel research suggests that grid cells of the mammalian hippocampal formation are fundamental to spatial cognition but their diverse response properties still defy explanation. No plausible model exists which explains stable grids in darkness for twenty minutes or longer, despite being one of the first results ever published on grid cells. Similarly, no current explanation can tie together grid fragmentation and grid rescaling, which show very different forms of flexibility in grid responses when the environment is varied. Other properties such as attractor dynamics and grid anisotropy seem to be at odds with one another unless additional properties are assumed such as a varying velocity gain. Modelling efforts have largely ignored the breadth of response patterns, while also failing to account for the disastrous effects of sensory noise during spatial learning and recall, especially in darkness. Here, published electrophysiological evidence from a range of experiments are reinterpreted using a novel probabilistic learning model, which shows that grid cell responses are accurately predicted by a probabilistic learning process. Diverse response properties of probabilistic grid cells are statistically indistinguishable from rat grid cells across key manipulations. A simple coherent set of probabilistic computations explains stable grid fields in darkness, partial grid rescaling in resized arenas, low-dimensional attractor grid cell dynamics, and grid fragmentation in hairpin mazes. The same computations also reconcile oscillatory dynamics at the single cell level with attractor dynamics at the cell ensemble level. Additionally, a clear functional role for boundary cells is proposed for spatial learning. These findings provide a parsimonious and unified explanation of grid cell function, and implicate grid cells as an accessible neuronal population readout of a set of probabilistic spatial computations.",
      "publication_date": "2016-10-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b8ab26cb57b8dcc19019ccf36db682c2a4eedce8",
      "title": "Towards Emergence of Grid Cells by Deep Reinforcement Learning",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "bc7417738a7f7186ce9b06933072c4ddd39ce4b1",
      "title": "Actionable Neural Representations: Grid Cells from Minimal Constraints",
      "abstract": "To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won't have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an `actionable representation'. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode.",
      "publication_date": "2022-09-30",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b629916af0db6f56f3b318f5043c81b374271fa5",
      "title": "Coordinated Learning of Entorhinal Grid Cells and Hippocampal Place Cells: Space, Time, Attention and Oscillations",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "20d6f4241af85a9c4fe5f0fcb583c818441282c4",
      "title": "Learning place cells, grid cells and invariances: A unifying model",
      "abstract": "Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction. The underlying circuit mechanisms are not fully resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, a place cell is highly selective to location, but typically invariant to head direction. Here, we propose that all observed spatial tuning patterns \u2013 in both their selectivity and their invariance \u2013 are a consequence of the same mechanism: Excitatory and inhibitory synaptic plasticity that is driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. The model is robust to changes in parameters, develops patterns on behavioral time scales and makes distinctive experimental predictions. Our results suggest that the interaction of excitatory and inhibitory plasticity is a general principle for the formation of neural representations.",
      "publication_date": "2017-02-17",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "917922f63a9920b940c05fa802deb3d7d83dd6b0",
      "title": "Using Convolutional Neural Networks to Build a Lightweight Flood Height Prediction Model with Grad-Cam for the Selection of Key Grid Cells in Radar Echo Maps",
      "abstract": "Recent climate change has brought extremely heavy rains and widescale flooding to many areas around the globe. However, previous flood prediction methods usually require a lot of computation to obtain the prediction results and impose a heavy burden on the unit cost of the prediction. This paper proposes the use of a deep learning model (DLM) to overcome these problems. We alleviated the high computational overhead of this approach by developing a novel framework for the construction of lightweight DLMs. The proposed scheme involves training a convolutional neural network (CNN) by using a radar echo map in conjunction with historical flood records at target sites and using Grad-Cam to extract key grid cells from these maps (representing regions with the greatest impact on flooding) for use as inputs in another DLM. Finally, we used real radar echo maps of five locations and the flood heights record to verify the validity of the method proposed in this paper. The experimental results show that our proposed lightweight model can achieve similar or even better prediction accuracy at all locations with only about 5~15% of the operation time and about 30~35% of the memory space of the CNN.",
      "publication_date": "2022-01-07",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "27d6a46db40f93d4ca0659e19125a7ac5396d0da",
      "title": "Identifying key grid cells for crowd flow predictions based on CNN-based models with the Grad-CAM kit",
      "abstract": null,
      "publication_date": "2022-10-08",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c2228c6e59fdb4e63f2e873077f1690e16fa39f8",
      "title": "Dynamic Occupancy Grid Map with Semantic Information Using Deep Learning-Based BEVFusion Method with Camera and LiDAR Fusion",
      "abstract": "In the field of robotics and autonomous driving, dynamic occupancy grid maps (DOGMs) are typically used to represent the position and velocity information of objects. Although three-dimensional light detection and ranging (LiDAR) sensor-based DOGMs have been actively researched, they have limitations, as they cannot classify types of objects. Therefore, in this study, a deep learning-based camera\u2013LiDAR sensor fusion technique is employed as input to DOGMs. Consequently, not only the position and velocity information of objects but also their class information can be updated, expanding the application areas of DOGMs. Moreover, unclassified LiDAR point measurements contribute to the formation of a map of the surrounding environment, improving the reliability of perception by registering objects that were not classified by deep learning. To achieve this, we developed update rules on the basis of the Dempster\u2013Shafer evidence theory, incorporating class information and the uncertainty of objects occupying grid cells. Furthermore, we analyzed the accuracy of the velocity estimation using two update models. One assigns the occupancy probability only to the edges of the oriented bounding box, whereas the other assigns the occupancy probability to the entire area of the box. The performance of the developed perception technique is evaluated using the public nuScenes dataset. The developed DOGM with object class information will help autonomous vehicles to navigate in complex urban driving environments by providing them with rich information, such as the class and velocity of nearby obstacles.",
      "publication_date": "2024-04-29",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0a292908d132abc2bf5fc5413d8c121600b5288e",
      "title": "A Computational Model of Grid Cells based on Dendritic Self-organized Learning",
      "abstract": "In this paper we present a new computational model for grid cells. These cells are neurons in the entorhinal cortex of the hippocampal region that encode allocentric spatial information. They possess a peculiar, triangular firing pattern that spans the entire environment with a virtual lattice. We show that such a firing pattern can emerge from a dendritic, self-organized learning process. A key aspect of the proposed model is the hypothesis that the dendritic tree of a grid cell can behave like a sparse self organizing map that tries to cover its input space as best as possible. We argue, that the encoding scheme used by grid cells is possibly not limited to the description of spatial information and may represent a general principle on how complex information is encoded in higher level brain areas like the hippocampal region.",
      "publication_date": null,
      "venue": "",
      "year": 2013,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "7a795b581de305191235367b71d65ab8038d0f73",
      "title": "A unified theory for the origin of grid cells through the lens of pattern formation",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "789703d2841acb82dde6d6bc8e37eda5772760c3",
      "title": "Energy Efficiency Optimization in HetNets for Smart Grid Wide Area Network Using Deep Reinforcement Learning",
      "abstract": "The rapid expansion of smart grid infrastructure necessitates advanced communication networks capable of supporting extensive data exchange with high reliability and energy efficiency. This paper investigates energy efficiency in cellular networks using Heterogeneous Networks (HetNets) for Smart Grid Wide Area Networks (SG-WANs). HetNets, which include both macro and small cells, could enhance coverage and capacity of cellular network but are challenging to manage due to their complexity and dynamic traffic demands. This work adopts a deep reinforcement learning (DRL)-based framework to optimize network resources, adapting to environment changes and minimizing energy consumption while maintaining Quality of Service (QoS). The DRL agent learns to make real-time decisions on cell activation, user association, and power control, improving the network's overall energy efficiency.",
      "publication_date": "2024-09-09",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5be0bfaa322272f3c70a203bcc0aef87a6a6dc13",
      "title": "A Brain-Inspired Adaptive Space Representation Model Based on Grid Cells and Place Cells",
      "abstract": "Grid cells and place cells are important neurons in the animal brain. The information transmission between them provides the basis for the spatial representation and navigation of animals and also provides reference for the research on the autonomous navigation mechanism of intelligent agents. Grid cells are important information source of place cells. The supervised learning and unsupervised learning models can be used to simulate the generation of place cells from grid cell inputs. However, the existing models preset the firing characteristics of grid cell. In this paper, we propose a united generation model of grid cells and place cells. First, the visual place cells with nonuniform distribution generate the visual grid cells with regional firing field through feedforward network. Second, the visual grid cells and the self-motion information generate the united grid cells whose firing fields extend to the whole space through genetic algorithm. Finally, the visual place cells and the united grid cells generate the united place cells with uniform distribution through supervised fuzzy adaptive resonance theory (ART) network. Simulation results show that this model has stronger environmental adaptability and can provide reference for the research on spatial representation model and brain-inspired navigation mechanism of intelligent agents under the condition of nonuniform environmental information.",
      "publication_date": "2020-08-11",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "cbeb0b647964650757d5e25aa711916e12a896dc",
      "title": "Grid cell firing patterns maintain their hexagonal firing patterns on a circular track",
      "abstract": "In an open two-dimensional environment, grid cells in the medial entorhinal cortex are known to be active in multiple locations, displaying a striking periodic hexagonal firing pattern covering the entire space. Both modeling and experimental data suggest that such periodic spatial representations may emerge from a continuous attractor network. According to this theory, grid cell activity in any stable 1D environment is a slice through an underlying 2D hexagonal pattern, which is supported by some experimental studies but challenged by others. Grid cells are believed to play a fundamental role in path integration, and so understanding their behavior in various environments is crucial for understanding the flow of information through the entorhinal-hippocampal system. To this end, we analyzed the activity of grid cells when rats traversed a circular track. A previous study involving this data set analyzed individual grid cell activity patterns separately, but we found that individual grid cells do not provide sufficient data for determining the under-lying spatial activity pattern. To circumvent this, we compute the population autocorrelation, which pools together population responses from all grid cells within the same module. This novel approach recovers the underlying six-peak hexagonal pattern that was not observable in the individual autocorrelations. We also use the population autocorrelation to infer the spacing and orientation of the population lattice, revealing how the lattice differs across environments. Furthermore, the population autocorrelation of the linearized track reveals that at the level of the population, grid cells have an allocentric code for space. These results are strong support for the attractor network theory for grid cells, and our novel approach can be used to analyze grid cell activity in any undersampled environment.",
      "publication_date": "2023-09-15",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "48fa2392ceb51aa5fc57766d513efe1fa9bf3de0",
      "title": "Spatial periodicity in grid cell firing is explained by a neural sequence code of 2-D trajectories",
      "abstract": "Spatial periodicity in grid cell firing has been interpreted as a neural metric for space providing animals with a coordinate system in navigating physical and mental spaces. However, the specific computational problem being solved by grid cells has remained elusive. Here, we provide mathematical proof that spatial periodicity in grid cell firing is the only possible solution to a neural sequence code of 2-D trajectories and that the hexagonal firing pattern of grid cells is the most parsimonious solution to such a sequence code. We thereby provide a likely teleological cause for the existence of grid cells and reveal the underlying nature of the global geometric organization in grid maps as a direct consequence of a simple local sequence code. A sequence code by grid cells provides intuitive explanations for many previously puzzling experimental observations and may transform our thinking about grid cells.",
      "publication_date": "2023-05-30",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "affe48f3661f2e4d1869c992a5f9aed3e849c050",
      "title": "Spatial periodicity in grid cell firing is explained by a neural sequence code of 2D trajectories",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5b66541e9cc6708be8f70405b6f51b1c41fa34dc",
      "title": "Features of formation of the celsian phase during firing of heat-resistant ceramics in the system BaO\u2013Al2O3\u2013SiO2",
      "abstract": "In the synthesis of heat-resistant ceramics in the system BaO\u2013Al2O3\u2013SiO2 (BAS), it is important to find ways of intensifying the process of transition of hexagonal celsian to monoclinic celsian without an increase in sintering temperature. Monoclinic form of celsian is characterized by higher thermal, electro-insulating, and mechanical properties. This paper deals with the features of formation of the phase composition of celsian ceramics when using BAS glass of eutectic composition and glass in the system Li2O\u2013Al2O3\u2013B2O3\u2013SiO2 (LABS) of spodumene composition as modifying components. It is shown that monoclinic celsian is the final crystalline phase formed in ceramics synthesized on the basis of barium carbonate and kaolin. Monoclinic celsian is formed stepwise; and the hexagonal celsian appears first. The complete transition hexagonal celsian\uf0aemonoclinic celsian occurs only in the process of high temperature firing at 12500C. Notably, the degree of ceramic sintering remains low (water absorption is 11.0%). Introduction of BAS glass contributes to the complete transition of hexagonal celsian to monoclinic celsian at a reduced temperature of 11000C. Maximum effect in the formation of monoclinic celsian is achieved by the introduction of LABS glass. As a result, the temperature of formation of this modification maximally decreases to 8000C. In this case, complete sintering of celsian ceramics is achieved at the temperature of 12500\u0421.",
      "publication_date": "2022-06-01",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "de629192ca5b0b7195355149dad23bd4124bf3df",
      "title": "Irregular distribution of grid cell firing fields in rats exploring a 3D volumetric space",
      "abstract": "We investigated how entorhinal grid cells encode volumetric space. On a horizontal surface, grid cells usually produce multiple, spatially focal, approximately circular firing fields that are evenly sized and spaced to form a regular, close-packed, hexagonal array. This spatial regularity has been suggested to underlie navigational computations. In three dimensions, theoretically the equivalent firing pattern would be a regular, hexagonal close packing of evenly sized spherical fields. In the present study, we report that, in rats foraging in a cubic lattice, grid cells maintained normal temporal firing characteristics and produced spatially stable firing fields. However, although most grid fields were ellipsoid, they were sparser, larger, more variably sized and irregularly arranged, even when only fields abutting the lower surface (equivalent to the floor) were considered. Thus, grid self-organization is shaped by the environment\u2019s structure and/or movement affordances, and grids may not need to be regular to support spatial computations. Grieves et al. show that when rats explore a 3D space, grid cells in the entorhinal cortex exchange their usual spatially regular firing patterns for more irregular ones, suggesting that 3D space is mapped differently than previously thought.",
      "publication_date": "2021-08-11",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "03915b784afbe730a33d5803600bde7c497c7cc1",
      "title": "Hexagonal grid \ufb01elds optimally",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ee73107de55b07d563246ced858ba561de1d80f9",
      "title": "Low-Temperature Firing of Substituted M-Type Hexagonal Ferrites for Multilayer Inductors",
      "abstract": null,
      "publication_date": "2012-03-22",
      "venue": "",
      "year": 2012,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "79be3dcc0cd53613e5386425e387b2664c9c9ab0",
      "title": "Untethered firing fields and intermittent silences: Why grid\u2010cell discharge is so variable",
      "abstract": "Grid cells in medial entorhinal cortex are notoriously variable in their responses, despite the striking hexagonal arrangement of their spatial firing fields. Indeed, when the animal moves through a firing field, grid cells often fire much more vigorously than predicted or do not fire at all. The source of this trial\u2010to\u2010trial variability is not completely understood. By analyzing grid\u2010cell spike trains from mice running in open arenas and on linear tracks, we characterize the phenomenon of \u201cmissed\u201d firing fields using the statistical theory of zero inflation. We find that one major cause of grid\u2010cell variability lies in the spatial representation itself: firing fields are not as strongly anchored to spatial location as the averaged grid suggests. In addition, grid fields from different cells drift together from trial to trial, regardless of whether the environment is real or virtual, or whether the animal moves in light or darkness. Spatial realignment across trials sharpens the grid representation, yielding firing fields that are more pronounced and significantly narrower. These findings indicate that ensembles of grid cells encode relative position more reliably than absolute position.",
      "publication_date": "2020-02-11",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3bc48861d66b2ee2968ed26c0d96c9b2e1ab06e8",
      "title": "Path integration maintains spatial periodicity of grid cell firing in a 1D circular track",
      "abstract": "Entorhinal grid cells are thought to provide a 2D spatial metric of the environment. In this study we demonstrate that in a familiar 1D circular track (i.e., a continuous space) grid cells display a novel 1D equidistant firing pattern based on integrated distance rather than travelled distance or time. In addition, field spacing is increased compared to a 2D open field, probably due to a reduced access to the visual cue in the track. This metrical modification is accompanied by a change in LFP theta oscillations, but no change in intrinsic grid cell rhythmicity, or firing activity of entorhinal speed and head-direction cells. These results suggest that in a 1D circular space grid cell spatial selectivity is shaped by path integration processes, while grid scale relies on external information. In an open field, the preferential firing of grid cells on a hexagonal lattice is formed by integrating external as well as self-motion cues. Here, the authors show that on a 1D circular track, path integration cues shape the spatial selectivity of grid cells while external cues determine the scale of the grid.",
      "publication_date": "2019-02-19",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ddf9626f10f4a5fd38e2332f1720c3f505783632",
      "title": "Influence of Firing Atmosphere on the Cubic-Hexagonal Transition and the Chemical State of Titanium in BaTiO3",
      "abstract": "The effect of reducing gas mixture, H2-N2 or CO-N2, on the cubic-hexagonal transition in BaTiO3 has been investigated by X-ray diffraction, ESR, ESCA, PAS and chemical analysis. The firing temperatures of BaTiO3 were 1380\u00b0C and 1500\u00b0C, and the transition temperature in air is already known to be 1460\u00b0C. The fractional conversion to the hexagonal phase increased with increasing concentrations of reducing gases, larger for samples fired in H2-N2 than those fired in CO-N2 for the same concentrations of the reducing gases. The hexagonal phase in all samples increased proportionally to the amount of Ti3+ produced by reduction of Ti4+ ions, regardless of the reducing gas species and firing temperatures. It was found that the minimum amount of Ti3+ ions to stabilize the hexagonal BaTiO3 at room temperature was 0.3% of total Ti ions. ESCA spectra due to Ti3+ were observed in the sample fired in 100% H2 at 1380\u00b0C.",
      "publication_date": null,
      "venue": "",
      "year": 1987,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "efa178733034990962a2379ce972a229f61da1cd",
      "title": "Co-firing behavior of ZnTiO3\u2013TiO2 dielectrics/hexagonal ferrite composites for multi-layer LC filters",
      "abstract": null,
      "publication_date": "2003-05-25",
      "venue": "",
      "year": 2003,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4a5f9b6232a0ac08c09ef473c419e691295e8a23",
      "title": "Effect of Firing Atmosphere on the Cubic-Hexagonal Transition in Ba0.99Sr0.01TiO3",
      "abstract": "Ba0.99Sr0.01Tio3 ceramics were fired for 1 hour at a temperature from 1410\u00b0 to 1460\u00b0C in H2-N2 atmospheres. A pure hexagonal phase, which was not formed by calcination in air, was obtained by firing for 1 hour at 1430\u00b0C in H2 atmosphere. A part of the oxygen in Ba0.99Sr0.01TiO3 was removed under the reducing condition, which led to the formation of the hexagonal phase. The amount of the hexagonal phase of Ba0.99Sr0.01TiO3-x increased proportionally with an increase in oxygen deficiency (x). The value of x required for stabilizing the hexagonal phase at room temperature was found to be larger than 0.008. The transition temperature from cubic to hexagonal phase of Ba0.99Sr0.01TiO3-x was higher than that of BaTiO3-x, and oxygen deficiency required for stabilizing the hexagonal phase was larger than that for BaTiO3-x. These results indicated that Ba0.99Sr0.01TiO3-x does not easily transformed into the hexagonal phase.",
      "publication_date": "1990-08-01",
      "venue": "",
      "year": 1990,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "93ea481402dcf2ea344baa9d1fe51a58b28b9b17",
      "title": "Co-Firing Behavior of Co2Z Hexagonal Ferrite/Ag Internal Electrode for MLCIs",
      "abstract": null,
      "publication_date": "2004-12-01",
      "venue": "",
      "year": 2004,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "099c02c7a4c3c403eb5525673fcdd3ce5e4bd5c8",
      "title": "AgCrO2 formation mechanism during silver inner electrode and Fe\u2013Si\u2013Cr alloy powder co-firing in metal multilayer chip power inductors",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "aa8ab6a377cdcc5f0614b8f8beb7f267782319ac",
      "title": "Control of hexagonal plate-like microstructure of in-situ calcium hexaluminate in monolithic refractories",
      "abstract": "ABSTRACT The self-toughening property of calcium hexaluminate (CaO\u20226Al2O3, CA6) in engineering ceramics causes a remarkable interest in the in-situ CA6 in monolithic refractory. The determination of significant factors which controls the formation and microstructure of CA6 in monolithic refractory is mainly discussed in this study. Samples were prepared by using calcium aluminate cement and sintered alumina and the chemical composition was fixed at the molar ratio of CaO: Al2O3 = 1:6. In order to evaluate the suitable sintering condition and SiO2 content for CA6 phase formation, the variation of firing temperature and holding time was first modified at 1400\u20131500\u00b0C for 1\u20135 h. The second factor was done by adding 2\u201310 mass% of SiO2 into the primary mixture. In comparison, the results were confirmed that high firing temperature at 1500\u00b0C provided the highest quantity of CA6 phase and plate-like microstructure. In addition, longer proceeding time contributed to the grain growth of CA6, especially, within 5 h of holding time. SiO2 importantly helped to enhance hexagonal plate-like microstructure due to the ability of ion mobility in low-melting phase of gehlenite but the suitable content of SiO2 should not exceed 2 mass% for the better control ability of CA6 formation and microstructure.",
      "publication_date": "2018-07-03",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "838c48b300fd1be40014c97edd79233b1fb52f69",
      "title": "Hippocampal Spike-Timing Correlations Lead to Hexagonal Grid Fields.",
      "abstract": "Space is represented in the mammalian brain by the activity of hippocampal place cells, as well as in their spike-timing correlations. Here, we propose a theory for how this temporal code is transformed to spatial firing rate patterns via spike-timing-dependent synaptic plasticity. The resulting dynamics of synaptic weights resembles well-known pattern formation models in which a lateral inhibition mechanism gives rise to a Turing instability. We identify parameter regimes in which hexagonal firing patterns develop as they have been found in medial entorhinal cortex.",
      "publication_date": "2017-06-21",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0c48611df1f03ca7cec7f98a1ffaadf95dec1260",
      "title": "Grids from bands, or bands from grids? An examination of the effects of single unit contamination on grid cell firing fields.",
      "abstract": "Neural recording technology is improving rapidly, allowing for the detection of spikes from hundreds of cells simultaneously. The limiting step in multielectrode electrophysiology continues to be single cell isolation. However, this step is crucial to the interpretation of data from putative single neurons. We present here, in simulation, an illustration of possibly erroneous conclusions that may be reached when poorly isolated single cell data are analyzed. Grid cells are neurons recorded in rodents, and bats, that spike in equally spaced locations in a hexagonal pattern. One theory states that grid firing patterns arise from a combination of band firing patterns. However, we show here that summing the grid firing patterns of two poorly resolved neurons can result in spurious band-like patterns. Thus, evidence of neurons spiking in band patterns must undergo extreme scrutiny before it is accepted. Toward this aim, we discuss single cell isolation methods and metrics.",
      "publication_date": "2016-02-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d40cb3f74f7e4a0858bc63760ec983f15de8125c",
      "title": "Grid-Cell Activity on Linear Tracks Indicates Purely Translational Remapping of 2D Firing Patterns at Movement Turning Points",
      "abstract": "Grid cells in rodent medial entorhinal cortex are thought to play a critical role for spatial navigation. When the animal is freely moving in an open arena the firing fields of each grid cell tend to form a hexagonal lattice spanning the environment. For movements along a linear track the cells seem to respond differently. They show multiple firing fields that are not periodically arranged and whose shape and position change when the running direction is reversed. In addition, peak firing rates vary widely from field to field. Measured along one running direction only, firing fields are, however, compatible with a slice through a two-dimensional (2D) hexagonal pattern. It is an open question, whether this is also true if leftward and rightward runs are jointly considered. By analyzing data from 15 male Long\u2013Evans rats, we show that a single hexagonal firing pattern explains the linear-track data if translational shifts of the pattern are allowed at the movement turning points. A rotation or scaling of the grid is not required. The agreement is further improved if the peak firing rates of the underlying 2D grid fields can vary from field to field, as suggested by recent studies. These findings have direct consequences for experiments using linear tracks in virtual reality. SIGNIFICANCE STATEMENT Various types of neurons support spatial navigation. Their response properties are often studied in reduced settings and might change when the animal can freely explore its environment. Grid cells in rodents, for example, exhibit seemingly irregular firing fields when animal movement is restricted to a linear track but highly regular patterns in two-dimensional (2D) arenas. We show that linear-track responses of a cell for both leftward and rightward running directions can be explained as cuts through a single hexagonal pattern if translational remapping is allowed at movement turning points; neither rotations nor scale transformations are needed. These results provide a basis to quantify grid-cell activity in 1D virtual reality and could help to detect and categorize grid cells without experiments in 2D environments.",
      "publication_date": "2018-07-05",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "115806ae388f43e1e25062ce88d2415c64c580dc",
      "title": "Grid cell firing fields in a volumetric space",
      "abstract": "We investigated how entorhinal grid cells represent volumetric (three-dimensional) space. On a flat surface, grid cell firing fields are circular and arranged in a close-packed hexagonal array. In three dimensions, theoretical and computational work suggests that the most efficient configuration would be a regular close packing of spherical fields. We report that in rats exploring a cubic lattice, grid cells were spatially stable and maintained normal directional modulation, theta modulation and spike dynamics. However, while the majority of grid fields were spherical, they were irregularly arranged, even when only fields abutting the lower surface (equivalent to the floor) were considered. Thus, grid organization is shaped by the environment\u2019s movement affordances, and may not need to be regular to support spatial computations. One Sentence Summary In rats exploring a volumetric space, grid cells are spatially modulated but their firing fields are irregularly arranged.",
      "publication_date": "2020-12-07",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "62f16717f8359d414513cc77ccdd363b508dae35",
      "title": "Brain-Inspired Spatial Representation Based on Grid Cells with Finite-Spacing Firing Field",
      "abstract": "Grid cell is a kind of important neuron cell related to spatial cognition and navigation in animal brain. It has a hexagonal firing field extending to the whole space. Grid cells exist in the form of modules, whose number is finite, and the grid cells between adjacent modules have discrete firing- field spacing with constant ratio. The existing models can simulate the constant ratio discrete of firing-field spacing between modules, but the finite characteristic of firing-field spacing cannot be simulated. We propose a recurrent attractor network model of grid cell, which can generate grid cells with finite firing-field spacing by analyzing the connections between grid cells in a single network. Then, the grid cells in different modules could possess discrete firing-field spacing with constant ratio through the competition among the recurrent networks. The recurrent attractor network model can provide reference for the construction of brain-inspired navigation system of unmanned platform.",
      "publication_date": "2019-11-01",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "7662551500dbdea149bbb44181db7592ad93c0f7",
      "title": "Cage-rotor induction motor inter-turn short circuit fault detection with and without saturation effect by MEC model.",
      "abstract": null,
      "publication_date": "2016-09-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "78c169ad1636ec9eeea8c02e5c26452272727ede",
      "title": "MEC: An Open-source Fine-grained Mapping Equivalence Checking Tool for FPGA",
      "abstract": "Technology mapping is an essential step in EDA flow. However, the function of the circuit may be changed after technology mapping, and equivalence checking (EC) based verification is highly necessary. The traditional EC method has significant time and resource constraints, making it only feasible to carry out at a coarse-grained level. To make it efficient for technology mapping, we propose a fine-grained method called MEC, which leverages a combination of two approaches to significantly reduce the time cost of verification. The local block verification approach performs fast verification and the global graph cover approach guarantees correctness. The proposed method is rigorously tested and compared to three EC tools, and the results show that MEC technique offers a substantial improvement in speed. MEC not only offers a faster and more efficient way of performing EC on technology mapping but also opens up new opportunities for more fine-grained verification in the future.",
      "publication_date": "2023-05-08",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "569043747e04038c86b73c28f3d269d01e31d7ff",
      "title": "Magnetic Equivalent Circuit and Lagrange Interpolation Function Modeling of Induction Machines Under Broken Bar Faults",
      "abstract": "This article introduces a mesh-based magnetic equivalent circuit (MEC) modeling technique for induction machines (IMs) in healthy and broken rotor bars conditions. The MEC model is presented as a highly accurate and computationally efficient alternative to finite element (FE) models. By incorporating modifications to the air gap coupling method, including a new Lagrange interpolation function, and utilizing a harmonic MEC model, the accuracy of the solution is improved while reducing electrical and mechanical transients. Compared to experiments and 2-D FE models, this model achieves precise results for electromagnetic torque, rotational speed, and forces across various conditions. The Lagrange interpolation function forms the basis for the air gap coupling between stator and rotor flux densities. The results demonstrate the MEC model\u2019s exceptional accuracy in predicting speed oscillations, calculating forces, and analyzing current harmonics in faulty IMs. Furthermore, the MEC model performs over 30 times faster than the 2-D FE models.",
      "publication_date": "2024-03-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2ddc7239d99b4b3c5c5407bb62c47b6d48ad8161",
      "title": "Armature Reaction Analysis and Suppression of Voice Coil Actuator Based on an Improved Magnetic Equivalent Circuit Model",
      "abstract": "The high thrust density voice coil actuator (VCA) is a key component for the deployers to determine the CubeSats\u2019 separation velocity in orbit directly. To improve the VCA's thrust density and power consumption, it is essential to analyze the armature reaction under heavy current work conditions. This article proposes a dynamic splitting method of the magnetic equivalent circuit (MEC) model for analyzing the armature reaction of the VCA. This MEC model can effectively reveal the interaction mechanism between the dynamic winding field and the permanent magnet (PM) field, which provides an approach to readily and quickly analyze the armature reaction. Based on the understanding of the mechanism, we found an effective way to suppress the armature reaction and propose a new VCA with double PM flux circuits. The electromagnetic performances of the new configuration are evaluated by comparing it with the conventional one. Both the theoretical and experimental results show that the new structure achieves lower magnetic saturation, lower armature reaction, and higher thrust density. A research prototype was constructed and experimentally tested to verify the theoretical results of the armature reaction, magnetic field, thrust, separation velocity, and other performances. The new VCA realizes a wide and accurate modulation of separation velocity for the CubeSats.",
      "publication_date": "2024-07-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0b5487d66232ab352af73092a29c5508bd18e4e3",
      "title": "Analysis of Proximity Loss of Electrical Machines Using Mesh-Based Magnetic Equivalent Circuit",
      "abstract": "In high-power density machines, proximity loss presents an unavoidable obstacle due to its significant impact on thermal dissipation and insulation aging. To address the need for rapid and accurate proximity loss prediction, this study presents a novel methodology that employs a mesh-based magnetic equivalent circuit (MEC) for calculating proximity loss in electrical machines. Using an existing machine as an example, the proposed approach is applied to various scenarios, yielding results that demonstrate close agreement with both finite element analysis (FEA) and experimental results, validating its effectiveness. Notably, the technique exhibits high flexibility and can be extended to accommodate slots of various shapes. This innovative approach, which involves flux leakage calculation, represents a previously unexplored avenue and could potentially serve as a fundamental basis for expeditious ac loss calculations.",
      "publication_date": "2024-12-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2c76fb8c1d8f87a1178a2b0eca79f868b828b384",
      "title": "Magnetic Equivalent Circuit Model for a Two Degree-of-Freedom Rotary-Linear Machine With Transverse Flux Structure",
      "abstract": "This article proposes a magnetic equivalent circuit (MEC) model for a two degree-of-freedom (2DOF) rotary-linear (RL) machine with transverse flux structure. The permeance grid is formed according to the flux pattern generated in the finite-element method (FEM). By tuning the leakage flux permeance, the electromagnetic performance of the RL machine calculated by the MEC model shows good alignment with that by FEM. To fully demonstrate the benefits of the MEC model, the calculation accuracy and time consumption comparisons among the MEC model, magnetomotive force (MMF) model, and FEM are conducted. In addition, the MEC model is applied to the parametric study, which proves that the MEC model is capable of obtaining the accurate results under various geometry designs. Based on that, a hierarchical optimization flow is put forward, which is a practical method for global optimization on 3-D structure. Finally, an experimental prototype is manufactured to verify the proposed concepts.",
      "publication_date": "2024-12-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "abd0285f56bd668a31d99825ee48640c8026605a",
      "title": "Exploring torque characteristics of an integrated ultra conducting magnetic coupling incorporating axial magnetic field through magnetic equivalent circuit modelling",
      "abstract": "Ultra-conductors are a recent class of materials that display superconducting attributes at room temperature and even at higher temperatures. In this paper, we introduce a novel hybrid ultra-conducting coupling (NHUC) where the secondary component includes copper and an ultra-conductor (considered as a room temperature superconductor in much research). We suggest a model based on magnetic equivalent circuit (MEC) to evaluate the distribution of magnetic fields and characteristics related to torque. The 3D model accounts for boundary impacts during mating, and formulation for magnetic flux and torque are derived using Kirchhoff\u2019s and Ampere\u2019s loop laws, respectively. A 3D finite element (FEM) model is constructed, and simulations are performed using iodine-doped double-walled carbon nanotube (IDWCT) as the ultra-conductor for a wider operating range. At a high slip speed of 1200\u00a0rpm, a maximum torque of 309 Nm and stable torque of 113 Nm are observed. Adjusting air gap thickness or permanent magnet dimensions results in torque variations. Additionally, due to carbon nanotube (CNT) characteristics, a reduction in losses from the Joule effect is noted. The maximum error between torque results from simulation and the suggested model is 7.68%, confirming accordance. Comparison alongside the slotted eddy current coupling (SEC) reveals that the torque of the NHUC exceeds that of the SEC, diminishing as the air gap widens.",
      "publication_date": "2024-07-05",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "22eb7f74d797e486fca0a7f320945b5124eb9e98",
      "title": "A Brushless Hybrid Excited Starter-Generator With Independent Excitation Flux Circuit for Aircraft AC Power Generation",
      "abstract": "To improve the power density and efficiency of starter-generators (SGs) for aircraft ac power systems, a hybrid excited SG (HESG) with an independent excitation flux circuit is presented in this article. Thanks to the Halbach-array permanent magnet (PM) arrangement, isolated PM, and electrical excited (EE) rotor magnetic paths, this topology is capable of handling strong overload capability and high efficiency. Then, the principle of flux regulation is revealed based on the magnetic equivalent circuit (MEC) and 3-D finite element analysis (3-D-FEA). Then, the key parameters, such as PM thickness, PM segments, PM rotor length, and EE rotor offset distance, are determined to improve the air-gap flux density and power density, and to reduce back EMF harmonic components. Furthermore, the electromagnetic performance of the proposed HESG is investigated, including the no-load flux density distribution, voltage regulation, on-load output and regulating characteristics, loss, as well as efficiency. It is demonstrated that the proposed HESG efficiency can achieve 95.8%. Finally, a 60 kVA prototype is manufactured and tested to verify the FE-predicted results.",
      "publication_date": "2024-09-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9c1b89701dbf1bb6094536248c0948949012c63c",
      "title": "A Novel Nonlinear Magnetic Equivalent Circuit Model for Magnetic Flux Leakage System",
      "abstract": "To ensure efficient inspection using the magnetic flux leakage (MFL) method, generating a flux density near the saturation level within the tested material is essential. This requirement brings high flux density conditions in the system\u2019s pole regions. Hence, leakage flux within the slot is excessively triggered, leading to distortion of the defect signal. In this context, the system dimensions stand out as one of the most significant factors affecting the mentioned flux distributions. Therefore, various alternative solutions with different system dimensions arise in the design process of the MFL system. This study proposes a magnetic equivalent circuit (MEC) model to achieve optimal system design. The proposed MEC model is designed considering the nonlinear behavior of the material, leakage flux, and fringing effects. Verification results demonstrate that the MEC model consistently tracks the finite element analysis (FEA) results in calculating the flux densities. Furthermore, the relative errors in the flux density calculations of the tested material are at a maximum level of 10.2% and an average of 5.2% compared to the FEA. These findings indicate that the proposed MEC model can be effectively utilized in rapid prototyping and optimization procedures of MFL system design by providing fast solutions with reasonable accuracy.",
      "publication_date": "2024-05-10",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4771f1a1f4e723e7efdc1b7747b00ff17b26ec6e",
      "title": "Two-Segment Magnet Transverse Flux Ferrite PM Generator for Direct-Drive Wind Turbine Applications: Nonlinear 3-D MEC Modeling and Experimental Validation",
      "abstract": "Transverse flux PM generator (TFPMG) is a capable option for direct-drive wind turbine (DDWT) applications due to its high-power characteristics at low speeds. NdFeB-based TFPMGs may suffer from a higher total cost and lower thermal capabilities compared to the ferrite-based TFPMGs. Not yet covered in the existing literature, in this paper a transverse flux ferrite PM generator (TFFPMG) is proposed, designed, and modeled, which can also resolve the unipolar flux generation and even-order harmonics in the flux linkage of the conventional TFPMGs that occurred in conventional TFPMGs through its innovative two-segment trapezoidal shape magnet structure. Due to the 3-D flux path nature in the proposed TFFPMG, either 3-D finite element analysis (FEA) or 3-D magnetic equivalent circuit (MEC) modeling should be used. As a computationally efficient modeling method, a nonlinear 3-D MEC is established to model the entire structure of the TFFPMG while the core saturation and nonlinear permeances are also fully considered to improve the modeling accuracy. The electromagnetic performance of the proposed TFFPMG modeled by 3-D MEC in various loading conditions is validated by both 3-D FEA and experimental results using a 2.5 kW TFFPMG. A close agreement between 3-D MEC modeling predictions, 3-D FEA, and test results confirms the reliability of the MEC modeling for the proposed TFFPMG.",
      "publication_date": "2022-09-01",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "02e384f3b77800ffd9e7c276bf401a620a0a8c1c",
      "title": "An Interpolation Based Meshed Magnetic Equivalent Circuit for Interior Permanent Magnet Synchronous Machines",
      "abstract": "This article proposes a new modeling approach for V-shaped interior permanent magnet synchronous motors (IPMSMs) using Lagrange interpolation in mesh-based magnetic equivalent circuits (MECs). The method divides each MEC block in the air gap into several child angles, interpolating flux densities and scalar potentials to ensure smooth induced voltages and torque waveforms. The article introduces a new meshed magnetic equivalent (M-MEC) circuit model, which can be used for IPMSMs of any shape. The M-MEC model achieves an error of less than 5% for all electromagnetic parameters and is 13 times faster than finite element models. This innovative technique is suitable for various applications requiring accurate electromagnetic parameter predictions for different geometry and loading conditions compared to traditional MEC models.",
      "publication_date": "2024-09-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "618e2830b9f1c67b265d25125a6b48520f1a4ac3",
      "title": "Utilizing an Efficient Magnetic Equivalent Circuit Model and Manifold Mapping Method for Two-Level Optimization of Axial Flux Machine",
      "abstract": "In this paper, a two-level optimization algorithm is developed based on rapid magnetic equivalent circuit (MEC) model and manifold mapping (MM) method. The performance of an axial flux permanent magnet machine (AFPM) is optimized using this approach. The rapid MEC model based on grid division exhibits significant computational efficiency, while reducing the influence of artificial factors in modeling process. Further, quantitative comparison of the error performance under different combinations of divisions are carried out. Subsequently, coarse and fine models are selected, respectively. The iterative process indicates that the modeling approach enables the coarse model to correctly follow the trends of fine model as input changes. As a result, the fast convergence of the MM is successfully achieved.",
      "publication_date": "2024-06-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "39201db4005cfccd9290bf96e1d2cb3132db1e16",
      "title": "Development of a Combined Maxwell's Equations and Magnetic Equivalent Circuit Solution for Induction Machines in Electric Vehicle Applications",
      "abstract": "This study introduces a novel analytical technique for analyzing the magnetic field in induction motors (IM). This method combines Maxwell's equations with the magnetic equivalent circuit (MEC) framework to determine flux densities in various regions of the motor. By integrating flux sources from Maxwell's equations into the MEC's network of reluctances, this method accurately considers the distribution of main and leakage magnetic fluxes within the motor. The approach significantly reduces analysis time while maintaining high accuracy and reliability when compared to numerical methods. Furthermore, it provides important motor features such as the radial and tangential components of the flux density in the air-gap, stator and rotor areas, as well as the induced voltage. Finally, to validate the accuracy of the proposed method, the analytical results are compared to the case that no leakage fluxes are included in the analytical model and to the finite-element method (FEM) in terms of computation time and accuracy. As such, this method is suggested to serve as a valuable analytical tool during the design process of IMs.",
      "publication_date": "2024-05-05",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ab7b9e504d427abbe7798e441442566ed7ea967a",
      "title": "Joint Computation Offloading and Radio Resource Allocation in MEC-Based Wireless-Powered Backscatter Communication Networks",
      "abstract": "The multi-access edge computing (MEC)-based wireless-powered backscatter communication networks (WP-BackComNets) allow wireless devices (WDs) to offload computation resources to lightweight and widely deployed MEC servers with the assistance of backscatter devices (BDs), which have substantial application prospects for the emerging Internet-of-Things applications. However, the limited battery capacity of WDs is one of the bottlenecks restricting its further development. Reducing the energy consumption and the computation burden of WDs while ensuring the quality-of-service requirements is an urgent issue. To this end, a joint computation offloading and radio resource allocation problem is formulated to minimize the total energy consumption of WDs for an MEC-based WP-BackComNet by jointly optimizing user association, the transmit power and transmission time of WDs, the computational offloading coefficient of each task, and the reflection coefficients of BDs, where the circuit power consumption of BDs, the computational capabilities of WDs, and the task execution delay budgets are considered. To handle this non-convex problem, we propose an efficient algorithm to obtain a suboptimal solution. Simulation results demonstrate that the proposed scheme can effectively decrease the energy consumption compared with the benchmarks.",
      "publication_date": "2021-05-03",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e73959ee703c09e2b05531cf5ef1bff51598abe3",
      "title": "Bioelectrochemical enhancement of methane production from exhausted vine shoot fermentation broth by integration of MEC with anaerobic digestion",
      "abstract": "A microbial electrolysis cell integrated in an anaerobic digestion system (MEC-AD) is an efficient configuration to produce methane from an exhausted vine shoot fermentation broth (EVS). The cell worked in a single-chamber two-electrode configuration at an applied potential of 1\u00a0V with a feeding ratio of 30/70 (30% EVS to 70% synthetic medium). In addition, an identical cell operated in an open circuit was used as a control reactor. Experimental results showed similar behavior in terms of carbon removal (70\u201376%), while the specific averaged methane production from cycle 7 was more stable and higher in the connected cell (MECAD) compared with the unpolarized one (OCAD) accounting for 403.7\u2009\u00b1\u200933.6 L CH4\u00b7kg VS\u22121 and 121.3\u2009\u00b1\u200949.7 L CH4\u00b7kg VS\u22121, respectively. In addition, electrochemical impedance spectroscopy revealed that the electrical capacitance of the bioanode in MECAD was twice the capacitance shown by OCAD. The bacterial community in both cells was similar but a clear adaptation of Methanosarcina Archaea was exhibited in MECAD, which could explain the increased yields in CH4 production. In summary, the results reported here confirm the advantages of integrating MEC-AD for the treatment of real organic liquid waste instead of traditional AD treatment.",
      "publication_date": "2022-06-08",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5336257faf7eeaceb742b55f81d4154f89998e42",
      "title": "Mesh-Based 3D MEC Modeling of a Novel Hybrid Claw Pole Generator",
      "abstract": "A brushless parallel hybrid excitation claw pole generator (HECPG) is proposed for electric vehicle (EV) application. Permanent magnet (PM) excitation method can reduce the volume of the machine and improve the power density and efficiency. Moreover, the voltage regulation can be ensured by field excitation. The flux path of the proposed HECPG is complex, and it will take a long time for 3D finite element analysis (FEA) to process it. To reduce simulation time, the mathematical model of the generator is given by a mesh-based 3D magnet equivalent circuit (MEC) network method considering radial and axial flux, magnetic saturation, and magnetic flux leakage. The performance of the generator is analyzed by FEA and prototype experiment. Finally, the results of 3D MEC, FEA, and experiment are compared. There is little difference between the three results, so 3D MEC can ensure the accuracy and significantly reduce the simulation time. The efficiency of the proposed HECPG is 90%, and the DC-Bus voltage can be modulated by changing the amplitude of field current.",
      "publication_date": "2022-02-24",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ed185db19b0b1025817bcde562b7e9b7b2ba1f7e",
      "title": "A Novel Modeling Technique via Coupled Magnetic Equivalent Circuit With Vector Hysteresis Characteristics of Laminated Steels",
      "abstract": "This paper proposes a method to include the anisotropic hysteresis characteristics of soft-magnetic laminated steels in the magnetic equivalent circuit (MEC) modeling. The loop-based MEC formulation is improved to handle the nonlinearity of the anisotropic magnetic hysteresis, including the dynamic classical eddy-current and excess fields. The developed MEC model is coupled with both the single-valued $B$-$H$ curve (SVC) in magnetostatic and the dynamic vector hysteresis model (VHM) in transient analysis. Results with a single elementary MEC element show that an alternating magnetic field in a single direction with a peak value smaller than 300 A/m causes a discrepancy of more than 10% between the magnetic flux densities calculated by the VHM and SVC at 50 and 200 Hz excitation frequencies. Moreover, the proposed modeling technique is verified experimentally using the laminated transformer core of TEAM problem 32. The induced voltage calculated by the MEC model with the VHM demonstrates a good agreement with the measurements, while the MEC model with the SVC calculates inaccurate voltage waveforms. Lastly, the total iron loss dissipated in the transformer's iron core is investigated to verify the proposed technique under different excitation levels and frequencies up to 500 Hz. It is observed that the proposed MEC model with the vector hysteresis characteristics of laminated steels is able to calculate the iron loss accurately, while the conventional single-valued curve method fails to estimate the iron loss.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1b34f47656f7d490bc433aeb0d4875f2247cb7d3",
      "title": "Improved Configuration Proposal for Axial Reluctance Resolver Using 3-D Magnetic Equivalent Circuit Model and Winding Function Approach",
      "abstract": "Resolver is a position sensor that is widely employed in motor servo systems under harsh working conditions. In this article, a novel axial flux resolver with a fractional-slot sinusoidal distributed winding configuration is developed, which has the advantages of a simple structure and convenient mass production. Sinusoidal distribution winding expands the optional range of pole\u2013slot combinations, and its harmonic suppression capability is proposed and verified by the winding function approach (WFA). Furthermore, the magnetic equivalent circuit (MEC) model is formulated to analyze the impact of structural parameters on the induced voltage envelope, through which the optimal parameters on the resolver are developed. In terms of computation time and accuracy, the results of the MEC model under healthy and eccentric conditions are compared with the finite-element method (FEM). In addition, the antiaxial offset capability of the half-wave resolver is illustrated and verified. The effectiveness and superiority of the proposed resolver are illustrated and verified based on numerical simulations and experimental tests.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "fb3dee071c90c2e860cd4023f682f2b11b8955c1",
      "title": "3D Rotor Position-Dependant MEC Modelling of Different Claw Pole Machine Topologies",
      "abstract": "The paper is aimed at a 3D magnetic equivalent circuit (MEC)-based modelling of claw pole synchronous machine topologies. Beyond the magnetic saturation and the armature magnetic reaction, the proposed modelling approach takes into consideration the rotor position variation, yielding the so-called: rotor position-dependant MEC. Accounting for the complexity of the magnetic circuit of claw pole topologies, specific assumptions are adopted prior a general analytical derivation of their MEC models. The developed analytical approach focuses on the air gap reluctance under variable rotor position considering a simplified geometry of the claw. A dedicated numerical procedure based on the Newton-Raphson algorithm is proposed for the resolution of the designed rotor position-dependant MEC. The proposed approach is applied to three claw pole topologies. The two first ones are equipped with a single source of excitation achieved by a field. Their analytically-predicted features are validated by experiments. The third topology has a dual excitation achieved by a field and permanent magnets (PMs) in the rotor. Its analytically-predicted features are validated by 3D finite element analysis (FEA). It is found that both experimental and FEA results are in quite good agreement with the analytical predictions yielded by the proposed rotor position-dependant MEC.",
      "publication_date": null,
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e294c3cc0071c5cd34273455342b75182b621908",
      "title": "Analysis and Compensation of the Longitudinal End Effect in Variable Reluctance Linear Resolvers Using Magnetic Equivalent Circuit Model",
      "abstract": "This article focused on the longitudinal end effect (LEE) in linear variable reluctance (VR) resolvers. The LEE can significantly decrease the accuracy of the linear sensor. Therefore, a novel method based on a magnetic equivalent circuit (MEC) model is proposed for accurate modeling of this phenomenon and used in an optimization routine to suppress that. The LEE is modeled by adding three extra permeances into the conventional MEC model without increasing the complexity and simulation time. Besides, the method of calculating these permeances is described. After that, some techniques are presented to decrease the computational burden of the MEC model. Then, the model is used to compensate the LEE by optimizing the turn number of coils. Comparing the results of the proposed model and the finite-element method (FEM) shows that the deviation of the model in the prediction of different inductances is less than 5%, while the simulation time is about 31 times less than that of FEM. Besides, the comparison between the initial design and the optimal design shows the usefulness of the model in the optimization process, in which the accuracy of the sensor improved by 74.1%. Finally, experimental tests on the prototype sensor verify the success of the analysis.",
      "publication_date": "2023-09-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    }
  ],
  "final_ranked_papers": [
    {
      "paper_id": "cd98854d3482e276b7124712c02e20ed48f56a0f",
      "title": "A generative model of the hippocampal formation trained with theta driven local learning rules",
      "abstract": "Advances in generative models have recently revolutionised machine learning. Meanwhile, in neuroscience, generative models have long been thought fundamental to animal intelligence. Understanding the biological mechanisms that support these processes promises to shed light on the relationship between biological and artificial intelligence. In animals, the hippocampal formation is thought to learn and use a generative model to support its role in spatial and non-spatial memory. Here we introduce a biologically plausible model of the hippocampal formation tantamount to a Helmholtz machine that we apply to a temporal stream of inputs. A novel component of our model is that fast theta-band oscillations (5-10 Hz) gate the direction of information flow throughout the network, training it akin to a high-frequency wake-sleep algorithm. Our model accurately infers the latent state of high-dimensional sensory environments and generates realistic sensory predictions. Furthermore, it can learn to path integrate by developing a ring attractor connectivity structure matching previous theoretical proposals and flexibly transfer this structure between environments. Whereas many models trade-off biological plausibility with generality, our model captures a variety of hippocampal cognitive functions under one biologically plausible local learning rule.",
      "publication_date": "2023-12-13",
      "venue": "bioRxiv",
      "year": 2023,
      "citation_count": 9,
      "authors": [
        "Tom M George",
        "C. Barry",
        "K. Stachenfeld",
        "C. Clopath",
        "T. Fukai"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "791686d4fa91082e225797a13060adeeb0fddf13",
      "title": "A unified theory for the computational and mechanistic origins of grid cells",
      "abstract": "The discovery of entorhinal grid cells has generated considerable interest in how and why hexagonal firing fields might mechanistically emerge in a generic manner from neural circuits, and what their computational significance might be. Here we forge an intimate link between the computational problem of path-integration and the existence of hexagonal grids, by demonstrating that such grids arise generically in biologically plausible neural networks trained to path integrate. Moreover, we develop a unifying theory for why hexagonal grids are so ubiquitous in path-integrator circuits. Such trained networks also yield powerful mechanistic hypotheses, exhibiting realistic levels of biological variability not captured by hand-designed models. We furthermore develop methods to analyze the connectome and activity maps of our trained networks to elucidate fundamental mechanisms underlying path integration. These methods provide an instructive roadmap to go from connectomic and physiological measurements to conceptual understanding in a manner that might be generalizable to other settings.",
      "publication_date": "2020-12-30",
      "venue": "Neuron",
      "year": 2020,
      "citation_count": 83,
      "authors": [
        "Ben Sorscher",
        "Gabriel C. Mel",
        "Samuel A. Ocko",
        "Lisa M. Giocomo",
        "S. Ganguli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2510c7bcbd6278b50dc73e02d127546ef70141bd",
      "title": "Learning an efficient place cell map from grid cells using non-negative sparse coding",
      "abstract": "Experimental studies of grid cells in the Medial Entorhinal Cortex (MEC) have shown that they are selective to an array of spatial locations in the environment that form a hexagonal grid. However, in a small environment, place cells in the hippocampus are only selective to a single-location of the environment while granule cells in the dentate gyrus of the hippocampus have multiple discrete firing locations, but lack spatial periodicity. Given the anatomical connection from MEC to the hippocampus, previous feedforward models of grid-to-place have been proposed. Here, we propose a unified learning model that can describe the spatial tuning properties of both hippocampal place cells and dentate gyrus granule cells based on non-negative sparse coding. Sparse coding plays an important role in many cortical areas and is proposed here to have a key role in the navigational system of the brain in the hippocampus. Our results show that the hexagonal patterns of grid cells with various orientations, grid spacings and phases are necessary for model cells to learn a single spatial field that efficiently tile the entire spatial environment. However, if there is a lack of diversity in any grid parameters or a lack of cells in the network, this will lead to the emergence of place cells that have multiple firing locations. More surprisingly, the model shows that place cells can also emerge even when non-negative sparse coding is used with weakly-tuned MEC cells, instead of MEC grid cells, as the input to place cells. This work suggests that sparse coding may be one of the underlying organizing principles for the navigational system of the brain.",
      "publication_date": "2020-08-13",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c1585013b1dbb7341a8a6821159d8d7681e81503",
      "title": "Predictive grid coding in the medial entorhinal cortex",
      "abstract": "The entorhinal cortex represents allocentric spatial geometry and egocentric speed and heading information required for spatial navigation. However, it remains unclear whether it contributes to the prediction of an animal\u2019s future location. We discovered grid cells in the medial entorhinal cortex (MEC) that have grid fields representing future locations during goal-directed behavior. These predictive grid cells represented prospective spatial information by shifting their grid fields against the direction of travel. Predictive grid cells discharged at the trough phases of the hippocampal CA1 theta oscillation and, together with other types of grid cells, organized sequences of the trajectory from the current to future positions across each theta cycle. Our results suggest that the MEC provides a predictive map that supports forward planning in spatial navigation. Editor\u2019s summary Grid cells in the entorhinal cortex create a coordinate system of the environment for spatial navigation. However, it is not clear whether the entorhinal grid system is also involved in predicting where an animal will be in the next moment. Ouchi and Fujisawa performed high-density neuronal recordings in the entorhinal cortex and hippocampal area CA1 of rats during goal-directed behavior in an open field. They observed neurons in layer 3 of the medial entorhinal cortex that explicitly encode a grid representation of an animal\u2019s future projected location, not the current position, and named them \u201c\u201cpredictive grid cells.\u201d Neuronal assemblies in the medial entorhinal cortex thus organize a predictive cognitive map. \u2014Peter Stern",
      "publication_date": "2024-08-16",
      "venue": "Science",
      "year": 2024,
      "citation_count": 7,
      "authors": [
        "Ayako Ouchi",
        "Shigeyoshi Fujisawa"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a538579ac50d659ac0bca9824d6446e741c586b3",
      "title": "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization",
      "abstract": "Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits.",
      "publication_date": "2018-02-15",
      "venue": "International Conference on Learning Representations",
      "year": 2018,
      "citation_count": 216,
      "authors": [
        "Christopher J. Cueva",
        "Xue-Xin Wei"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "27bc20a95969cffa29065fcc1beb63ee2386b852",
      "title": "Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells",
      "abstract": "To solve the spatial problems of mapping, localization and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learnt this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, there is no satisfactory synthesis of core principles that lead to multi-modular grid cells in deep recurrent neural networks. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: coding theory, dynamical systems, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, without access to supervised position information or engineering of particular readout representations as needed in previous approaches. We show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks and emergent representations generalize well outside their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.",
      "publication_date": "2023-11-04",
      "venue": "Neural Information Processing Systems",
      "year": 2023,
      "citation_count": 20,
      "authors": [
        "Rylan Schaeffer",
        "Mikail Khona",
        "Tzuhsuan Ma",
        "Cristobal Eyzaguirre",
        "Sanmi Koyejo",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2bf3295a0cec7163c063c0d3967cc0935396bb3e",
      "title": "Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis",
      "abstract": "Many recent models study the downstream projection from grid cells to place cells, while recent data have pointed out the importance of the feedback projection. We thus asked how grid cells are affected by the nature of the input from the place cells. We propose a single-layer neural network with feedforward weights connecting place-like input cells to grid cell outputs. Place-to-grid weights are learned via a generalized Hebbian rule. The architecture of this network highly resembles neural networks used to perform Principal Component Analysis (PCA). Both numerical results and analytic considerations indicate that if the components of the feedforward neural network are non-negative, the output converges to a hexagonal lattice. Without the non-negativity constraint, the output converges to a square lattice. Consistent with experiments, grid spacing ratio between the first two consecutive modules is \u22121.4. Our results express a possible linkage between place cell to grid cell interactions and PCA. DOI: http://dx.doi.org/10.7554/eLife.10094.001",
      "publication_date": "2016-03-08",
      "venue": "eLife",
      "year": 2016,
      "citation_count": 147,
      "authors": [
        "Yedidyah Dordek",
        "Daniel Soudry",
        "R. Meir",
        "D. Derdikman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "cbeb0b647964650757d5e25aa711916e12a896dc",
      "title": "Grid cell firing patterns maintain their hexagonal firing patterns on a circular track",
      "abstract": "In an open two-dimensional environment, grid cells in the medial entorhinal cortex are known to be active in multiple locations, displaying a striking periodic hexagonal firing pattern covering the entire space. Both modeling and experimental data suggest that such periodic spatial representations may emerge from a continuous attractor network. According to this theory, grid cell activity in any stable 1D environment is a slice through an underlying 2D hexagonal pattern, which is supported by some experimental studies but challenged by others. Grid cells are believed to play a fundamental role in path integration, and so understanding their behavior in various environments is crucial for understanding the flow of information through the entorhinal-hippocampal system. To this end, we analyzed the activity of grid cells when rats traversed a circular track. A previous study involving this data set analyzed individual grid cell activity patterns separately, but we found that individual grid cells do not provide sufficient data for determining the under-lying spatial activity pattern. To circumvent this, we compute the population autocorrelation, which pools together population responses from all grid cells within the same module. This novel approach recovers the underlying six-peak hexagonal pattern that was not observable in the individual autocorrelations. We also use the population autocorrelation to infer the spacing and orientation of the population lattice, revealing how the lattice differs across environments. Furthermore, the population autocorrelation of the linearized track reveals that at the level of the population, grid cells have an allocentric code for space. These results are strong support for the attractor network theory for grid cells, and our novel approach can be used to analyze grid cell activity in any undersampled environment.",
      "publication_date": "2023-09-15",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1fe70364ca7c7cf3b5008969518fd3257e62a232",
      "title": "The hippocampus as a predictive map",
      "abstract": "A cognitive map has long been the dominant metaphor for hippocampal function, embracing the idea that place cells encode a geometric representation of space. However, evidence for predictive coding, reward sensitivity and policy dependence in place cells suggests that the representation is not purely spatial. We approach this puzzle from a reinforcement learning perspective: what kind of spatial representation is most useful for maximizing future reward? We show that the answer takes the form of a predictive representation. This representation captures many aspects of place cell responses that fall outside the traditional view of a cognitive map. Furthermore, we argue that entorhinal grid cells encode a low-dimensionality basis set for the predictive representation, useful for suppressing noise in predictions and extracting multiscale structure for hierarchical planning.",
      "publication_date": "2017-06-07",
      "venue": "Nature Neuroscience",
      "year": 2017,
      "citation_count": 757,
      "authors": [
        "Kimberly L. Stachenfeld",
        "M. Botvinick",
        "S. Gershman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "26fc224a118e20e5d6bc98b57711001abc20ec00",
      "title": "Recurrent predictive coding models for associative memory employing covariance learning",
      "abstract": "The computational principles adopted by the hippocampus in associative memory (AM) tasks have been one of the mostly studied topics in computational and theoretical neuroscience. Classical models of the hippocampal network assume that AM is performed via a form of covariance learning, where associations between memorized items are represented by entries in the learned covariance matrix encoded in the recurrent connections in the hippocampal subfield CA3. On the other hand, it has been recently proposed that AM in the hippocampus is achieved through predictive coding. Hierarchical predictive coding models following this theory perform AM, but fail to capture the recurrent hippocampal structure that encodes the covariance in the classical models. Such a dichotomy pose potential difficulties for developing a unitary theory of how memory is formed and recalled in the hippocampus. Earlier predictive coding models that learn the covariance information of inputs explicitly seem to be a solution to this dichotomy. Here, we show that although these models can perform AM, they do it in an implausible and numerically unstable way. Instead, we propose alternatives to these earlier covariance-learning predictive coding networks, which learn the covariance information implicitly and plausibly, and can use dendritic structures to encode prediction errors. We show analytically that our proposed models are perfectly equivalent to the earlier predictive coding model learning covariance explicitly, and encounter no numerical issues when performing AM tasks in practice. We further show that our models can be combined with hierarchical predictive coding networks to model the hippocampo-neocortical interactions. Our models provide a biologically plausible approach to modelling the hippocampal network, pointing to a potential computational mechanism employed by the hippocampus during memory formation and recall, which unifies predictive coding and covariance learning based on the recurrent network structure. Author summary The hippocampus and adjacent cortical areas have long been considered essential for the formation of associative memories. Earlier theoretical works have assumed that the hippocampus stores in its recurrent connections statistical regularities embedded in the sensory inputs. On the other hand, it has been recently suggested that the hippocampus retrieves memory by generating predictions of ongoing sensory inputs. Computational models have thus been proposed to account for this predictive nature of the hippocampal network using predictive coding, a general theory of information processing in the cortex. However, these hierarchical predictive coding models of the hippocampus did not describe how it stores the statistical regularities that play a key role for associative memory in the classical hippocampal models, hindering a unified understanding of the underlying computational principles employed by the hippocampus. To address this dichotomy, here we present a family of predictive coding models that also learn the statistical information needed for associative memory. Our models can stably perform associative memory tasks in a biologically plausible manner, even with large structured data such as natural scenes. Our work provides a possible mechanism of how the recurrent hippocampal network may employ various computational principles concurrently to perform associative memory.",
      "publication_date": "2022-11-09",
      "venue": "bioRxiv",
      "year": 2022,
      "citation_count": 25,
      "authors": [
        "Mufeng Tang",
        "Tommaso Salvatori",
        "Beren Millidge",
        "Yuhang Song",
        "Thomas Lukasiewicz",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "20d6f4241af85a9c4fe5f0fcb583c818441282c4",
      "title": "Learning place cells, grid cells and invariances: A unifying model",
      "abstract": "Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction. The underlying circuit mechanisms are not fully resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, a place cell is highly selective to location, but typically invariant to head direction. Here, we propose that all observed spatial tuning patterns \u2013 in both their selectivity and their invariance \u2013 are a consequence of the same mechanism: Excitatory and inhibitory synaptic plasticity that is driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. The model is robust to changes in parameters, develops patterns on behavioral time scales and makes distinctive experimental predictions. Our results suggest that the interaction of excitatory and inhibitory plasticity is a general principle for the formation of neural representations.",
      "publication_date": "2017-02-17",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6a77b5266b492c69dec62163cd692f86b9b72860",
      "title": "Emergent elasticity in the neural code for space",
      "abstract": "Significance We develop a theoretical model, grounded in known properties of neural dynamics and synaptic plasticity, that can fuse information gathered from the past history of velocity and sequence of encountered landmarks during exploratory behavior, to construct a self-consistent internal representation of space. Moreover, through model reduction techniques, we obtain conceptual insights into how consistent internal spatial representations naturally emerge through an elastic relaxation process in an effective spring\u2013particle system. We verify several experimentally testable predictions of our model involving the spatial behavior of grid cells in the medial entorhinal cortex, as well as suggest additional experiments. Upon encountering a novel environment, an animal must construct a consistent environmental map, as well as an internal estimate of its position within that map, by combining information from two distinct sources: self-motion cues and sensory landmark cues. How do known aspects of neural circuit dynamics and synaptic plasticity conspire to accomplish this feat? Here we show analytically how a neural attractor model that combines path integration of self-motion cues with Hebbian plasticity in synaptic weights from landmark cells can self-organize a consistent map of space as the animal explores an environment. Intriguingly, the emergence of this map can be understood as an elastic relaxation process between landmark cells mediated by the attractor network. Moreover, our model makes several experimentally testable predictions, including (i) systematic path-dependent shifts in the firing fields of grid cells toward the most recently encountered landmark, even in a fully learned environment; (ii) systematic deformations in the firing fields of grid cells in irregular environments, akin to elastic deformations of solids forced into irregular containers; and (iii) the creation of topological defects in grid cell firing patterns through specific environmental manipulations. Taken together, our results conceptually link known aspects of neurons and synapses to an emergent solution of a fundamental computational problem in navigation, while providing a unified account of disparate experimental observations.",
      "publication_date": "2018-05-21",
      "venue": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": 2018,
      "citation_count": 70,
      "authors": [
        "Samuel A. Ocko",
        "Kiah Hardcastle",
        "Lisa M. Giocomo",
        "S. Ganguli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8a8f7953894fef09d32c5f35fd79cfbff025e56f",
      "title": "Sequential Memory with Temporal Predictive Coding",
      "abstract": "Forming accurate memory of sequential stimuli is a fundamental function of biological agents. However, the computational mechanism underlying sequential memory in the brain remains unclear. Inspired by neuroscience theories and recent successes in applying predictive coding (PC) to static memory tasks, in this work we propose a novel PC-based model for sequential memory, called temporal predictive coding (tPC). We show that our tPC models can memorize and retrieve sequential inputs accurately with a biologically plausible neural implementation. Importantly, our analytical study reveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN) with an implicit statistical whitening process, which leads to more stable performance in sequential memory tasks of structured inputs. Moreover, we find that tPC exhibits properties consistent with behavioral observations and theories in neuroscience, thereby strengthening its biological relevance. Our work establishes a possible computational mechanism underlying sequential memory in the brain that can also be theoretically interpreted using existing memory model frameworks.",
      "publication_date": "2023-05-19",
      "venue": "Neural Information Processing Systems",
      "year": 2023,
      "citation_count": 19,
      "authors": [
        "Mufeng Tang",
        "Helen C. Barron",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "138a75cab26100df70bdb66dd1a28e68612db8ff",
      "title": "Learning place cells, grid cells and invariances with excitatory and inhibitory plasticity",
      "abstract": "Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction, and the underlying circuit mechanisms are not yet resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, place cells are typically invariant to head direction. We propose that all observed spatial tuning patterns \u2013 in both their selectivity and their invariance \u2013 arise from the same mechanism: Excitatory and inhibitory synaptic plasticity driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. Our proposed model is robust to changes in parameters, develops patterns on behavioral timescales and makes distinctive experimental predictions.",
      "publication_date": "2018-02-21",
      "venue": "eLife",
      "year": 2018,
      "citation_count": 50,
      "authors": [
        "Simon Nikolaus Weber",
        "Henning Sprekeler"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "35ce44e06802d9f778eb02bf03a6fc5aa657bd56",
      "title": "Inferring neural activity before plasticity as a foundation for learning beyond backpropagation",
      "abstract": "This paper introduces \u2018prospective configuration\u2019, a new principle for learning in neural networks, which differs from backpropagation and is more efficient in learning and more consistent with data on neural activity and behavior. For both humans and machines, the essence of learning is to pinpoint which components in its information processing pipeline are responsible for an error in its output, a challenge that is known as \u2018credit assignment\u2019. It has long been assumed that credit assignment is best solved by backpropagation, which is also the foundation of modern machine learning. Here, we set out a fundamentally different principle on credit assignment called \u2018prospective configuration\u2019. In prospective configuration, the network first infers the pattern of neural activity that should result from learning, and then the synaptic weights are modified to consolidate the change in neural activity. We demonstrate that this distinct mechanism, in contrast to backpropagation, (1) underlies learning in a well-established family of models of cortical circuits, (2) enables learning that is more efficient and effective in many contexts faced by biological organisms and (3) reproduces surprising patterns of neural activity and behavior observed in diverse human and rat learning experiments.",
      "publication_date": "2024-01-03",
      "venue": "Nature Neuroscience",
      "year": 2024,
      "citation_count": 16,
      "authors": [
        "Yuhang Song",
        "Beren Millidge",
        "Tommaso Salvatori",
        "Thomas Lukasiewicz",
        "Zhenghua Xu",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "bc7417738a7f7186ce9b06933072c4ddd39ce4b1",
      "title": "Actionable Neural Representations: Grid Cells from Minimal Constraints",
      "abstract": "To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won't have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an `actionable representation'. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode.",
      "publication_date": "2022-09-30",
      "venue": "International Conference on Learning Representations",
      "year": 2022,
      "citation_count": 23,
      "authors": [
        "W. Dorrell",
        "P. Latham",
        "T. Behrens",
        "James C. R. Whittington"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d40cb3f74f7e4a0858bc63760ec983f15de8125c",
      "title": "Grid-Cell Activity on Linear Tracks Indicates Purely Translational Remapping of 2D Firing Patterns at Movement Turning Points",
      "abstract": "Grid cells in rodent medial entorhinal cortex are thought to play a critical role for spatial navigation. When the animal is freely moving in an open arena the firing fields of each grid cell tend to form a hexagonal lattice spanning the environment. For movements along a linear track the cells seem to respond differently. They show multiple firing fields that are not periodically arranged and whose shape and position change when the running direction is reversed. In addition, peak firing rates vary widely from field to field. Measured along one running direction only, firing fields are, however, compatible with a slice through a two-dimensional (2D) hexagonal pattern. It is an open question, whether this is also true if leftward and rightward runs are jointly considered. By analyzing data from 15 male Long\u2013Evans rats, we show that a single hexagonal firing pattern explains the linear-track data if translational shifts of the pattern are allowed at the movement turning points. A rotation or scaling of the grid is not required. The agreement is further improved if the peak firing rates of the underlying 2D grid fields can vary from field to field, as suggested by recent studies. These findings have direct consequences for experiments using linear tracks in virtual reality. SIGNIFICANCE STATEMENT Various types of neurons support spatial navigation. Their response properties are often studied in reduced settings and might change when the animal can freely explore its environment. Grid cells in rodents, for example, exhibit seemingly irregular firing fields when animal movement is restricted to a linear track but highly regular patterns in two-dimensional (2D) arenas. We show that linear-track responses of a cell for both leftward and rightward running directions can be explained as cuts through a single hexagonal pattern if translational remapping is allowed at movement turning points; neither rotations nor scale transformations are needed. These results provide a basis to quantify grid-cell activity in 1D virtual reality and could help to detect and categorize grid cells without experiments in 2D environments.",
      "publication_date": "2018-07-05",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "de629192ca5b0b7195355149dad23bd4124bf3df",
      "title": "Irregular distribution of grid cell firing fields in rats exploring a 3D volumetric space",
      "abstract": "We investigated how entorhinal grid cells encode volumetric space. On a horizontal surface, grid cells usually produce multiple, spatially focal, approximately circular firing fields that are evenly sized and spaced to form a regular, close-packed, hexagonal array. This spatial regularity has been suggested to underlie navigational computations. In three dimensions, theoretically the equivalent firing pattern would be a regular, hexagonal close packing of evenly sized spherical fields. In the present study, we report that, in rats foraging in a cubic lattice, grid cells maintained normal temporal firing characteristics and produced spatially stable firing fields. However, although most grid fields were ellipsoid, they were sparser, larger, more variably sized and irregularly arranged, even when only fields abutting the lower surface (equivalent to the floor) were considered. Thus, grid self-organization is shaped by the environment\u2019s structure and/or movement affordances, and grids may not need to be regular to support spatial computations. Grieves et al. show that when rats explore a 3D space, grid cells in the entorhinal cortex exchange their usual spatially regular firing patterns for more irregular ones, suggesting that 3D space is mapped differently than previously thought.",
      "publication_date": "2021-08-11",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d09c79f6b7420c8c8a28ecab0836ec9bacb8f8cf",
      "title": "A non-spatial account of place and grid cells based on clustering models of concept learning",
      "abstract": "One view is that conceptual knowledge is organized using the circuitry in the medial temporal lobe (MTL) that supports spatial processing and navigation. In contrast, we find that a domain-general learning algorithm explains key findings in both spatial and conceptual domains. When the clustering model is applied to spatial navigation tasks, so-called place and grid cell-like representations emerge because of the relatively uniform distribution of possible inputs in these tasks. The same mechanism applied to conceptual tasks, where the overall space can be higher-dimensional and sampling sparser, leading to representations more aligned with human conceptual knowledge. Although the types of memory supported by the MTL are superficially dissimilar, the information processing steps appear shared. Our account suggests that the MTL uses a general-purpose algorithm to learn and organize context-relevant information in a useful format, rather than relying on navigation-specific neural circuitry. Spatial maps in the medial temporal lobe (MTL) have been proposed to map abstract conceptual knowledge. Rather than grounding abstract knowledge in a spatial map, the authors propose a general-purpose clustering algorithm that explains how both spatial (including place and grid cells) and higher-dimensional conceptual representations arise during learning.",
      "publication_date": "2018-09-19",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "da741827ab6f5bf9762d30b91fd2409ac9e0e272",
      "title": "World models and predictive coding for cognitive and developmental robotics: frontiers and challenges",
      "abstract": "ABSTRACT Creating autonomous robots that can actively explore the environment, acquire knowledge and learn skills continuously is the ultimate achievement envisioned in cognitive and developmental robotics. Importantly, if the aim is to create robots that can continuously develop through interactions with their environment, their learning processes should be based on interactions with their physical and social world in the manner of human learning and cognitive development. Based on this context, in this paper, we focus on the two concepts of world models and predictive coding. Recently, world models have attracted renewed attention as a topic of considerable interest in artificial intelligence. Cognitive systems learn world models to better predict future sensory observations and optimize their policies, i.e. controllers. Alternatively, in neuroscience, predictive coding proposes that the brain continuously predicts its inputs and adapts to model its own dynamics and control behavior in its environment. Both ideas may be considered as underpinning the cognitive development of robots and humans capable of continual or lifelong learning. Although many studies have been conducted on predictive coding in cognitive robotics and neurorobotics, the relationship between world model-based approaches in AI and predictive coding in robotics has rarely been discussed. Therefore, in this paper, we clarify the definitions, relationships, and status of current research on these topics, as well as missing pieces of world models and predictive coding in conjunction with crucially related concepts such as the free-energy principle and active inference in the context of cognitive and developmental robotics. Furthermore, we outline the frontiers and challenges involved in world models and predictive coding toward the further integration of AI and robotics, as well as the creation of robots with real cognitive and developmental capabilities in the future. GRAPHICAL ABSTRACT",
      "publication_date": "2023-01-14",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4af94a163a695f14597ac101a9562a68772010ae",
      "title": "Probabilistic Learning by Rodent Grid Cells",
      "abstract": "Mounting evidence shows mammalian brains are probabilistic computers, but the specific cells involved remain elusive. Parallel research suggests that grid cells of the mammalian hippocampal formation are fundamental to spatial cognition but their diverse response properties still defy explanation. No plausible model exists which explains stable grids in darkness for twenty minutes or longer, despite being one of the first results ever published on grid cells. Similarly, no current explanation can tie together grid fragmentation and grid rescaling, which show very different forms of flexibility in grid responses when the environment is varied. Other properties such as attractor dynamics and grid anisotropy seem to be at odds with one another unless additional properties are assumed such as a varying velocity gain. Modelling efforts have largely ignored the breadth of response patterns, while also failing to account for the disastrous effects of sensory noise during spatial learning and recall, especially in darkness. Here, published electrophysiological evidence from a range of experiments are reinterpreted using a novel probabilistic learning model, which shows that grid cell responses are accurately predicted by a probabilistic learning process. Diverse response properties of probabilistic grid cells are statistically indistinguishable from rat grid cells across key manipulations. A simple coherent set of probabilistic computations explains stable grid fields in darkness, partial grid rescaling in resized arenas, low-dimensional attractor grid cell dynamics, and grid fragmentation in hairpin mazes. The same computations also reconcile oscillatory dynamics at the single cell level with attractor dynamics at the cell ensemble level. Additionally, a clear functional role for boundary cells is proposed for spatial learning. These findings provide a parsimonious and unified explanation of grid cell function, and implicate grid cells as an accessible neuronal population readout of a set of probabilistic spatial computations.",
      "publication_date": "2016-10-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    }
  ],
  "all_retrieved_papers": [
    {
      "paper_id": "5e2fdf42d985b3eb9eeb8387e1f7d093ade81525",
      "title": "Vector-based navigation using grid-like representations in artificial agents",
      "abstract": null,
      "publication_date": "2018-05-09",
      "venue": "Nature",
      "year": 2018,
      "citation_count": 612,
      "authors": [
        "Andrea Banino",
        "C. Barry",
        "Benigno Uria",
        "C. Blundell",
        "T. Lillicrap",
        "Piotr Wojciech Mirowski",
        "A. Pritzel",
        "M. Chadwick",
        "T. Degris",
        "Joseph Modayil",
        "Greg Wayne",
        "Hubert Soyer",
        "Fabio Viola",
        "Brian Zhang",
        "Ross Goroshin",
        "Neil C. Rabinowitz",
        "Razvan Pascanu",
        "Charlie Beattie",
        "Stig Petersen",
        "Amir Sadik",
        "Stephen Gaffney",
        "Helen King",
        "K. Kavukcuoglu",
        "D. Hassabis",
        "R. Hadsell",
        "D. Kumaran"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "72823437d781996a412428b097b4b562fe846716",
      "title": "Prediction and memory: A predictive coding account",
      "abstract": null,
      "publication_date": "2020-05-21",
      "venue": "Progress in neurobiology",
      "year": 2020,
      "citation_count": 144,
      "authors": [
        "Helen C. Barron",
        "R. Auksztulewicz",
        "Karl J. Friston"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "858549b00245aadc92f91a2540f01398f5f389ae",
      "title": "A solution to the learning dilemma for recurrent networks of spiking neurons",
      "abstract": "Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method\u2013called e-prop\u2013approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence. Bellec et al. present a mathematically founded approximation for gradient descent training of recurrent neural networks without backwards propagation in time. This enables biologically plausible training of spike-based neural network models with working memory and supports on-chip training of neuromorphic hardware.",
      "publication_date": "2019-08-19",
      "venue": "Nature Communications",
      "year": 2019,
      "citation_count": 466,
      "authors": [
        "G. Bellec",
        "Franz Scherr",
        "Anand Subramoney",
        "Elias Hajek",
        "Darjan Salaj",
        "R. Legenstein",
        "W. Maass"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "57070167d647bfa23bc5c5895623af13576db66b",
      "title": "A tutorial on the free-energy framework for modelling perception and learning",
      "abstract": null,
      "publication_date": "2017-02-01",
      "venue": "Journal of Mathematical Psychology",
      "year": 2017,
      "citation_count": 269,
      "authors": [
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "198acc8504895b1ba3211ea3f1e8fe0cf106b753",
      "title": "Impression learning: Online representation learning with synaptic plasticity",
      "abstract": null,
      "publication_date": null,
      "venue": "Neural Information Processing Systems",
      "year": 2021,
      "citation_count": 9,
      "authors": [
        "C. Bredenberg",
        "Benjamin Lyo",
        "Eero P. Simoncelli",
        "Cristina Savin"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "b820ad4a35a6587b44ab03c0e70672a2ed5e9c5f",
      "title": "Accurate Path Integration in Continuous Attractor Network Models of Grid Cells",
      "abstract": "Grid cells in the rat entorhinal cortex display strikingly regular firing responses to the animal's position in 2-D space and have been hypothesized to form the neural substrate for dead-reckoning. However, errors accumulate rapidly when velocity inputs are integrated in existing models of grid cell activity. To produce grid-cell-like responses, these models would require frequent resets triggered by external sensory cues. Such inadequacies, shared by various models, cast doubt on the dead-reckoning potential of the grid cell system. Here we focus on the question of accurate path integration, specifically in continuous attractor models of grid cell activity. We show, in contrast to previous models, that continuous attractor models can generate regular triangular grid responses, based on inputs that encode only the rat's velocity and heading direction. We consider the role of the network boundary in the integration performance of the network and show that both periodic and aperiodic networks are capable of accurate path integration, despite important differences in their attractor manifolds. We quantify the rate at which errors in the velocity integration accumulate as a function of network size and intrinsic noise within the network. With a plausible range of parameters and the inclusion of spike variability, our model networks can accurately integrate velocity inputs over a maximum of \u223c10\u2013100 meters and \u223c1\u201310 minutes. These findings form a proof-of-concept that continuous attractor dynamics may underlie velocity integration in the dorsolateral medial entorhinal cortex. The simulations also generate pertinent upper bounds on the accuracy of integration that may be achieved by continuous attractor dynamics in the grid cell network. We suggest experiments to test the continuous attractor model and differentiate it from models in which single cells establish their responses independently of each other.",
      "publication_date": "2008-11-12",
      "venue": "PLoS Comput. Biol.",
      "year": 2008,
      "citation_count": 703,
      "authors": [
        "Y. Burak",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "fad77fa5f1b84eac354b6d6f7e011259092ce87d",
      "title": "What do grid cells contribute to place cell firing?",
      "abstract": null,
      "publication_date": "2014-03-01",
      "venue": "Trends in Neurosciences",
      "year": 2014,
      "citation_count": 151,
      "authors": [
        "D. Bush",
        "C. Barry",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "554a5385b955e3be4a0621b643b476c0a5514d46",
      "title": "What Does the Anatomical Organization of the Entorhinal Cortex Tell Us?",
      "abstract": "The entorhinal cortex is commonly perceived as a major input and output structure of the hippocampal formation, entertaining the role of the nodal point of cortico-hippocampal circuits. Superficial layers receive convergent cortical information, which is relayed to structures in the hippocampus, and hippocampal output reaches deep layers of entorhinal cortex, that project back to the cortex. The finding of the grid cells in all layers and reports on interactions between deep and superficial layers indicate that this rather simplistic perception may be at fault. Therefore, an integrative approach on the entorhinal cortex, that takes into account recent additions to our knowledge database on entorhinal connectivity, is timely. We argue that layers in entorhinal cortex show different functional characteristics most likely not on the basis of strikingly different inputs or outputs, but much more likely on the basis of differences in intrinsic organization, combined with very specific sets of inputs. Here, we aim to summarize recent anatomical data supporting the notion that the traditional description of the entorhinal cortex as a layered input-output structure for the hippocampal formation does not give the deserved credit to what this structure might be contributing to the overall functions of cortico-hippocampal networks.",
      "publication_date": "2008-08-28",
      "venue": "Journal of Neural Transplantation and Plasticity",
      "year": 2008,
      "citation_count": 407,
      "authors": [
        "Cathrin B. Canto",
        "F. Wouterlood",
        "M. Witter"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a538579ac50d659ac0bca9824d6446e741c586b3",
      "title": "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization",
      "abstract": "Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits.",
      "publication_date": "2018-02-15",
      "venue": "International Conference on Learning Representations",
      "year": 2018,
      "citation_count": 216,
      "authors": [
        "Christopher J. Cueva",
        "Xue-Xin Wei"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2495e0b51367beb828d68abbd00999b4bcdf3c44",
      "title": "Evidence for grid cells in a human memory network",
      "abstract": null,
      "publication_date": "2010-02-04",
      "venue": "Nature",
      "year": 2010,
      "citation_count": 706,
      "authors": [
        "Christian F. Doeller",
        "C. Barry",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2bf3295a0cec7163c063c0d3967cc0935396bb3e",
      "title": "Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis",
      "abstract": "Many recent models study the downstream projection from grid cells to place cells, while recent data have pointed out the importance of the feedback projection. We thus asked how grid cells are affected by the nature of the input from the place cells. We propose a single-layer neural network with feedforward weights connecting place-like input cells to grid cell outputs. Place-to-grid weights are learned via a generalized Hebbian rule. The architecture of this network highly resembles neural networks used to perform Principal Component Analysis (PCA). Both numerical results and analytic considerations indicate that if the components of the feedforward neural network are non-negative, the output converges to a hexagonal lattice. Without the non-negativity constraint, the output converges to a square lattice. Consistent with experiments, grid spacing ratio between the first two consecutive modules is \u22121.4. Our results express a possible linkage between place cell to grid cell interactions and PCA. DOI: http://dx.doi.org/10.7554/eLife.10094.001",
      "publication_date": "2016-03-08",
      "venue": "eLife",
      "year": 2016,
      "citation_count": 147,
      "authors": [
        "Yedidyah Dordek",
        "Daniel Soudry",
        "R. Meir",
        "D. Derdikman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "bc7417738a7f7186ce9b06933072c4ddd39ce4b1",
      "title": "Actionable Neural Representations: Grid Cells from Minimal Constraints",
      "abstract": "To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won't have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an `actionable representation'. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode.",
      "publication_date": "2022-09-30",
      "venue": "International Conference on Learning Representations",
      "year": 2022,
      "citation_count": 23,
      "authors": [
        "W. Dorrell",
        "P. Latham",
        "T. Behrens",
        "James C. R. Whittington"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "28c3c3d07d27b854babfac7b109ba15fca09437c",
      "title": "Evidence for area CA1 as a match/mismatch detector: A high\u2010resolution fMRI study of the human hippocampus",
      "abstract": null,
      "publication_date": "2012-03-01",
      "venue": "Hippocampus",
      "year": 2012,
      "citation_count": 238,
      "authors": [
        "Katherine Duncan",
        "Nicholas A. Ketz",
        "S. Inati",
        "L. Davachi"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8fcbc38e7196b0dc8748f04cd6101e71f92c158e",
      "title": "A theory of cortical responses",
      "abstract": null,
      "publication_date": "2005-04-29",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences",
      "year": 2005,
      "citation_count": 3972,
      "authors": [
        "Karl J. Friston"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "f4649234d52d1848495dcebd9f0cc9042e636bff",
      "title": "A Spin Glass Model of Path Integration in Rat Medial Entorhinal Cortex",
      "abstract": "Electrophysiological recording studies in the dorsocaudal region of medial entorhinal cortex (dMEC) of the rat reveal cells whose spatial firing fields show a remarkably regular hexagonal grid pattern (Fyhn et al., 2004; Hafting et al., 2005). We describe a symmetric, locally connected neural network, or spin glass model, that spontaneously produces a hexagonal grid of activity bumps on a two-dimensional sheet of units. The spatial firing fields of the simulated cells closely resemble those of dMEC cells. A collection of grids with different scales and/or orientations forms a basis set for encoding position. Simulations show that the animal\u2019s location can easily be determined from the population activity pattern. Introducing an asymmetry in the model allows the activity bumps to be shifted in any direction, at a rate proportional to velocity, to achieve path integration. Furthermore, information about the structure of the environment can be superimposed on the spatial position signal by modulation of the bump activity levels without significantly interfering with the hexagonal periodicity of firing fields. Our results support the conjecture of Hafting et al. (2005) that an attractor network in dMEC may be the source of path integration information afferent to hippocampus.",
      "publication_date": "2006-04-19",
      "venue": "Journal of Neuroscience",
      "year": 2006,
      "citation_count": 617,
      "authors": [
        "Mark C. Fuhs",
        "D. Touretzky"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "84c888d94d93137b767ec982c58e6829b10ed88a",
      "title": "Grid cells in mice",
      "abstract": null,
      "publication_date": "2008-12-01",
      "venue": "Hippocampus",
      "year": 2008,
      "citation_count": 174,
      "authors": [
        "M. Fyhn",
        "T. Hafting",
        "M. Witter",
        "E. Moser",
        "M. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "cd98854d3482e276b7124712c02e20ed48f56a0f",
      "title": "A generative model of the hippocampal formation trained with theta driven local learning rules",
      "abstract": "Advances in generative models have recently revolutionised machine learning. Meanwhile, in neuroscience, generative models have long been thought fundamental to animal intelligence. Understanding the biological mechanisms that support these processes promises to shed light on the relationship between biological and artificial intelligence. In animals, the hippocampal formation is thought to learn and use a generative model to support its role in spatial and non-spatial memory. Here we introduce a biologically plausible model of the hippocampal formation tantamount to a Helmholtz machine that we apply to a temporal stream of inputs. A novel component of our model is that fast theta-band oscillations (5-10 Hz) gate the direction of information flow throughout the network, training it akin to a high-frequency wake-sleep algorithm. Our model accurately infers the latent state of high-dimensional sensory environments and generates realistic sensory predictions. Furthermore, it can learn to path integrate by developing a ring attractor connectivity structure matching previous theoretical proposals and flexibly transfer this structure between environments. Whereas many models trade-off biological plausibility with generality, our model captures a variety of hippocampal cognitive functions under one biologically plausible local learning rule.",
      "publication_date": "2023-12-13",
      "venue": "bioRxiv",
      "year": 2023,
      "citation_count": 9,
      "authors": [
        "Tom M George",
        "C. Barry",
        "K. Stachenfeld",
        "C. Clopath",
        "T. Fukai"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "57eb4d347212a9123ff2cfc17839ccd231f6b4b9",
      "title": "Computational Models of Grid Cells",
      "abstract": null,
      "publication_date": "2011-08-25",
      "venue": "Neuron",
      "year": 2011,
      "citation_count": 218,
      "authors": [
        "Lisa M. Giocomo",
        "M. Moser",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "b0004097eac896ee4bf8b45ba3a7d9f101782542",
      "title": "Microstructure of a spatial map in the entorhinal cortex",
      "abstract": null,
      "publication_date": "2005-08-11",
      "venue": "Nature",
      "year": 2005,
      "citation_count": 3671,
      "authors": [
        "T. Hafting",
        "M. Fyhn",
        "S. Molden",
        "M. Moser",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "14a90fb87242fd7033eca54671f9f1f7c09c05e1",
      "title": "Grid cell firing may arise from interference of theta frequency membrane potential oscillations in single neurons",
      "abstract": null,
      "publication_date": "2007-12-01",
      "venue": "Hippocampus",
      "year": 2007,
      "citation_count": 265,
      "authors": [
        "M. Hasselmo",
        "Lisa M. Giocomo",
        "Eric A. Zilli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "252b2d7ab977ed1d1dfc659294318bb2394410d7",
      "title": "Basket-like interneurones in layer II of the entorhinal cortex exhibit a powerful NMDA-mediated synaptic excitation",
      "abstract": null,
      "publication_date": "1993-01-04",
      "venue": "Neuroscience Letters",
      "year": 1993,
      "citation_count": 185,
      "authors": [
        "R. Jones",
        "E. H. B\u00fchl"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1c9852b8cbeb0668cfd5e3aeef01f9ce9a123843",
      "title": "The emergence of grid cells: Intelligent design or just adaptation?",
      "abstract": null,
      "publication_date": "2008-12-01",
      "venue": "Hippocampus",
      "year": 2008,
      "citation_count": 245,
      "authors": [
        "E. Kropff",
        "A. Treves"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6ea99443ff4d599ee67b42a5afd5c7bf6729c2c5",
      "title": "Neural Representations of Location Composed of Spatially Periodic Bands",
      "abstract": null,
      "publication_date": "2012-08-17",
      "venue": "Science",
      "year": 2012,
      "citation_count": 175,
      "authors": [
        "J. Krupic",
        "N. Burgess",
        "J. O\u2019Keefe"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "ae47b8bef62e47285f5acc7d81ea3e265171e2ef",
      "title": "The contributions of entorhinal cortex and hippocampus to error driven learning",
      "abstract": "Computational models proposed that the medial temporal lobe (MTL) contributes importantly to error-driven learning, though little direct in-vivo evidence for this hypothesis exists. To test this, we recorded in the entorhinal cortex (EC) and hippocampus (HPC) as macaques performed an associative learning task using an error-driven learning strategy, defined as better performance after error relative to correct trials. Error-detection signals were more prominent in the EC relative to HPC. Early in learning hippocampal but not EC neurons signaled error-driven learning by increasing their population stimulus-selectivity following error trials. This same pattern was not seen in another task where error-driven learning was not used. After learning, different populations of cells in both the EC and HPC signaled long-term memory of newly learned associations with enhanced stimulus-selective responses. These results suggest prominent but differential contributions of EC and HPC to learning from errors and a particularly important role of the EC in error-detection. Ku et al. recorded in the entorhinal cortex (EC) and hippocampus (HPC) of macaques during associative learning tasks in order to test the computational model prediction that they contribute to error-driven learning. They demonstrate that the EC and HPC have prominent but differential contributions to learning from errors, with the EC having a particularly prominent role in error-detection.",
      "publication_date": "2020-09-29",
      "venue": "Communications Biology",
      "year": 2020,
      "citation_count": 7,
      "authors": [
        "S-P Ku",
        "E. Hargreaves",
        "S. Wirth",
        "W. Suzuki"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3edd190cc7d541f6ae410e5be3d82868532a39f6",
      "title": "Development of the Spatial Representation System in the Rat",
      "abstract": null,
      "publication_date": "2010-06-18",
      "venue": "Science",
      "year": 2010,
      "citation_count": 600,
      "authors": [
        "R. Langston",
        "J. Ainge",
        "J. J. Couey",
        "Cathrin B. Canto",
        "T. Bjerknes",
        "M. Witter",
        "E. Moser",
        "M. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "42ce761c85bdb0d422917b03751ab9cbc72a3417",
      "title": "Backpropagation through time and the brain",
      "abstract": null,
      "publication_date": "2019-03-07",
      "venue": "Current Opinion in Neurobiology",
      "year": 2019,
      "citation_count": 127,
      "authors": [
        "T. Lillicrap",
        "Adam Santoro"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "c04713059cfdbff7f3cce0b10a5fd6fc7cdee43e",
      "title": "Relating Hippocampal Circuitry to Function Recall of Memory Sequences by Reciprocal Dentate\u2013CA3 Interactions",
      "abstract": null,
      "publication_date": "1999-02-01",
      "venue": "Neuron",
      "year": 1999,
      "citation_count": 632,
      "authors": [
        "J. Lisman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "998628588f7850d533a172c883872057a9198d82",
      "title": "Predictive Coding: a Theoretical and Experimental Review",
      "abstract": "Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.",
      "publication_date": "2021-07-27",
      "venue": "arXiv.org",
      "year": 2021,
      "citation_count": 127,
      "authors": [
        "Beren Millidge",
        "A. Seth",
        "C. Buckley"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2c0203ff41fbe8f3fc2a0e706ecb3ecf806f2108",
      "title": "Predictive Coding Approximates Backprop Along Arbitrary Computation Graphs",
      "abstract": "Abstract Backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. Recently it has been shown that backprop in multilayer perceptrons (MLPs) can be approximated using predictive coding, a biologically plausible process theory of cortical computation that relies solely on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs but in the concept of automatic differentiation, which allows for the optimization of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice, rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding convolutional neural networks, recurrent neural networks, and the more complex long short-term memory, which include a nonlayer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks while using only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry and may also contribute to the development of completely distributed neuromorphic architectures.",
      "publication_date": "2020-06-07",
      "venue": "Neural Computation",
      "year": 2020,
      "citation_count": 119,
      "authors": [
        "Beren Millidge",
        "Alexander Tschantz",
        "C. Buckley"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "870d2b9c71c4448498a47870f2622f0a2db44b20",
      "title": "Predictive coding networks for temporal prediction",
      "abstract": "One of the key problems the brain faces is inferring the state of the world from a sequence of dynamically changing stimuli, and it is not yet clear how the sensory system achieves this task. A well-established computational framework for describing perceptual processes in the brain is provided by the theory of predictive coding. Although the original proposals of predictive coding have discussed temporal prediction, later work developing this theory mostly focused on static stimuli, and key questions on neural implementation and computational properties of temporal predictive coding networks remain open. Here, we address these questions and present a formulation of the temporal predictive coding model that can be naturally implemented in recurrent networks, in which activity dynamics rely only on local inputs to the neurons, and learning only utilises local Hebbian plasticity. Additionally, we show that temporal predictive coding networks can approximate the performance of the Kalman filter in predicting behaviour of linear systems, and behave as a variant of a Kalman filter which does not track its own subjective posterior variance. Importantly, temporal predictive coding networks can achieve similar accuracy as the Kalman filter without performing complex mathematical operations, but just employing simple computations that can be implemented by biological networks. Moreover, when trained with natural dynamic inputs, we found that temporal predictive coding can produce Gabor-like, motion-sensitive receptive fields resembling those observed in real neurons in visual areas. In addition, we demonstrate how the model can be effectively generalized to nonlinear systems. Overall, models presented in this paper show how biologically plausible circuits can predict future stimuli and may guide research on understanding specific neural circuits in brain areas involved in temporal prediction. Author summary While significant advances have been made in the neuroscience of how the brain processes static stimuli, the time dimension has often been relatively neglected. However, time is crucial since the stimuli perceived by our senses typically dynamically vary in time, and the cortex needs to make sense of these changing inputs. This paper describes a computational model of cortical networks processing temporal stimuli. This model is able to infer and track the state of the environment based on noisy inputs, and predict future sensory stimuli. By ensuring that these predictions match the incoming stimuli, the model is able to learn the structure and statistics of its temporal inputs and produces responses of neurons resembling those in the brain. The model may help in further understanding neural circuits in sensory cortical areas.",
      "publication_date": "2024-03-09",
      "venue": "bioRxiv",
      "year": 2024,
      "citation_count": 20,
      "authors": [
        "Beren Millidge",
        "Mufeng Tang",
        "Mahyar Osanlouy",
        "N. Harper",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "0c48611df1f03ca7cec7f98a1ffaadf95dec1260",
      "title": "Grids from bands, or bands from grids? An examination of the effects of single unit contamination on grid cell firing fields.",
      "abstract": "Neural recording technology is improving rapidly, allowing for the detection of spikes from hundreds of cells simultaneously. The limiting step in multielectrode electrophysiology continues to be single cell isolation. However, this step is crucial to the interpretation of data from putative single neurons. We present here, in simulation, an illustration of possibly erroneous conclusions that may be reached when poorly isolated single cell data are analyzed. Grid cells are neurons recorded in rodents, and bats, that spike in equally spaced locations in a hexagonal pattern. One theory states that grid firing patterns arise from a combination of band firing patterns. However, we show here that summing the grid firing patterns of two poorly resolved neurons can result in spurious band-like patterns. Thus, evidence of neurons spiking in band patterns must undergo extreme scrutiny before it is accepted. Toward this aim, we discuss single cell isolation methods and metrics.",
      "publication_date": "2016-02-01",
      "venue": "Journal of Neurophysiology",
      "year": 2016,
      "citation_count": 21,
      "authors": [
        "Zaneta Navratilova",
        "Zaneta Navratilova",
        "Keith B. Godfrey",
        "B. McNaughton",
        "B. McNaughton"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6a77b5266b492c69dec62163cd692f86b9b72860",
      "title": "Emergent elasticity in the neural code for space",
      "abstract": "Significance We develop a theoretical model, grounded in known properties of neural dynamics and synaptic plasticity, that can fuse information gathered from the past history of velocity and sequence of encountered landmarks during exploratory behavior, to construct a self-consistent internal representation of space. Moreover, through model reduction techniques, we obtain conceptual insights into how consistent internal spatial representations naturally emerge through an elastic relaxation process in an effective spring\u2013particle system. We verify several experimentally testable predictions of our model involving the spatial behavior of grid cells in the medial entorhinal cortex, as well as suggest additional experiments. Upon encountering a novel environment, an animal must construct a consistent environmental map, as well as an internal estimate of its position within that map, by combining information from two distinct sources: self-motion cues and sensory landmark cues. How do known aspects of neural circuit dynamics and synaptic plasticity conspire to accomplish this feat? Here we show analytically how a neural attractor model that combines path integration of self-motion cues with Hebbian plasticity in synaptic weights from landmark cells can self-organize a consistent map of space as the animal explores an environment. Intriguingly, the emergence of this map can be understood as an elastic relaxation process between landmark cells mediated by the attractor network. Moreover, our model makes several experimentally testable predictions, including (i) systematic path-dependent shifts in the firing fields of grid cells toward the most recently encountered landmark, even in a fully learned environment; (ii) systematic deformations in the firing fields of grid cells in irregular environments, akin to elastic deformations of solids forced into irregular containers; and (iii) the creation of topological defects in grid cell firing patterns through specific environmental manipulations. Taken together, our results conceptually link known aspects of neurons and synapses to an emergent solution of a fundamental computational problem in navigation, while providing a unified account of disparate experimental observations.",
      "publication_date": "2018-05-21",
      "venue": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": 2018,
      "citation_count": 70,
      "authors": [
        "Samuel A. Ocko",
        "Kiah Hardcastle",
        "Lisa M. Giocomo",
        "S. Ganguli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6aa68970ec381a78a75fc30555884d4e8f1cb672",
      "title": "Place units in the hippocampus of the freely moving rat",
      "abstract": null,
      "publication_date": "1976-12-31",
      "venue": "Experimental Neurology",
      "year": 1976,
      "citation_count": 1807,
      "authors": [
        "J. O\u2019Keefe"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "de76cd6c61199445e2688402cabbb3119dadc877",
      "title": "Dual phase and rate coding in hippocampal place cells: Theoretical significance and relationship to entorhinal grid cells",
      "abstract": null,
      "publication_date": null,
      "venue": "Hippocampus",
      "year": 2005,
      "citation_count": 521,
      "authors": [
        "J. O\u2019Keefe",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
      "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
      "abstract": null,
      "publication_date": "1996-06-13",
      "venue": "Nature",
      "year": 1996,
      "citation_count": 6169,
      "authors": [
        "B. Olshausen",
        "D. Field"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "c1585013b1dbb7341a8a6821159d8d7681e81503",
      "title": "Predictive grid coding in the medial entorhinal cortex",
      "abstract": "The entorhinal cortex represents allocentric spatial geometry and egocentric speed and heading information required for spatial navigation. However, it remains unclear whether it contributes to the prediction of an animal\u2019s future location. We discovered grid cells in the medial entorhinal cortex (MEC) that have grid fields representing future locations during goal-directed behavior. These predictive grid cells represented prospective spatial information by shifting their grid fields against the direction of travel. Predictive grid cells discharged at the trough phases of the hippocampal CA1 theta oscillation and, together with other types of grid cells, organized sequences of the trajectory from the current to future positions across each theta cycle. Our results suggest that the MEC provides a predictive map that supports forward planning in spatial navigation. Editor\u2019s summary Grid cells in the entorhinal cortex create a coordinate system of the environment for spatial navigation. However, it is not clear whether the entorhinal grid system is also involved in predicting where an animal will be in the next moment. Ouchi and Fujisawa performed high-density neuronal recordings in the entorhinal cortex and hippocampal area CA1 of rats during goal-directed behavior in an open field. They observed neurons in layer 3 of the medial entorhinal cortex that explicitly encode a grid representation of an animal\u2019s future projected location, not the current position, and named them \u201c\u201cpredictive grid cells.\u201d Neuronal assemblies in the medial entorhinal cortex thus organize a predictive cognitive map. \u2014Peter Stern",
      "publication_date": "2024-08-16",
      "venue": "Science",
      "year": 2024,
      "citation_count": 7,
      "authors": [
        "Ayako Ouchi",
        "Shigeyoshi Fujisawa"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a424ec3b8846f57b8ffdb566d272e28d5a525909",
      "title": "Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects.",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 1999,
      "citation_count": 4448,
      "authors": [
        "Rajesh P. N. Rao",
        "D. Ballard"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "abb5aa3ff57e043bbaf59239ab27150a2545aee5",
      "title": "Associative Memories via Predictive Coding",
      "abstract": "Associative memories in the brain receive and store patterns of activity registered by the sensory neurons, and are able to retrieve them when necessary. Due to their importance in human intelligence, computational models of associative memories have been developed for several decades now. In this paper, we present a novel neural model for realizing associative memories, which is based on a hierarchical generative network that receives external stimuli via sensory neurons. It is trained using predictive coding, an error-based learning algorithm inspired by information processing in the cortex. To test the model's capabilities, we perform multiple retrieval experiments from both corrupted and incomplete data points. In an extensive comparison, we show that this new model outperforms in retrieval accuracy and robustness popular associative memory models, such as autoencoders trained via backpropagation, and modern Hopfield networks. In particular, in completing partial data points, our model achieves remarkable results on natural image datasets, such as ImageNet, with a surprisingly high accuracy, even when only a tiny fraction of pixels of the original images is presented. Our model provides a plausible framework to study learning and retrieval of memories in the brain, as it closely mimics the behavior of the hippocampus as a memory index and generative model.",
      "publication_date": "2021-09-16",
      "venue": "Neural Information Processing Systems",
      "year": 2021,
      "citation_count": 64,
      "authors": [
        "Tommaso Salvatori",
        "Yuhang Song",
        "Yujian Hong",
        "Simon Frieder",
        "Lei Sha",
        "Zhenghua Xu",
        "R. Bogacz",
        "Thomas Lukasiewicz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "e83816c0e7ed01ee48d579568ba97db72931d979",
      "title": "Learning on Arbitrary Graph Topologies via Predictive Coding",
      "abstract": "Training with backpropagation (BP) in standard deep learning consists of two main steps: a forward pass that maps a data point to its prediction, and a backward pass that propagates the error of this prediction back through the network. This process is highly effective when the goal is to minimize a specific objective function. However, it does not allow training on networks with cyclic or backward connections. This is an obstacle to reaching brain-like capabilities, as the highly complex heterarchical structure of the neural connections in the neocortex are potentially fundamental for its effectiveness. In this paper, we show how predictive coding (PC), a theory of information processing in the cortex, can be used to perform inference and learning on arbitrary graph topologies. We experimentally show how this formulation, called PC graphs, can be used to flexibly perform different tasks with the same network by simply stimulating specific neurons. This enables the model to be queried on stimuli with different structures, such as partial images, images with labels, or images without labels. We conclude by investigating how the topology of the graph influences the final performance, and comparing against simple baselines trained with BP.",
      "publication_date": "2022-01-31",
      "venue": "Neural Information Processing Systems",
      "year": 2022,
      "citation_count": 34,
      "authors": [
        "Tommaso Salvatori",
        "Luca Pinchetti",
        "Beren Millidge",
        "Yuhang Song",
        "R. Bogacz",
        "Thomas Lukasiewicz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "709b4bfc5198336ba5d70da987889a157f695c1e",
      "title": "Optimal unsupervised learning in a single-layer linear feedforward neural network",
      "abstract": null,
      "publication_date": null,
      "venue": "Neural Networks",
      "year": 1989,
      "citation_count": 1603,
      "authors": [
        "T. Sanger"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "4b5daaa4edd3caadfb1fa214f44140877828051f",
      "title": "Conjunctive Representation of Position, Direction, and Velocity in Entorhinal Cortex",
      "abstract": null,
      "publication_date": "2006-05-05",
      "venue": "Science",
      "year": 2006,
      "citation_count": 609,
      "authors": [
        "Francesca Sargolini",
        "M. Fyhn",
        "T. Hafting",
        "Bruce L. McNaughton",
        "M. Witter",
        "M. Moser",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "25b422756cbddb3193002c9a49c844641fff2127",
      "title": "No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit",
      "abstract": "Research in Neuroscience, as in many scientific disciplines, is undergoing a renaissance based on deep learning. Unique to Neuroscience, deep learning models can be used not only as a tool but interpreted as models of the brain. The central claims of recent deep learning-based models of brain circuits are that they make novel predictions about neural phenomena or shed light on the fundamental functions being optimized. We show, through the case-study of grid cells in the entorhinal-hippocampal circuit, that one may get neither. We begin by reviewing the principles of grid cell mechanism and function obtained from first-principles modeling efforts, then rigorously examine the claims of deep learning models of grid cells. Using large-scale architectural and hyperparameter sweeps and theory-driven experimentation, we demonstrate that the results of such models may be more strongly driven by particular, non-fundamental, and post-hoc implementation choices than fundamental truths about neural circuits or the loss function(s) they might optimize. We discuss why these models cannot be expected to produce accurate models of the brain without the addition of substantial amounts of inductive bias, an informal No Free Lunch result for Neuroscience. Based on first principles work, we provide hypotheses for what additional loss functions will produce grid cells more robustly. In conclusion, circumspection and transparency, together with biological knowledge, are warranted in building and interpreting deep learning models in Neuroscience.",
      "publication_date": "2023-08-15",
      "venue": "bioRxiv",
      "year": 2023,
      "citation_count": 57,
      "authors": [
        "Rylan Schaeffer",
        "Mikail Khona",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "27bc20a95969cffa29065fcc1beb63ee2386b852",
      "title": "Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells",
      "abstract": "To solve the spatial problems of mapping, localization and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learnt this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, there is no satisfactory synthesis of core principles that lead to multi-modular grid cells in deep recurrent neural networks. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: coding theory, dynamical systems, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, without access to supervised position information or engineering of particular readout representations as needed in previous approaches. We show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks and emergent representations generalize well outside their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.",
      "publication_date": "2023-11-04",
      "venue": "Neural Information Processing Systems",
      "year": 2023,
      "citation_count": 20,
      "authors": [
        "Rylan Schaeffer",
        "Mikail Khona",
        "Tzuhsuan Ma",
        "Cristobal Eyzaguirre",
        "Sanmi Koyejo",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "f24588ae91d1015ef9e85004624db6434448403a",
      "title": "Biological Softmax: Demonstrated in Modern Hopfield Networks",
      "abstract": null,
      "publication_date": null,
      "venue": "Annual Meeting of the Cognitive Science Society",
      "year": 2022,
      "citation_count": 3,
      "authors": [
        "Mallory A. Snow",
        "Jeff Orchard"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "35ce44e06802d9f778eb02bf03a6fc5aa657bd56",
      "title": "Inferring neural activity before plasticity as a foundation for learning beyond backpropagation",
      "abstract": "This paper introduces \u2018prospective configuration\u2019, a new principle for learning in neural networks, which differs from backpropagation and is more efficient in learning and more consistent with data on neural activity and behavior. For both humans and machines, the essence of learning is to pinpoint which components in its information processing pipeline are responsible for an error in its output, a challenge that is known as \u2018credit assignment\u2019. It has long been assumed that credit assignment is best solved by backpropagation, which is also the foundation of modern machine learning. Here, we set out a fundamentally different principle on credit assignment called \u2018prospective configuration\u2019. In prospective configuration, the network first infers the pattern of neural activity that should result from learning, and then the synaptic weights are modified to consolidate the change in neural activity. We demonstrate that this distinct mechanism, in contrast to backpropagation, (1) underlies learning in a well-established family of models of cortical circuits, (2) enables learning that is more efficient and effective in many contexts faced by biological organisms and (3) reproduces surprising patterns of neural activity and behavior observed in diverse human and rat learning experiments.",
      "publication_date": "2024-01-03",
      "venue": "Nature Neuroscience",
      "year": 2024,
      "citation_count": 16,
      "authors": [
        "Yuhang Song",
        "Beren Millidge",
        "Tommaso Salvatori",
        "Thomas Lukasiewicz",
        "Zhenghua Xu",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "791686d4fa91082e225797a13060adeeb0fddf13",
      "title": "A unified theory for the computational and mechanistic origins of grid cells",
      "abstract": "The discovery of entorhinal grid cells has generated considerable interest in how and why hexagonal firing fields might mechanistically emerge in a generic manner from neural circuits, and what their computational significance might be. Here we forge an intimate link between the computational problem of path-integration and the existence of hexagonal grids, by demonstrating that such grids arise generically in biologically plausible neural networks trained to path integrate. Moreover, we develop a unifying theory for why hexagonal grids are so ubiquitous in path-integrator circuits. Such trained networks also yield powerful mechanistic hypotheses, exhibiting realistic levels of biological variability not captured by hand-designed models. We furthermore develop methods to analyze the connectome and activity maps of our trained networks to elucidate fundamental mechanisms underlying path integration. These methods provide an instructive roadmap to go from connectomic and physiological measurements to conceptual understanding in a manner that might be generalizable to other settings.",
      "publication_date": "2020-12-30",
      "venue": "Neuron",
      "year": 2020,
      "citation_count": 83,
      "authors": [
        "Ben Sorscher",
        "Gabriel C. Mel",
        "Samuel A. Ocko",
        "Lisa M. Giocomo",
        "S. Ganguli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1fe70364ca7c7cf3b5008969518fd3257e62a232",
      "title": "The hippocampus as a predictive map",
      "abstract": "A cognitive map has long been the dominant metaphor for hippocampal function, embracing the idea that place cells encode a geometric representation of space. However, evidence for predictive coding, reward sensitivity and policy dependence in place cells suggests that the representation is not purely spatial. We approach this puzzle from a reinforcement learning perspective: what kind of spatial representation is most useful for maximizing future reward? We show that the answer takes the form of a predictive representation. This representation captures many aspects of place cell responses that fall outside the traditional view of a cognitive map. Furthermore, we argue that entorhinal grid cells encode a low-dimensionality basis set for the predictive representation, useful for suppressing noise in predictions and extracting multiscale structure for hierarchical planning.",
      "publication_date": "2017-06-07",
      "venue": "Nature Neuroscience",
      "year": 2017,
      "citation_count": 757,
      "authors": [
        "Kimberly L. Stachenfeld",
        "M. Botvinick",
        "S. Gershman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "26fc224a118e20e5d6bc98b57711001abc20ec00",
      "title": "Recurrent predictive coding models for associative memory employing covariance learning",
      "abstract": "The computational principles adopted by the hippocampus in associative memory (AM) tasks have been one of the mostly studied topics in computational and theoretical neuroscience. Classical models of the hippocampal network assume that AM is performed via a form of covariance learning, where associations between memorized items are represented by entries in the learned covariance matrix encoded in the recurrent connections in the hippocampal subfield CA3. On the other hand, it has been recently proposed that AM in the hippocampus is achieved through predictive coding. Hierarchical predictive coding models following this theory perform AM, but fail to capture the recurrent hippocampal structure that encodes the covariance in the classical models. Such a dichotomy pose potential difficulties for developing a unitary theory of how memory is formed and recalled in the hippocampus. Earlier predictive coding models that learn the covariance information of inputs explicitly seem to be a solution to this dichotomy. Here, we show that although these models can perform AM, they do it in an implausible and numerically unstable way. Instead, we propose alternatives to these earlier covariance-learning predictive coding networks, which learn the covariance information implicitly and plausibly, and can use dendritic structures to encode prediction errors. We show analytically that our proposed models are perfectly equivalent to the earlier predictive coding model learning covariance explicitly, and encounter no numerical issues when performing AM tasks in practice. We further show that our models can be combined with hierarchical predictive coding networks to model the hippocampo-neocortical interactions. Our models provide a biologically plausible approach to modelling the hippocampal network, pointing to a potential computational mechanism employed by the hippocampus during memory formation and recall, which unifies predictive coding and covariance learning based on the recurrent network structure. Author summary The hippocampus and adjacent cortical areas have long been considered essential for the formation of associative memories. Earlier theoretical works have assumed that the hippocampus stores in its recurrent connections statistical regularities embedded in the sensory inputs. On the other hand, it has been recently suggested that the hippocampus retrieves memory by generating predictions of ongoing sensory inputs. Computational models have thus been proposed to account for this predictive nature of the hippocampal network using predictive coding, a general theory of information processing in the cortex. However, these hierarchical predictive coding models of the hippocampus did not describe how it stores the statistical regularities that play a key role for associative memory in the classical hippocampal models, hindering a unified understanding of the underlying computational principles employed by the hippocampus. To address this dichotomy, here we present a family of predictive coding models that also learn the statistical information needed for associative memory. Our models can stably perform associative memory tasks in a biologically plausible manner, even with large structured data such as natural scenes. Our work provides a possible mechanism of how the recurrent hippocampal network may employ various computational principles concurrently to perform associative memory.",
      "publication_date": "2022-11-09",
      "venue": "bioRxiv",
      "year": 2022,
      "citation_count": 25,
      "authors": [
        "Mufeng Tang",
        "Tommaso Salvatori",
        "Beren Millidge",
        "Yuhang Song",
        "Thomas Lukasiewicz",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8a8f7953894fef09d32c5f35fd79cfbff025e56f",
      "title": "Sequential Memory with Temporal Predictive Coding",
      "abstract": "Forming accurate memory of sequential stimuli is a fundamental function of biological agents. However, the computational mechanism underlying sequential memory in the brain remains unclear. Inspired by neuroscience theories and recent successes in applying predictive coding (PC) to static memory tasks, in this work we propose a novel PC-based model for sequential memory, called temporal predictive coding (tPC). We show that our tPC models can memorize and retrieve sequential inputs accurately with a biologically plausible neural implementation. Importantly, our analytical study reveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN) with an implicit statistical whitening process, which leads to more stable performance in sequential memory tasks of structured inputs. Moreover, we find that tPC exhibits properties consistent with behavioral observations and theories in neuroscience, thereby strengthening its biological relevance. Our work establishes a possible computational mechanism underlying sequential memory in the brain that can also be theoretically interpreted using existing memory model frameworks.",
      "publication_date": "2023-05-19",
      "venue": "Neural Information Processing Systems",
      "year": 2023,
      "citation_count": 19,
      "authors": [
        "Mufeng Tang",
        "Helen C. Barron",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "138a75cab26100df70bdb66dd1a28e68612db8ff",
      "title": "Learning place cells, grid cells and invariances with excitatory and inhibitory plasticity",
      "abstract": "Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction, and the underlying circuit mechanisms are not yet resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, place cells are typically invariant to head direction. We propose that all observed spatial tuning patterns \u2013 in both their selectivity and their invariance \u2013 arise from the same mechanism: Excitatory and inhibitory synaptic plasticity driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. Our proposed model is robust to changes in parameters, develops patterns on behavioral timescales and makes distinctive experimental predictions.",
      "publication_date": "2018-02-21",
      "venue": "eLife",
      "year": 2018,
      "citation_count": 50,
      "authors": [
        "Simon Nikolaus Weber",
        "Henning Sprekeler"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "c124a6aec4b1833e4e86092e20a782183349d57e",
      "title": "An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity",
      "abstract": "Abstract To efficiently learn from feedback, cortical networks need to update synaptic weights on multiple levels of cortical hierarchy. An effective and well-known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm. However, in this algorithm, the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons. Several models have been proposed that approximate the backpropagation algorithm with local synaptic plasticity, but these models require complex external control over the network or relatively complex plasticity rules. Here we show that a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple local Hebbian plasticity. Furthermore, for certain parameters, the weight change in the predictive coding model converges to that of the backpropagation algorithm. This suggests that it is possible for cortical networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output.",
      "publication_date": "2017-03-23",
      "venue": "Neural Computation",
      "year": 2017,
      "citation_count": 284,
      "authors": [
        "James C. R. Whittington",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "f509100a8b6ec4036798fe857ea7ca75572b8278",
      "title": "The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation",
      "abstract": null,
      "publication_date": "2019-09-16",
      "venue": "Cell",
      "year": 2019,
      "citation_count": 447,
      "authors": [
        "James C. R. Whittington",
        "Timothy H. Muller",
        "Shirley Mark",
        "Guifen Chen",
        "C. Barry",
        "N. Burgess",
        "T. Behrens"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "bf2f79d0f4accdf9b42dd7be901b77c6bd5c88ed",
      "title": "A Model of Grid Cell Development through Spatial Exploration and Spike Time-Dependent Plasticity",
      "abstract": null,
      "publication_date": "2014-07-16",
      "venue": "Neuron",
      "year": 2014,
      "citation_count": 93,
      "authors": [
        "John Widloski",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2ae5a5507253aa3cada113d41d35fada1e84555f",
      "title": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories",
      "abstract": null,
      "publication_date": "1990-12-01",
      "venue": "Neural Computation",
      "year": 1990,
      "citation_count": 718,
      "authors": [
        "Ronald J. Williams",
        "Jing Peng"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "4935965339f2ae6990cc72721a2bed33c6c4121a",
      "title": "Development of the Hippocampal Cognitive Map in Preweanling Rats",
      "abstract": null,
      "publication_date": "2010-06-18",
      "venue": "Science",
      "year": 2010,
      "citation_count": 565,
      "authors": [
        "Thomas J. Wills",
        "F. Cacucci",
        "N. Burgess",
        "J. O\u2019Keefe"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a8c9ad94386d3a08e11cf653bca6da51ce8d1d79",
      "title": "Passive Transport Disrupts Grid Signals in the Parahippocampal Cortex",
      "abstract": null,
      "publication_date": "2015-10-05",
      "venue": "Current Biology",
      "year": 2015,
      "citation_count": 75,
      "authors": [
        "Shawn S. Winter",
        "Max L Mehlman",
        "B. J. Clark",
        "J. Taube"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d157e8ada2d3cd7c634f08e293f6b4ce0bc13e34",
      "title": "Trial Outcome and Associative Learning Signals in the Monkey Hippocampus",
      "abstract": null,
      "publication_date": "2009-03-01",
      "venue": "Neuron",
      "year": 2009,
      "citation_count": 73,
      "authors": [
        "S. Wirth",
        "E. Av\u015far",
        "Cindy C. Chiu",
        "V. Sharma",
        "Anne C. Smith",
        "E. Brown",
        "W. Suzuki"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2e108d408a92abbbfa3cabffc3c8a3b0a62c6004",
      "title": "Spatial representation and the architecture of the entorhinal cortex",
      "abstract": null,
      "publication_date": "2006-12-01",
      "venue": "Trends in Neurosciences",
      "year": 2006,
      "citation_count": 228,
      "authors": [
        "M. Witter",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d7c42c3a58f05ba4c1aaf3754691ff6fb44f43ba",
      "title": "Grid cells without theta oscillations in the entorhinal cortex of bats",
      "abstract": null,
      "publication_date": "2011-11-03",
      "venue": "Nature",
      "year": 2011,
      "citation_count": 383,
      "authors": [
        "M. Yartsev",
        "M. Witter",
        "N. Ulanovsky"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "b9ea31c9c8719a504dd349feffbd79c7c8d18e23",
      "title": "Coupled Noisy Spiking Neurons as Velocity-Controlled Oscillators in a Model of Grid Cell Spatial Firing",
      "abstract": "One of the two primary classes of models of grid cell spatial firing uses interference between oscillators at dynamically modulated frequencies. Generally, these models are presented in terms of idealized oscillators (modeled as sinusoids), which differ from biological oscillators in multiple important ways. Here we show that two more realistic, noisy neural models (Izhikevich's simple model and a biophysical model of an entorhinal cortex stellate cell) can be successfully used as oscillators in a model of this type. When additive noise is included in the models such that uncoupled or sparsely coupled cells show realistic interspike interval variance, both synaptic and gap-junction coupling can synchronize networks of cells to produce comparatively less variable network-level oscillations. We show that the frequency of these oscillatory networks can be controlled sufficiently well to produce stable grid cell spatial firing on the order of at least 2\u20135 min, despite the high noise level. Our results suggest that the basic principles of oscillatory interference models work with more realistic models of noisy neurons. Nevertheless, a number of simplifications were still made and future work should examine increasingly realistic models.",
      "publication_date": "2010-10-13",
      "venue": "Journal of Neuroscience",
      "year": 2010,
      "citation_count": 108,
      "authors": [
        "Eric A. Zilli",
        "M. Hasselmo"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "b227f3e4c0dc96e5ac5426b85485a70f2175a205",
      "title": "Representation Learning with Contrastive Predictive Coding",
      "abstract": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
      "publication_date": "2018-07-10",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "66c286df54551baba7351a1ed44019367e5aa7ea",
      "title": "Evidence of a predictive coding hierarchy in the human brain listening to speech",
      "abstract": "Current machine learning language algorithms make adjacent word-level predictions. In this work, Caucheteux et al. show that the human brain probably uses long-range and hierarchical predictions, taking into account up to eight possible words into the future. Considerable progress has recently been made in natural language processing: deep learning algorithms are increasingly able to generate, summarize, translate and classify texts. Yet, these language models still fail to match the language abilities of humans. Predictive coding theory offers a tentative explanation to this discrepancy: while language models are optimized to predict nearby words, the human brain would continuously predict a hierarchy of representations that spans multiple timescales. To test this hypothesis, we analysed the functional magnetic resonance imaging brain signals of 304 participants listening to short stories. First, we confirmed that the activations of modern language models linearly map onto the brain responses to speech. Second, we showed that enhancing these algorithms with predictions that span multiple timescales improves this brain mapping. Finally, we showed that these predictions are organized hierarchically: frontoparietal cortices predict higher-level, longer-range and more contextual representations than temporal cortices. Overall, these results strengthen the role of hierarchical predictive coding in language processing and illustrate how the synergy between neuroscience and artificial intelligence can unravel the computational bases of human cognition.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1cae417456711c4da184f5efcd1b7464a7a0661a",
      "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding",
      "abstract": "Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.",
      "publication_date": "2019-05-22",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "da741827ab6f5bf9762d30b91fd2409ac9e0e272",
      "title": "World models and predictive coding for cognitive and developmental robotics: frontiers and challenges",
      "abstract": "ABSTRACT Creating autonomous robots that can actively explore the environment, acquire knowledge and learn skills continuously is the ultimate achievement envisioned in cognitive and developmental robotics. Importantly, if the aim is to create robots that can continuously develop through interactions with their environment, their learning processes should be based on interactions with their physical and social world in the manner of human learning and cognitive development. Based on this context, in this paper, we focus on the two concepts of world models and predictive coding. Recently, world models have attracted renewed attention as a topic of considerable interest in artificial intelligence. Cognitive systems learn world models to better predict future sensory observations and optimize their policies, i.e. controllers. Alternatively, in neuroscience, predictive coding proposes that the brain continuously predicts its inputs and adapts to model its own dynamics and control behavior in its environment. Both ideas may be considered as underpinning the cognitive development of robots and humans capable of continual or lifelong learning. Although many studies have been conducted on predictive coding in cognitive robotics and neurorobotics, the relationship between world model-based approaches in AI and predictive coding in robotics has rarely been discussed. Therefore, in this paper, we clarify the definitions, relationships, and status of current research on these topics, as well as missing pieces of world models and predictive coding in conjunction with crucially related concepts such as the free-energy principle and active inference in the context of cognitive and developmental robotics. Furthermore, we outline the frontiers and challenges involved in world models and predictive coding toward the further integration of AI and robotics, as well as the creation of robots with real cognitive and developmental capabilities in the future. GRAPHICAL ABSTRACT",
      "publication_date": "2023-01-14",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "36ec1bf8a3a21321b7887caaeee9c6ee6cfc0db8",
      "title": "A predictive coding model of the N400",
      "abstract": "The N400 event-related component has been widely used to investigate the neural mechanisms underlying real-time language comprehension. However, despite decades of research, there is still no unifying theory that can explain both its temporal dynamics and functional properties. In this work, we show that predictive coding \u2013 a biologically plausible algorithm for approximating Bayesian inference \u2013 offers a promising framework for characterizing the N400. Using an implemented predictive coding computational model, we demonstrate how the N400 can be formalized as the lexico-semantic prediction error produced as the brain infers meaning from linguistic form of incoming words. We show that the magnitude of lexico-semantic prediction error mirrors the functional sensitivity of the N400 to various lexical variables, priming, contextual effects, as well as their higher-order interactions. We further show that the dynamics of the predictive coding algorithm provide a natural explanation for the temporal dynamics of the N400, and a biologically plausible link to neural activity. Together, these findings directly situate the N400 within the broader context of predictive coding research, and suggest that the brain may use the same computational mechanism for inference across linguistic and non-linguistic domains.",
      "publication_date": "2023-04-11",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9f6eb784ffc460cba56077b4b8739d4eb9115f18",
      "title": "StockFormer: Learning Hybrid Trading Machines with Predictive Coding",
      "abstract": "Typical RL-for-finance solutions directly optimize trading policies over the noisy market data, such as stock prices and trading volumes, without explicitly considering the future trends and correlations of different investment assets as we humans do. In this paper, we present StockFormer, a hybrid trading machine that integrates the forward modeling capabilities of predictive coding with the advantages of RL agents in policy flexibility. The predictive coding part consists of three Transformer branches with modified structures, which respectively extract effective latent states of long-/short-term future dynamics and asset relations. The RL agent adaptively fuses these states and then executes an actor-critic algorithm in the unified state space. The entire model is jointly trained by propagating the critic's gradients back to the predictive coding module. StockFormer significantly outperforms existing approaches across three publicly available financial datasets in terms of portfolio returns and Sharpe ratios.",
      "publication_date": "2023-08-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9480a64d9520f4b8857ed291fd42db11e46594a7",
      "title": "Contrastive Difference Predictive Coding",
      "abstract": "Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \\times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20 \\times$ more sample efficient than the successor representation and $1500 \\times$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.",
      "publication_date": "2023-10-31",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "202c79bbb45ab6524141feacc81caacc4ba00401",
      "title": "Memory-augmented Dense Predictive Coding for Video Representation Learning",
      "abstract": "The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.",
      "publication_date": "2020-08-03",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ad367b44f3434b9ba6b46b41ab083210f6827a9f",
      "title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning",
      "abstract": "While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (\"PredNet\") architecture that is inspired by the concept of \"predictive coding\" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.",
      "publication_date": "2016-05-25",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "46e2af3d7e8dd50675b0062fe9e4fce5cd5a0508",
      "title": "Predictive and Adaptive Deep Coding for Wireless Image Transmission in Semantic Communication",
      "abstract": "Semantic communication is a newly emerged communication paradigm that exploits deep learning (DL) models to realize communication processes like source coding and channel coding. Recent advances have demonstrated that DL-based joint source-channel coding (DeepJSCC) can achieve exciting data compression and noise-resiliency performances for wireless image transmission tasks, especially in environments with low channel signal-to-noises (SNRs). However, existing DeepJSCC-based semantic communication frameworks still cannot achieve adaptive code rates for different channel SNRs and image contents, which reduces its flexibility and bandwidth efficiency. In this paper, we propose a predictive and adaptive deep coding (PADC) framework for realizing flexible code rate optimization with a given target transmission quality requirement. PADC is realized by a variable code length enabled DeepJSCC (DeepJSCC-V) model for realizing flexible code length adjustment, an Oracle Network (OraNet) model for predicting peak-signal-to-noise (PSNR) value for an image transmission task according to its contents, channel signal to noise ratio (SNR) and the compression ratio (CR) value, and a CR optimizer aims at finding the minimal data-level or instance-level CR with a PSNR quality constraint. By using the above three modules, PADC can transmit the image data with minimal CR, which greatly increases bandwidth efficiency. Simulation results demonstrate that the proposed DeepJSCC-V model can achieve similar PSNR performances compared with the state-of-the-art Attention-based DeepJSCC (ADJSCC) model, and the proposed OraNet model is able to predict high-quality PSNR values with an average error lower than 0.5dB. Results also demonstrate that the proposed PADC can use nearly minimal bandwidth consumption for wireless image transmission tasks with different channel SNR and image contents, at the same time guaranteeing the PSNR constraint for each image data.",
      "publication_date": "2023-08-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "23e04389f8728a5736382d3662341a1a2a25e171",
      "title": "Predictive Coding or Just Feature Discovery? An Alternative Account of Why Language Models Fit Brain Data",
      "abstract": "Abstract Many recent studies have shown that representations drawn from neural network language models are extremely effective at predicting brain responses to natural language. But why do these models work so well? One proposed explanation is that language models and brains are similar because they have the same objective: to predict upcoming words before they are perceived. This explanation is attractive because it lends support to the popular theory of predictive coding. We provide several analyses that cast doubt on this claim. First, we show that the ability to predict future words does not uniquely (or even best) explain why some representations are a better match to the brain than others. Second, we show that within a language model, representations that are best at predicting future words are strictly worse brain models than other representations. Finally, we argue in favor of an alternative explanation for the success of language models in neuroscience: These models are effective at predicting brain responses because they generally capture a wide variety of linguistic phenomena.",
      "publication_date": "2022-11-03",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6bcdf260d7927d8fe4ff030c20ee1db974d0c969",
      "title": "Predictive Coding: Towards a Future of Deep Learning beyond Backpropagation?",
      "abstract": "The backpropagation of error algorithm (BP) used to train deep neural networks has been fundamental to the successes of deep learning. However, it requires sequential backwards updates and non-local computations which make it challenging to parallelize at scale and is unlike how learning works in the brain. Neuroscience-inspired learning algorithms, however, such as \\emph{predictive coding} which utilize local learning have the potential to overcome these limitations and advance beyond deep learning technologies in the future. While predictive coding originated in theoretical neuroscience as a model of information processing in the cortex, recent work has developed the idea into a general-purpose algorithm able to train neural networks using only local computations. In this survey, we review works that have contributed to this perspective and demonstrate the close connection between predictive coding and backpropagation in terms of generalization quality, as well as works that highlight the multiple advantages of using predictive coding models over backprop-trained neural networks. Specifically, we show the substantially greater flexibility of predictive coding networks against equivalent deep neural networks, which can function as classifiers, generators, and associative memories simultaneously, and can be defined on arbitrary graph topologies. Finally, we review direct benchmarks of predictive coding networks on machine learning classification tasks, as well as its close connections to control theory and applications in robotics.",
      "publication_date": "2022-02-18",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "34a9b0c7ddc9012b8657c4bc1de5f14d45e646b4",
      "title": "Hybrid predictive coding: Inferring, fast and slow",
      "abstract": "Predictive coding is an influential model of cortical neural activity. It proposes that perceptual beliefs are furnished by sequentially minimising \u201cprediction errors\u201d\u2014the differences between predicted and observed data. Implicit in this proposal is the idea that successful perception requires multiple cycles of neural activity. This is at odds with evidence that several aspects of visual perception\u2014including complex forms of object recognition\u2014arise from an initial \u201cfeedforward sweep\u201d that occurs on fast timescales which preclude substantial recurrent activity. Here, we propose that the feedforward sweep can be understood as performing amortized inference (applying a learned function that maps directly from data to beliefs) and recurrent processing can be understood as performing iterative inference (sequentially updating neural activity in order to improve the accuracy of beliefs). We propose a hybrid predictive coding network that combines both iterative and amortized inference in a principled manner by describing both in terms of a dual optimization of a single objective function. We show that the resulting scheme can be implemented in a biologically plausible neural architecture that approximates Bayesian inference utilising local Hebbian update rules. We demonstrate that our hybrid predictive coding model combines the benefits of both amortized and iterative inference\u2014obtaining rapid and computationally cheap perceptual inference for familiar data while maintaining the context-sensitivity, precision, and sample efficiency of iterative inference schemes. Moreover, we show how our model is inherently sensitive to its uncertainty and adaptively balances iterative and amortized inference to obtain accurate beliefs using minimum computational expense. Hybrid predictive coding offers a new perspective on the functional relevance of the feedforward and recurrent activity observed during visual perception and offers novel insights into distinct aspects of visual phenomenology.",
      "publication_date": "2022-04-05",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0174d263d3a77bf03fce831a9a5ce2678e1959f0",
      "title": "Video Representation Learning by Dense Predictive Coding",
      "abstract": "The objective of this paper is self-supervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We make three contributions: First, we introduce the Dense Predictive Coding (DPC) framework for self-supervised representation learning on videos. This learns a dense encoding of spatio-temporal blocks by recurrently predicting future representations; Second, we propose a curriculum training scheme to predict further into the future with progressively less temporal context. This encourages the model to only encode slowly varying spatial-temporal signals, therefore leading to semantic representations; Third, we evaluate the approach by first training the DPC model on the Kinetics-400 dataset with self-supervised learning, and then finetuning the representation on a downstream task, i.e. action recognition. With single stream (RGB only), DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet.",
      "publication_date": "2019-09-10",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5374052df7a9a69499586b5acfc433f8a8e20e66",
      "title": "On Conformal Isometry of Grid Cells: Learning Distance-Preserving Position Embedding",
      "abstract": "This paper investigates the conformal isometry hypothesis as a potential explanation for the hexagonal periodic patterns in grid cell response maps. We posit that grid cell activities form a high-dimensional vector in neural space, encoding the agent's position in 2D physical space. As the agent moves, this vector rotates within a 2D manifold in the neural space, driven by a recurrent neural network. The conformal hypothesis proposes that this neural manifold is a conformal isometric embedding of 2D physical space, where local physical distance is preserved by the embedding up to a scaling factor (or unit of metric). Such distance-preserving position embedding is indispensable for path planning in navigation, especially planning local straight path segments. We conduct numerical experiments to show that this hypothesis leads to the hexagonal grid firing patterns by learning maximally distance-preserving position embedding, agnostic to the choice of the recurrent neural network. Furthermore, we present a theoretical explanation of why hexagon periodic patterns emerge by minimizing our loss function by showing that hexagon flat torus is maximally distance preserving.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e96016d5e3c6492230aa172efcf733596dc64b6e",
      "title": "Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells",
      "abstract": "Unsupervised text encoding models have recently fueled substantial progress in Natural Language Processing (NLP). The key idea is to use neural networks to convert words in texts to vector space representations (embeddings) based on word positions in a sentence and their contexts. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as Points of Interest (POIs) into models. A general space encoding method is valuable for a multitude of tasks such asPOI search, land use classification, point-based spatial interpolation and locationaware image classification. However, no such general model exists to date beyond simply applying discretizing or feed forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for encoding space and are critical for recognizing places and for path-integration. Inspired by this research, wepropose a representation learning model called Space2vec to encode the absolutepositions and spatial relationships of places. We conduct experiments on realworld geographic data and predict types of POIs at given positions based on their1) locations and 2) nearby POIs. Results show that because of its multi-scale representations Space2vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed forward nets, and tile embedding approaches.",
      "publication_date": "2020-02-16",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2510c7bcbd6278b50dc73e02d127546ef70141bd",
      "title": "Learning an efficient place cell map from grid cells using non-negative sparse coding",
      "abstract": "Experimental studies of grid cells in the Medial Entorhinal Cortex (MEC) have shown that they are selective to an array of spatial locations in the environment that form a hexagonal grid. However, in a small environment, place cells in the hippocampus are only selective to a single-location of the environment while granule cells in the dentate gyrus of the hippocampus have multiple discrete firing locations, but lack spatial periodicity. Given the anatomical connection from MEC to the hippocampus, previous feedforward models of grid-to-place have been proposed. Here, we propose a unified learning model that can describe the spatial tuning properties of both hippocampal place cells and dentate gyrus granule cells based on non-negative sparse coding. Sparse coding plays an important role in many cortical areas and is proposed here to have a key role in the navigational system of the brain in the hippocampus. Our results show that the hexagonal patterns of grid cells with various orientations, grid spacings and phases are necessary for model cells to learn a single spatial field that efficiently tile the entire spatial environment. However, if there is a lack of diversity in any grid parameters or a lack of cells in the network, this will lead to the emergence of place cells that have multiple firing locations. More surprisingly, the model shows that place cells can also emerge even when non-negative sparse coding is used with weakly-tuned MEC cells, instead of MEC grid cells, as the input to place cells. This work suggests that sparse coding may be one of the underlying organizing principles for the navigational system of the brain.",
      "publication_date": "2020-08-13",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2abc9d603a87b300a251a3796a12b8a2d21746df",
      "title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion",
      "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multiplication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.",
      "publication_date": "2018-09-27",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d09c79f6b7420c8c8a28ecab0836ec9bacb8f8cf",
      "title": "A non-spatial account of place and grid cells based on clustering models of concept learning",
      "abstract": "One view is that conceptual knowledge is organized using the circuitry in the medial temporal lobe (MTL) that supports spatial processing and navigation. In contrast, we find that a domain-general learning algorithm explains key findings in both spatial and conceptual domains. When the clustering model is applied to spatial navigation tasks, so-called place and grid cell-like representations emerge because of the relatively uniform distribution of possible inputs in these tasks. The same mechanism applied to conceptual tasks, where the overall space can be higher-dimensional and sampling sparser, leading to representations more aligned with human conceptual knowledge. Although the types of memory supported by the MTL are superficially dissimilar, the information processing steps appear shared. Our account suggests that the MTL uses a general-purpose algorithm to learn and organize context-relevant information in a useful format, rather than relying on navigation-specific neural circuitry. Spatial maps in the medial temporal lobe (MTL) have been proposed to map abstract conceptual knowledge. Rather than grounding abstract knowledge in a spatial map, the authors propose a general-purpose clustering algorithm that explains how both spatial (including place and grid cells) and higher-dimensional conceptual representations arise during learning.",
      "publication_date": "2018-09-19",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4af94a163a695f14597ac101a9562a68772010ae",
      "title": "Probabilistic Learning by Rodent Grid Cells",
      "abstract": "Mounting evidence shows mammalian brains are probabilistic computers, but the specific cells involved remain elusive. Parallel research suggests that grid cells of the mammalian hippocampal formation are fundamental to spatial cognition but their diverse response properties still defy explanation. No plausible model exists which explains stable grids in darkness for twenty minutes or longer, despite being one of the first results ever published on grid cells. Similarly, no current explanation can tie together grid fragmentation and grid rescaling, which show very different forms of flexibility in grid responses when the environment is varied. Other properties such as attractor dynamics and grid anisotropy seem to be at odds with one another unless additional properties are assumed such as a varying velocity gain. Modelling efforts have largely ignored the breadth of response patterns, while also failing to account for the disastrous effects of sensory noise during spatial learning and recall, especially in darkness. Here, published electrophysiological evidence from a range of experiments are reinterpreted using a novel probabilistic learning model, which shows that grid cell responses are accurately predicted by a probabilistic learning process. Diverse response properties of probabilistic grid cells are statistically indistinguishable from rat grid cells across key manipulations. A simple coherent set of probabilistic computations explains stable grid fields in darkness, partial grid rescaling in resized arenas, low-dimensional attractor grid cell dynamics, and grid fragmentation in hairpin mazes. The same computations also reconcile oscillatory dynamics at the single cell level with attractor dynamics at the cell ensemble level. Additionally, a clear functional role for boundary cells is proposed for spatial learning. These findings provide a parsimonious and unified explanation of grid cell function, and implicate grid cells as an accessible neuronal population readout of a set of probabilistic spatial computations.",
      "publication_date": "2016-10-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b8ab26cb57b8dcc19019ccf36db682c2a4eedce8",
      "title": "Towards Emergence of Grid Cells by Deep Reinforcement Learning",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b629916af0db6f56f3b318f5043c81b374271fa5",
      "title": "Coordinated Learning of Entorhinal Grid Cells and Hippocampal Place Cells: Space, Time, Attention and Oscillations",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "20d6f4241af85a9c4fe5f0fcb583c818441282c4",
      "title": "Learning place cells, grid cells and invariances: A unifying model",
      "abstract": "Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction. The underlying circuit mechanisms are not fully resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, a place cell is highly selective to location, but typically invariant to head direction. Here, we propose that all observed spatial tuning patterns \u2013 in both their selectivity and their invariance \u2013 are a consequence of the same mechanism: Excitatory and inhibitory synaptic plasticity that is driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. The model is robust to changes in parameters, develops patterns on behavioral time scales and makes distinctive experimental predictions. Our results suggest that the interaction of excitatory and inhibitory plasticity is a general principle for the formation of neural representations.",
      "publication_date": "2017-02-17",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "917922f63a9920b940c05fa802deb3d7d83dd6b0",
      "title": "Using Convolutional Neural Networks to Build a Lightweight Flood Height Prediction Model with Grad-Cam for the Selection of Key Grid Cells in Radar Echo Maps",
      "abstract": "Recent climate change has brought extremely heavy rains and widescale flooding to many areas around the globe. However, previous flood prediction methods usually require a lot of computation to obtain the prediction results and impose a heavy burden on the unit cost of the prediction. This paper proposes the use of a deep learning model (DLM) to overcome these problems. We alleviated the high computational overhead of this approach by developing a novel framework for the construction of lightweight DLMs. The proposed scheme involves training a convolutional neural network (CNN) by using a radar echo map in conjunction with historical flood records at target sites and using Grad-Cam to extract key grid cells from these maps (representing regions with the greatest impact on flooding) for use as inputs in another DLM. Finally, we used real radar echo maps of five locations and the flood heights record to verify the validity of the method proposed in this paper. The experimental results show that our proposed lightweight model can achieve similar or even better prediction accuracy at all locations with only about 5~15% of the operation time and about 30~35% of the memory space of the CNN.",
      "publication_date": "2022-01-07",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "27d6a46db40f93d4ca0659e19125a7ac5396d0da",
      "title": "Identifying key grid cells for crowd flow predictions based on CNN-based models with the Grad-CAM kit",
      "abstract": null,
      "publication_date": "2022-10-08",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c2228c6e59fdb4e63f2e873077f1690e16fa39f8",
      "title": "Dynamic Occupancy Grid Map with Semantic Information Using Deep Learning-Based BEVFusion Method with Camera and LiDAR Fusion",
      "abstract": "In the field of robotics and autonomous driving, dynamic occupancy grid maps (DOGMs) are typically used to represent the position and velocity information of objects. Although three-dimensional light detection and ranging (LiDAR) sensor-based DOGMs have been actively researched, they have limitations, as they cannot classify types of objects. Therefore, in this study, a deep learning-based camera\u2013LiDAR sensor fusion technique is employed as input to DOGMs. Consequently, not only the position and velocity information of objects but also their class information can be updated, expanding the application areas of DOGMs. Moreover, unclassified LiDAR point measurements contribute to the formation of a map of the surrounding environment, improving the reliability of perception by registering objects that were not classified by deep learning. To achieve this, we developed update rules on the basis of the Dempster\u2013Shafer evidence theory, incorporating class information and the uncertainty of objects occupying grid cells. Furthermore, we analyzed the accuracy of the velocity estimation using two update models. One assigns the occupancy probability only to the edges of the oriented bounding box, whereas the other assigns the occupancy probability to the entire area of the box. The performance of the developed perception technique is evaluated using the public nuScenes dataset. The developed DOGM with object class information will help autonomous vehicles to navigate in complex urban driving environments by providing them with rich information, such as the class and velocity of nearby obstacles.",
      "publication_date": "2024-04-29",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0a292908d132abc2bf5fc5413d8c121600b5288e",
      "title": "A Computational Model of Grid Cells based on Dendritic Self-organized Learning",
      "abstract": "In this paper we present a new computational model for grid cells. These cells are neurons in the entorhinal cortex of the hippocampal region that encode allocentric spatial information. They possess a peculiar, triangular firing pattern that spans the entire environment with a virtual lattice. We show that such a firing pattern can emerge from a dendritic, self-organized learning process. A key aspect of the proposed model is the hypothesis that the dendritic tree of a grid cell can behave like a sparse self organizing map that tries to cover its input space as best as possible. We argue, that the encoding scheme used by grid cells is possibly not limited to the description of spatial information and may represent a general principle on how complex information is encoded in higher level brain areas like the hippocampal region.",
      "publication_date": null,
      "venue": "",
      "year": 2013,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "7a795b581de305191235367b71d65ab8038d0f73",
      "title": "A unified theory for the origin of grid cells through the lens of pattern formation",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5be0bfaa322272f3c70a203bcc0aef87a6a6dc13",
      "title": "A Brain-Inspired Adaptive Space Representation Model Based on Grid Cells and Place Cells",
      "abstract": "Grid cells and place cells are important neurons in the animal brain. The information transmission between them provides the basis for the spatial representation and navigation of animals and also provides reference for the research on the autonomous navigation mechanism of intelligent agents. Grid cells are important information source of place cells. The supervised learning and unsupervised learning models can be used to simulate the generation of place cells from grid cell inputs. However, the existing models preset the firing characteristics of grid cell. In this paper, we propose a united generation model of grid cells and place cells. First, the visual place cells with nonuniform distribution generate the visual grid cells with regional firing field through feedforward network. Second, the visual grid cells and the self-motion information generate the united grid cells whose firing fields extend to the whole space through genetic algorithm. Finally, the visual place cells and the united grid cells generate the united place cells with uniform distribution through supervised fuzzy adaptive resonance theory (ART) network. Simulation results show that this model has stronger environmental adaptability and can provide reference for the research on spatial representation model and brain-inspired navigation mechanism of intelligent agents under the condition of nonuniform environmental information.",
      "publication_date": "2020-08-11",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "cbeb0b647964650757d5e25aa711916e12a896dc",
      "title": "Grid cell firing patterns maintain their hexagonal firing patterns on a circular track",
      "abstract": "In an open two-dimensional environment, grid cells in the medial entorhinal cortex are known to be active in multiple locations, displaying a striking periodic hexagonal firing pattern covering the entire space. Both modeling and experimental data suggest that such periodic spatial representations may emerge from a continuous attractor network. According to this theory, grid cell activity in any stable 1D environment is a slice through an underlying 2D hexagonal pattern, which is supported by some experimental studies but challenged by others. Grid cells are believed to play a fundamental role in path integration, and so understanding their behavior in various environments is crucial for understanding the flow of information through the entorhinal-hippocampal system. To this end, we analyzed the activity of grid cells when rats traversed a circular track. A previous study involving this data set analyzed individual grid cell activity patterns separately, but we found that individual grid cells do not provide sufficient data for determining the under-lying spatial activity pattern. To circumvent this, we compute the population autocorrelation, which pools together population responses from all grid cells within the same module. This novel approach recovers the underlying six-peak hexagonal pattern that was not observable in the individual autocorrelations. We also use the population autocorrelation to infer the spacing and orientation of the population lattice, revealing how the lattice differs across environments. Furthermore, the population autocorrelation of the linearized track reveals that at the level of the population, grid cells have an allocentric code for space. These results are strong support for the attractor network theory for grid cells, and our novel approach can be used to analyze grid cell activity in any undersampled environment.",
      "publication_date": "2023-09-15",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "48fa2392ceb51aa5fc57766d513efe1fa9bf3de0",
      "title": "Spatial periodicity in grid cell firing is explained by a neural sequence code of 2-D trajectories",
      "abstract": "Spatial periodicity in grid cell firing has been interpreted as a neural metric for space providing animals with a coordinate system in navigating physical and mental spaces. However, the specific computational problem being solved by grid cells has remained elusive. Here, we provide mathematical proof that spatial periodicity in grid cell firing is the only possible solution to a neural sequence code of 2-D trajectories and that the hexagonal firing pattern of grid cells is the most parsimonious solution to such a sequence code. We thereby provide a likely teleological cause for the existence of grid cells and reveal the underlying nature of the global geometric organization in grid maps as a direct consequence of a simple local sequence code. A sequence code by grid cells provides intuitive explanations for many previously puzzling experimental observations and may transform our thinking about grid cells.",
      "publication_date": "2023-05-30",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "affe48f3661f2e4d1869c992a5f9aed3e849c050",
      "title": "Spatial periodicity in grid cell firing is explained by a neural sequence code of 2D trajectories",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5b66541e9cc6708be8f70405b6f51b1c41fa34dc",
      "title": "Features of formation of the celsian phase during firing of heat-resistant ceramics in the system BaO\u2013Al2O3\u2013SiO2",
      "abstract": "In the synthesis of heat-resistant ceramics in the system BaO\u2013Al2O3\u2013SiO2 (BAS), it is important to find ways of intensifying the process of transition of hexagonal celsian to monoclinic celsian without an increase in sintering temperature. Monoclinic form of celsian is characterized by higher thermal, electro-insulating, and mechanical properties. This paper deals with the features of formation of the phase composition of celsian ceramics when using BAS glass of eutectic composition and glass in the system Li2O\u2013Al2O3\u2013B2O3\u2013SiO2 (LABS) of spodumene composition as modifying components. It is shown that monoclinic celsian is the final crystalline phase formed in ceramics synthesized on the basis of barium carbonate and kaolin. Monoclinic celsian is formed stepwise; and the hexagonal celsian appears first. The complete transition hexagonal celsian\uf0aemonoclinic celsian occurs only in the process of high temperature firing at 12500C. Notably, the degree of ceramic sintering remains low (water absorption is 11.0%). Introduction of BAS glass contributes to the complete transition of hexagonal celsian to monoclinic celsian at a reduced temperature of 11000C. Maximum effect in the formation of monoclinic celsian is achieved by the introduction of LABS glass. As a result, the temperature of formation of this modification maximally decreases to 8000C. In this case, complete sintering of celsian ceramics is achieved at the temperature of 12500\u0421.",
      "publication_date": "2022-06-01",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "de629192ca5b0b7195355149dad23bd4124bf3df",
      "title": "Irregular distribution of grid cell firing fields in rats exploring a 3D volumetric space",
      "abstract": "We investigated how entorhinal grid cells encode volumetric space. On a horizontal surface, grid cells usually produce multiple, spatially focal, approximately circular firing fields that are evenly sized and spaced to form a regular, close-packed, hexagonal array. This spatial regularity has been suggested to underlie navigational computations. In three dimensions, theoretically the equivalent firing pattern would be a regular, hexagonal close packing of evenly sized spherical fields. In the present study, we report that, in rats foraging in a cubic lattice, grid cells maintained normal temporal firing characteristics and produced spatially stable firing fields. However, although most grid fields were ellipsoid, they were sparser, larger, more variably sized and irregularly arranged, even when only fields abutting the lower surface (equivalent to the floor) were considered. Thus, grid self-organization is shaped by the environment\u2019s structure and/or movement affordances, and grids may not need to be regular to support spatial computations. Grieves et al. show that when rats explore a 3D space, grid cells in the entorhinal cortex exchange their usual spatially regular firing patterns for more irregular ones, suggesting that 3D space is mapped differently than previously thought.",
      "publication_date": "2021-08-11",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "03915b784afbe730a33d5803600bde7c497c7cc1",
      "title": "Hexagonal grid \ufb01elds optimally",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ee73107de55b07d563246ced858ba561de1d80f9",
      "title": "Low-Temperature Firing of Substituted M-Type Hexagonal Ferrites for Multilayer Inductors",
      "abstract": null,
      "publication_date": "2012-03-22",
      "venue": "",
      "year": 2012,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "79be3dcc0cd53613e5386425e387b2664c9c9ab0",
      "title": "Untethered firing fields and intermittent silences: Why grid\u2010cell discharge is so variable",
      "abstract": "Grid cells in medial entorhinal cortex are notoriously variable in their responses, despite the striking hexagonal arrangement of their spatial firing fields. Indeed, when the animal moves through a firing field, grid cells often fire much more vigorously than predicted or do not fire at all. The source of this trial\u2010to\u2010trial variability is not completely understood. By analyzing grid\u2010cell spike trains from mice running in open arenas and on linear tracks, we characterize the phenomenon of \u201cmissed\u201d firing fields using the statistical theory of zero inflation. We find that one major cause of grid\u2010cell variability lies in the spatial representation itself: firing fields are not as strongly anchored to spatial location as the averaged grid suggests. In addition, grid fields from different cells drift together from trial to trial, regardless of whether the environment is real or virtual, or whether the animal moves in light or darkness. Spatial realignment across trials sharpens the grid representation, yielding firing fields that are more pronounced and significantly narrower. These findings indicate that ensembles of grid cells encode relative position more reliably than absolute position.",
      "publication_date": "2020-02-11",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3bc48861d66b2ee2968ed26c0d96c9b2e1ab06e8",
      "title": "Path integration maintains spatial periodicity of grid cell firing in a 1D circular track",
      "abstract": "Entorhinal grid cells are thought to provide a 2D spatial metric of the environment. In this study we demonstrate that in a familiar 1D circular track (i.e., a continuous space) grid cells display a novel 1D equidistant firing pattern based on integrated distance rather than travelled distance or time. In addition, field spacing is increased compared to a 2D open field, probably due to a reduced access to the visual cue in the track. This metrical modification is accompanied by a change in LFP theta oscillations, but no change in intrinsic grid cell rhythmicity, or firing activity of entorhinal speed and head-direction cells. These results suggest that in a 1D circular space grid cell spatial selectivity is shaped by path integration processes, while grid scale relies on external information. In an open field, the preferential firing of grid cells on a hexagonal lattice is formed by integrating external as well as self-motion cues. Here, the authors show that on a 1D circular track, path integration cues shape the spatial selectivity of grid cells while external cues determine the scale of the grid.",
      "publication_date": "2019-02-19",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ddf9626f10f4a5fd38e2332f1720c3f505783632",
      "title": "Influence of Firing Atmosphere on the Cubic-Hexagonal Transition and the Chemical State of Titanium in BaTiO3",
      "abstract": "The effect of reducing gas mixture, H2-N2 or CO-N2, on the cubic-hexagonal transition in BaTiO3 has been investigated by X-ray diffraction, ESR, ESCA, PAS and chemical analysis. The firing temperatures of BaTiO3 were 1380\u00b0C and 1500\u00b0C, and the transition temperature in air is already known to be 1460\u00b0C. The fractional conversion to the hexagonal phase increased with increasing concentrations of reducing gases, larger for samples fired in H2-N2 than those fired in CO-N2 for the same concentrations of the reducing gases. The hexagonal phase in all samples increased proportionally to the amount of Ti3+ produced by reduction of Ti4+ ions, regardless of the reducing gas species and firing temperatures. It was found that the minimum amount of Ti3+ ions to stabilize the hexagonal BaTiO3 at room temperature was 0.3% of total Ti ions. ESCA spectra due to Ti3+ were observed in the sample fired in 100% H2 at 1380\u00b0C.",
      "publication_date": null,
      "venue": "",
      "year": 1987,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "efa178733034990962a2379ce972a229f61da1cd",
      "title": "Co-firing behavior of ZnTiO3\u2013TiO2 dielectrics/hexagonal ferrite composites for multi-layer LC filters",
      "abstract": null,
      "publication_date": "2003-05-25",
      "venue": "",
      "year": 2003,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4a5f9b6232a0ac08c09ef473c419e691295e8a23",
      "title": "Effect of Firing Atmosphere on the Cubic-Hexagonal Transition in Ba0.99Sr0.01TiO3",
      "abstract": "Ba0.99Sr0.01Tio3 ceramics were fired for 1 hour at a temperature from 1410\u00b0 to 1460\u00b0C in H2-N2 atmospheres. A pure hexagonal phase, which was not formed by calcination in air, was obtained by firing for 1 hour at 1430\u00b0C in H2 atmosphere. A part of the oxygen in Ba0.99Sr0.01TiO3 was removed under the reducing condition, which led to the formation of the hexagonal phase. The amount of the hexagonal phase of Ba0.99Sr0.01TiO3-x increased proportionally with an increase in oxygen deficiency (x). The value of x required for stabilizing the hexagonal phase at room temperature was found to be larger than 0.008. The transition temperature from cubic to hexagonal phase of Ba0.99Sr0.01TiO3-x was higher than that of BaTiO3-x, and oxygen deficiency required for stabilizing the hexagonal phase was larger than that for BaTiO3-x. These results indicated that Ba0.99Sr0.01TiO3-x does not easily transformed into the hexagonal phase.",
      "publication_date": "1990-08-01",
      "venue": "",
      "year": 1990,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "93ea481402dcf2ea344baa9d1fe51a58b28b9b17",
      "title": "Co-Firing Behavior of Co2Z Hexagonal Ferrite/Ag Internal Electrode for MLCIs",
      "abstract": null,
      "publication_date": "2004-12-01",
      "venue": "",
      "year": 2004,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "099c02c7a4c3c403eb5525673fcdd3ce5e4bd5c8",
      "title": "AgCrO2 formation mechanism during silver inner electrode and Fe\u2013Si\u2013Cr alloy powder co-firing in metal multilayer chip power inductors",
      "abstract": null,
      "publication_date": null,
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "aa8ab6a377cdcc5f0614b8f8beb7f267782319ac",
      "title": "Control of hexagonal plate-like microstructure of in-situ calcium hexaluminate in monolithic refractories",
      "abstract": "ABSTRACT The self-toughening property of calcium hexaluminate (CaO\u20226Al2O3, CA6) in engineering ceramics causes a remarkable interest in the in-situ CA6 in monolithic refractory. The determination of significant factors which controls the formation and microstructure of CA6 in monolithic refractory is mainly discussed in this study. Samples were prepared by using calcium aluminate cement and sintered alumina and the chemical composition was fixed at the molar ratio of CaO: Al2O3 = 1:6. In order to evaluate the suitable sintering condition and SiO2 content for CA6 phase formation, the variation of firing temperature and holding time was first modified at 1400\u20131500\u00b0C for 1\u20135 h. The second factor was done by adding 2\u201310 mass% of SiO2 into the primary mixture. In comparison, the results were confirmed that high firing temperature at 1500\u00b0C provided the highest quantity of CA6 phase and plate-like microstructure. In addition, longer proceeding time contributed to the grain growth of CA6, especially, within 5 h of holding time. SiO2 importantly helped to enhance hexagonal plate-like microstructure due to the ability of ion mobility in low-melting phase of gehlenite but the suitable content of SiO2 should not exceed 2 mass% for the better control ability of CA6 formation and microstructure.",
      "publication_date": "2018-07-03",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "838c48b300fd1be40014c97edd79233b1fb52f69",
      "title": "Hippocampal Spike-Timing Correlations Lead to Hexagonal Grid Fields.",
      "abstract": "Space is represented in the mammalian brain by the activity of hippocampal place cells, as well as in their spike-timing correlations. Here, we propose a theory for how this temporal code is transformed to spatial firing rate patterns via spike-timing-dependent synaptic plasticity. The resulting dynamics of synaptic weights resembles well-known pattern formation models in which a lateral inhibition mechanism gives rise to a Turing instability. We identify parameter regimes in which hexagonal firing patterns develop as they have been found in medial entorhinal cortex.",
      "publication_date": "2017-06-21",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d40cb3f74f7e4a0858bc63760ec983f15de8125c",
      "title": "Grid-Cell Activity on Linear Tracks Indicates Purely Translational Remapping of 2D Firing Patterns at Movement Turning Points",
      "abstract": "Grid cells in rodent medial entorhinal cortex are thought to play a critical role for spatial navigation. When the animal is freely moving in an open arena the firing fields of each grid cell tend to form a hexagonal lattice spanning the environment. For movements along a linear track the cells seem to respond differently. They show multiple firing fields that are not periodically arranged and whose shape and position change when the running direction is reversed. In addition, peak firing rates vary widely from field to field. Measured along one running direction only, firing fields are, however, compatible with a slice through a two-dimensional (2D) hexagonal pattern. It is an open question, whether this is also true if leftward and rightward runs are jointly considered. By analyzing data from 15 male Long\u2013Evans rats, we show that a single hexagonal firing pattern explains the linear-track data if translational shifts of the pattern are allowed at the movement turning points. A rotation or scaling of the grid is not required. The agreement is further improved if the peak firing rates of the underlying 2D grid fields can vary from field to field, as suggested by recent studies. These findings have direct consequences for experiments using linear tracks in virtual reality. SIGNIFICANCE STATEMENT Various types of neurons support spatial navigation. Their response properties are often studied in reduced settings and might change when the animal can freely explore its environment. Grid cells in rodents, for example, exhibit seemingly irregular firing fields when animal movement is restricted to a linear track but highly regular patterns in two-dimensional (2D) arenas. We show that linear-track responses of a cell for both leftward and rightward running directions can be explained as cuts through a single hexagonal pattern if translational remapping is allowed at movement turning points; neither rotations nor scale transformations are needed. These results provide a basis to quantify grid-cell activity in 1D virtual reality and could help to detect and categorize grid cells without experiments in 2D environments.",
      "publication_date": "2018-07-05",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "115806ae388f43e1e25062ce88d2415c64c580dc",
      "title": "Grid cell firing fields in a volumetric space",
      "abstract": "We investigated how entorhinal grid cells represent volumetric (three-dimensional) space. On a flat surface, grid cell firing fields are circular and arranged in a close-packed hexagonal array. In three dimensions, theoretical and computational work suggests that the most efficient configuration would be a regular close packing of spherical fields. We report that in rats exploring a cubic lattice, grid cells were spatially stable and maintained normal directional modulation, theta modulation and spike dynamics. However, while the majority of grid fields were spherical, they were irregularly arranged, even when only fields abutting the lower surface (equivalent to the floor) were considered. Thus, grid organization is shaped by the environment\u2019s movement affordances, and may not need to be regular to support spatial computations. One Sentence Summary In rats exploring a volumetric space, grid cells are spatially modulated but their firing fields are irregularly arranged.",
      "publication_date": "2020-12-07",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "62f16717f8359d414513cc77ccdd363b508dae35",
      "title": "Brain-Inspired Spatial Representation Based on Grid Cells with Finite-Spacing Firing Field",
      "abstract": "Grid cell is a kind of important neuron cell related to spatial cognition and navigation in animal brain. It has a hexagonal firing field extending to the whole space. Grid cells exist in the form of modules, whose number is finite, and the grid cells between adjacent modules have discrete firing- field spacing with constant ratio. The existing models can simulate the constant ratio discrete of firing-field spacing between modules, but the finite characteristic of firing-field spacing cannot be simulated. We propose a recurrent attractor network model of grid cell, which can generate grid cells with finite firing-field spacing by analyzing the connections between grid cells in a single network. Then, the grid cells in different modules could possess discrete firing-field spacing with constant ratio through the competition among the recurrent networks. The recurrent attractor network model can provide reference for the construction of brain-inspired navigation system of unmanned platform.",
      "publication_date": "2019-11-01",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "7662551500dbdea149bbb44181db7592ad93c0f7",
      "title": "Cage-rotor induction motor inter-turn short circuit fault detection with and without saturation effect by MEC model.",
      "abstract": null,
      "publication_date": "2016-09-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "78c169ad1636ec9eeea8c02e5c26452272727ede",
      "title": "MEC: An Open-source Fine-grained Mapping Equivalence Checking Tool for FPGA",
      "abstract": "Technology mapping is an essential step in EDA flow. However, the function of the circuit may be changed after technology mapping, and equivalence checking (EC) based verification is highly necessary. The traditional EC method has significant time and resource constraints, making it only feasible to carry out at a coarse-grained level. To make it efficient for technology mapping, we propose a fine-grained method called MEC, which leverages a combination of two approaches to significantly reduce the time cost of verification. The local block verification approach performs fast verification and the global graph cover approach guarantees correctness. The proposed method is rigorously tested and compared to three EC tools, and the results show that MEC technique offers a substantial improvement in speed. MEC not only offers a faster and more efficient way of performing EC on technology mapping but also opens up new opportunities for more fine-grained verification in the future.",
      "publication_date": "2023-05-08",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "569043747e04038c86b73c28f3d269d01e31d7ff",
      "title": "Magnetic Equivalent Circuit and Lagrange Interpolation Function Modeling of Induction Machines Under Broken Bar Faults",
      "abstract": "This article introduces a mesh-based magnetic equivalent circuit (MEC) modeling technique for induction machines (IMs) in healthy and broken rotor bars conditions. The MEC model is presented as a highly accurate and computationally efficient alternative to finite element (FE) models. By incorporating modifications to the air gap coupling method, including a new Lagrange interpolation function, and utilizing a harmonic MEC model, the accuracy of the solution is improved while reducing electrical and mechanical transients. Compared to experiments and 2-D FE models, this model achieves precise results for electromagnetic torque, rotational speed, and forces across various conditions. The Lagrange interpolation function forms the basis for the air gap coupling between stator and rotor flux densities. The results demonstrate the MEC model\u2019s exceptional accuracy in predicting speed oscillations, calculating forces, and analyzing current harmonics in faulty IMs. Furthermore, the MEC model performs over 30 times faster than the 2-D FE models.",
      "publication_date": "2024-03-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2ddc7239d99b4b3c5c5407bb62c47b6d48ad8161",
      "title": "Armature Reaction Analysis and Suppression of Voice Coil Actuator Based on an Improved Magnetic Equivalent Circuit Model",
      "abstract": "The high thrust density voice coil actuator (VCA) is a key component for the deployers to determine the CubeSats\u2019 separation velocity in orbit directly. To improve the VCA's thrust density and power consumption, it is essential to analyze the armature reaction under heavy current work conditions. This article proposes a dynamic splitting method of the magnetic equivalent circuit (MEC) model for analyzing the armature reaction of the VCA. This MEC model can effectively reveal the interaction mechanism between the dynamic winding field and the permanent magnet (PM) field, which provides an approach to readily and quickly analyze the armature reaction. Based on the understanding of the mechanism, we found an effective way to suppress the armature reaction and propose a new VCA with double PM flux circuits. The electromagnetic performances of the new configuration are evaluated by comparing it with the conventional one. Both the theoretical and experimental results show that the new structure achieves lower magnetic saturation, lower armature reaction, and higher thrust density. A research prototype was constructed and experimentally tested to verify the theoretical results of the armature reaction, magnetic field, thrust, separation velocity, and other performances. The new VCA realizes a wide and accurate modulation of separation velocity for the CubeSats.",
      "publication_date": "2024-07-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9c1b89701dbf1bb6094536248c0948949012c63c",
      "title": "A Novel Nonlinear Magnetic Equivalent Circuit Model for Magnetic Flux Leakage System",
      "abstract": "To ensure efficient inspection using the magnetic flux leakage (MFL) method, generating a flux density near the saturation level within the tested material is essential. This requirement brings high flux density conditions in the system\u2019s pole regions. Hence, leakage flux within the slot is excessively triggered, leading to distortion of the defect signal. In this context, the system dimensions stand out as one of the most significant factors affecting the mentioned flux distributions. Therefore, various alternative solutions with different system dimensions arise in the design process of the MFL system. This study proposes a magnetic equivalent circuit (MEC) model to achieve optimal system design. The proposed MEC model is designed considering the nonlinear behavior of the material, leakage flux, and fringing effects. Verification results demonstrate that the MEC model consistently tracks the finite element analysis (FEA) results in calculating the flux densities. Furthermore, the relative errors in the flux density calculations of the tested material are at a maximum level of 10.2% and an average of 5.2% compared to the FEA. These findings indicate that the proposed MEC model can be effectively utilized in rapid prototyping and optimization procedures of MFL system design by providing fast solutions with reasonable accuracy.",
      "publication_date": "2024-05-10",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4771f1a1f4e723e7efdc1b7747b00ff17b26ec6e",
      "title": "Two-Segment Magnet Transverse Flux Ferrite PM Generator for Direct-Drive Wind Turbine Applications: Nonlinear 3-D MEC Modeling and Experimental Validation",
      "abstract": "Transverse flux PM generator (TFPMG) is a capable option for direct-drive wind turbine (DDWT) applications due to its high-power characteristics at low speeds. NdFeB-based TFPMGs may suffer from a higher total cost and lower thermal capabilities compared to the ferrite-based TFPMGs. Not yet covered in the existing literature, in this paper a transverse flux ferrite PM generator (TFFPMG) is proposed, designed, and modeled, which can also resolve the unipolar flux generation and even-order harmonics in the flux linkage of the conventional TFPMGs that occurred in conventional TFPMGs through its innovative two-segment trapezoidal shape magnet structure. Due to the 3-D flux path nature in the proposed TFFPMG, either 3-D finite element analysis (FEA) or 3-D magnetic equivalent circuit (MEC) modeling should be used. As a computationally efficient modeling method, a nonlinear 3-D MEC is established to model the entire structure of the TFFPMG while the core saturation and nonlinear permeances are also fully considered to improve the modeling accuracy. The electromagnetic performance of the proposed TFFPMG modeled by 3-D MEC in various loading conditions is validated by both 3-D FEA and experimental results using a 2.5 kW TFFPMG. A close agreement between 3-D MEC modeling predictions, 3-D FEA, and test results confirms the reliability of the MEC modeling for the proposed TFFPMG.",
      "publication_date": "2022-09-01",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "618e2830b9f1c67b265d25125a6b48520f1a4ac3",
      "title": "Utilizing an Efficient Magnetic Equivalent Circuit Model and Manifold Mapping Method for Two-Level Optimization of Axial Flux Machine",
      "abstract": "In this paper, a two-level optimization algorithm is developed based on rapid magnetic equivalent circuit (MEC) model and manifold mapping (MM) method. The performance of an axial flux permanent magnet machine (AFPM) is optimized using this approach. The rapid MEC model based on grid division exhibits significant computational efficiency, while reducing the influence of artificial factors in modeling process. Further, quantitative comparison of the error performance under different combinations of divisions are carried out. Subsequently, coarse and fine models are selected, respectively. The iterative process indicates that the modeling approach enables the coarse model to correctly follow the trends of fine model as input changes. As a result, the fast convergence of the MM is successfully achieved.",
      "publication_date": "2024-06-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "39201db4005cfccd9290bf96e1d2cb3132db1e16",
      "title": "Development of a Combined Maxwell's Equations and Magnetic Equivalent Circuit Solution for Induction Machines in Electric Vehicle Applications",
      "abstract": "This study introduces a novel analytical technique for analyzing the magnetic field in induction motors (IM). This method combines Maxwell's equations with the magnetic equivalent circuit (MEC) framework to determine flux densities in various regions of the motor. By integrating flux sources from Maxwell's equations into the MEC's network of reluctances, this method accurately considers the distribution of main and leakage magnetic fluxes within the motor. The approach significantly reduces analysis time while maintaining high accuracy and reliability when compared to numerical methods. Furthermore, it provides important motor features such as the radial and tangential components of the flux density in the air-gap, stator and rotor areas, as well as the induced voltage. Finally, to validate the accuracy of the proposed method, the analytical results are compared to the case that no leakage fluxes are included in the analytical model and to the finite-element method (FEM) in terms of computation time and accuracy. As such, this method is suggested to serve as a valuable analytical tool during the design process of IMs.",
      "publication_date": "2024-05-05",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ab7b9e504d427abbe7798e441442566ed7ea967a",
      "title": "Joint Computation Offloading and Radio Resource Allocation in MEC-Based Wireless-Powered Backscatter Communication Networks",
      "abstract": "The multi-access edge computing (MEC)-based wireless-powered backscatter communication networks (WP-BackComNets) allow wireless devices (WDs) to offload computation resources to lightweight and widely deployed MEC servers with the assistance of backscatter devices (BDs), which have substantial application prospects for the emerging Internet-of-Things applications. However, the limited battery capacity of WDs is one of the bottlenecks restricting its further development. Reducing the energy consumption and the computation burden of WDs while ensuring the quality-of-service requirements is an urgent issue. To this end, a joint computation offloading and radio resource allocation problem is formulated to minimize the total energy consumption of WDs for an MEC-based WP-BackComNet by jointly optimizing user association, the transmit power and transmission time of WDs, the computational offloading coefficient of each task, and the reflection coefficients of BDs, where the circuit power consumption of BDs, the computational capabilities of WDs, and the task execution delay budgets are considered. To handle this non-convex problem, we propose an efficient algorithm to obtain a suboptimal solution. Simulation results demonstrate that the proposed scheme can effectively decrease the energy consumption compared with the benchmarks.",
      "publication_date": "2021-05-03",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e73959ee703c09e2b05531cf5ef1bff51598abe3",
      "title": "Bioelectrochemical enhancement of methane production from exhausted vine shoot fermentation broth by integration of MEC with anaerobic digestion",
      "abstract": "A microbial electrolysis cell integrated in an anaerobic digestion system (MEC-AD) is an efficient configuration to produce methane from an exhausted vine shoot fermentation broth (EVS). The cell worked in a single-chamber two-electrode configuration at an applied potential of 1\u00a0V with a feeding ratio of 30/70 (30% EVS to 70% synthetic medium). In addition, an identical cell operated in an open circuit was used as a control reactor. Experimental results showed similar behavior in terms of carbon removal (70\u201376%), while the specific averaged methane production from cycle 7 was more stable and higher in the connected cell (MECAD) compared with the unpolarized one (OCAD) accounting for 403.7\u2009\u00b1\u200933.6 L CH4\u00b7kg VS\u22121 and 121.3\u2009\u00b1\u200949.7 L CH4\u00b7kg VS\u22121, respectively. In addition, electrochemical impedance spectroscopy revealed that the electrical capacitance of the bioanode in MECAD was twice the capacitance shown by OCAD. The bacterial community in both cells was similar but a clear adaptation of Methanosarcina Archaea was exhibited in MECAD, which could explain the increased yields in CH4 production. In summary, the results reported here confirm the advantages of integrating MEC-AD for the treatment of real organic liquid waste instead of traditional AD treatment.",
      "publication_date": "2022-06-08",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5336257faf7eeaceb742b55f81d4154f89998e42",
      "title": "Mesh-Based 3D MEC Modeling of a Novel Hybrid Claw Pole Generator",
      "abstract": "A brushless parallel hybrid excitation claw pole generator (HECPG) is proposed for electric vehicle (EV) application. Permanent magnet (PM) excitation method can reduce the volume of the machine and improve the power density and efficiency. Moreover, the voltage regulation can be ensured by field excitation. The flux path of the proposed HECPG is complex, and it will take a long time for 3D finite element analysis (FEA) to process it. To reduce simulation time, the mathematical model of the generator is given by a mesh-based 3D magnet equivalent circuit (MEC) network method considering radial and axial flux, magnetic saturation, and magnetic flux leakage. The performance of the generator is analyzed by FEA and prototype experiment. Finally, the results of 3D MEC, FEA, and experiment are compared. There is little difference between the three results, so 3D MEC can ensure the accuracy and significantly reduce the simulation time. The efficiency of the proposed HECPG is 90%, and the DC-Bus voltage can be modulated by changing the amplitude of field current.",
      "publication_date": "2022-02-24",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ed185db19b0b1025817bcde562b7e9b7b2ba1f7e",
      "title": "A Novel Modeling Technique via Coupled Magnetic Equivalent Circuit With Vector Hysteresis Characteristics of Laminated Steels",
      "abstract": "This paper proposes a method to include the anisotropic hysteresis characteristics of soft-magnetic laminated steels in the magnetic equivalent circuit (MEC) modeling. The loop-based MEC formulation is improved to handle the nonlinearity of the anisotropic magnetic hysteresis, including the dynamic classical eddy-current and excess fields. The developed MEC model is coupled with both the single-valued $B$-$H$ curve (SVC) in magnetostatic and the dynamic vector hysteresis model (VHM) in transient analysis. Results with a single elementary MEC element show that an alternating magnetic field in a single direction with a peak value smaller than 300 A/m causes a discrepancy of more than 10% between the magnetic flux densities calculated by the VHM and SVC at 50 and 200 Hz excitation frequencies. Moreover, the proposed modeling technique is verified experimentally using the laminated transformer core of TEAM problem 32. The induced voltage calculated by the MEC model with the VHM demonstrates a good agreement with the measurements, while the MEC model with the SVC calculates inaccurate voltage waveforms. Lastly, the total iron loss dissipated in the transformer's iron core is investigated to verify the proposed technique under different excitation levels and frequencies up to 500 Hz. It is observed that the proposed MEC model with the vector hysteresis characteristics of laminated steels is able to calculate the iron loss accurately, while the conventional single-valued curve method fails to estimate the iron loss.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1b34f47656f7d490bc433aeb0d4875f2247cb7d3",
      "title": "Improved Configuration Proposal for Axial Reluctance Resolver Using 3-D Magnetic Equivalent Circuit Model and Winding Function Approach",
      "abstract": "Resolver is a position sensor that is widely employed in motor servo systems under harsh working conditions. In this article, a novel axial flux resolver with a fractional-slot sinusoidal distributed winding configuration is developed, which has the advantages of a simple structure and convenient mass production. Sinusoidal distribution winding expands the optional range of pole\u2013slot combinations, and its harmonic suppression capability is proposed and verified by the winding function approach (WFA). Furthermore, the magnetic equivalent circuit (MEC) model is formulated to analyze the impact of structural parameters on the induced voltage envelope, through which the optimal parameters on the resolver are developed. In terms of computation time and accuracy, the results of the MEC model under healthy and eccentric conditions are compared with the finite-element method (FEM). In addition, the antiaxial offset capability of the half-wave resolver is illustrated and verified. The effectiveness and superiority of the proposed resolver are illustrated and verified based on numerical simulations and experimental tests.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "fb3dee071c90c2e860cd4023f682f2b11b8955c1",
      "title": "3D Rotor Position-Dependant MEC Modelling of Different Claw Pole Machine Topologies",
      "abstract": "The paper is aimed at a 3D magnetic equivalent circuit (MEC)-based modelling of claw pole synchronous machine topologies. Beyond the magnetic saturation and the armature magnetic reaction, the proposed modelling approach takes into consideration the rotor position variation, yielding the so-called: rotor position-dependant MEC. Accounting for the complexity of the magnetic circuit of claw pole topologies, specific assumptions are adopted prior a general analytical derivation of their MEC models. The developed analytical approach focuses on the air gap reluctance under variable rotor position considering a simplified geometry of the claw. A dedicated numerical procedure based on the Newton-Raphson algorithm is proposed for the resolution of the designed rotor position-dependant MEC. The proposed approach is applied to three claw pole topologies. The two first ones are equipped with a single source of excitation achieved by a field. Their analytically-predicted features are validated by experiments. The third topology has a dual excitation achieved by a field and permanent magnets (PMs) in the rotor. Its analytically-predicted features are validated by 3D finite element analysis (FEA). It is found that both experimental and FEA results are in quite good agreement with the analytical predictions yielded by the proposed rotor position-dependant MEC.",
      "publication_date": null,
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e294c3cc0071c5cd34273455342b75182b621908",
      "title": "Analysis and Compensation of the Longitudinal End Effect in Variable Reluctance Linear Resolvers Using Magnetic Equivalent Circuit Model",
      "abstract": "This article focused on the longitudinal end effect (LEE) in linear variable reluctance (VR) resolvers. The LEE can significantly decrease the accuracy of the linear sensor. Therefore, a novel method based on a magnetic equivalent circuit (MEC) model is proposed for accurate modeling of this phenomenon and used in an optimization routine to suppress that. The LEE is modeled by adding three extra permeances into the conventional MEC model without increasing the complexity and simulation time. Besides, the method of calculating these permeances is described. After that, some techniques are presented to decrease the computational burden of the MEC model. Then, the model is used to compensate the LEE by optimizing the turn number of coils. Comparing the results of the proposed model and the finite-element method (FEM) shows that the deviation of the model in the prediction of different inductances is less than 5%, while the simulation time is about 31 times less than that of FEM. Besides, the comparison between the initial design and the optimal design shows the usefulness of the model in the optimization process, in which the accuracy of the sensor improved by 74.1%. Finally, experimental tests on the prototype sensor verify the success of the analysis.",
      "publication_date": "2023-09-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    }
  ],
  "queries_used": [
    "predictive coding",
    "grid cells learning",
    "hexagonal firing",
    "MEC circuit"
  ],
  "total_cost": 0.047852,
  "general_ranking": [
    {
      "paper_id": "cd98854d3482e276b7124712c02e20ed48f56a0f",
      "title": "A generative model of the hippocampal formation trained with theta driven local learning rules",
      "abstract": "Advances in generative models have recently revolutionised machine learning. Meanwhile, in neuroscience, generative models have long been thought fundamental to animal intelligence. Understanding the biological mechanisms that support these processes promises to shed light on the relationship between biological and artificial intelligence. In animals, the hippocampal formation is thought to learn and use a generative model to support its role in spatial and non-spatial memory. Here we introduce a biologically plausible model of the hippocampal formation tantamount to a Helmholtz machine that we apply to a temporal stream of inputs. A novel component of our model is that fast theta-band oscillations (5-10 Hz) gate the direction of information flow throughout the network, training it akin to a high-frequency wake-sleep algorithm. Our model accurately infers the latent state of high-dimensional sensory environments and generates realistic sensory predictions. Furthermore, it can learn to path integrate by developing a ring attractor connectivity structure matching previous theoretical proposals and flexibly transfer this structure between environments. Whereas many models trade-off biological plausibility with generality, our model captures a variety of hippocampal cognitive functions under one biologically plausible local learning rule.",
      "publication_date": "2023-12-13",
      "venue": "bioRxiv",
      "year": 2023,
      "citation_count": 9,
      "authors": [
        "Tom M George",
        "C. Barry",
        "K. Stachenfeld",
        "C. Clopath",
        "T. Fukai"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "791686d4fa91082e225797a13060adeeb0fddf13",
      "title": "A unified theory for the computational and mechanistic origins of grid cells",
      "abstract": "The discovery of entorhinal grid cells has generated considerable interest in how and why hexagonal firing fields might mechanistically emerge in a generic manner from neural circuits, and what their computational significance might be. Here we forge an intimate link between the computational problem of path-integration and the existence of hexagonal grids, by demonstrating that such grids arise generically in biologically plausible neural networks trained to path integrate. Moreover, we develop a unifying theory for why hexagonal grids are so ubiquitous in path-integrator circuits. Such trained networks also yield powerful mechanistic hypotheses, exhibiting realistic levels of biological variability not captured by hand-designed models. We furthermore develop methods to analyze the connectome and activity maps of our trained networks to elucidate fundamental mechanisms underlying path integration. These methods provide an instructive roadmap to go from connectomic and physiological measurements to conceptual understanding in a manner that might be generalizable to other settings.",
      "publication_date": "2020-12-30",
      "venue": "Neuron",
      "year": 2020,
      "citation_count": 83,
      "authors": [
        "Ben Sorscher",
        "Gabriel C. Mel",
        "Samuel A. Ocko",
        "Lisa M. Giocomo",
        "S. Ganguli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2510c7bcbd6278b50dc73e02d127546ef70141bd",
      "title": "Learning an efficient place cell map from grid cells using non-negative sparse coding",
      "abstract": "Experimental studies of grid cells in the Medial Entorhinal Cortex (MEC) have shown that they are selective to an array of spatial locations in the environment that form a hexagonal grid. However, in a small environment, place cells in the hippocampus are only selective to a single-location of the environment while granule cells in the dentate gyrus of the hippocampus have multiple discrete firing locations, but lack spatial periodicity. Given the anatomical connection from MEC to the hippocampus, previous feedforward models of grid-to-place have been proposed. Here, we propose a unified learning model that can describe the spatial tuning properties of both hippocampal place cells and dentate gyrus granule cells based on non-negative sparse coding. Sparse coding plays an important role in many cortical areas and is proposed here to have a key role in the navigational system of the brain in the hippocampus. Our results show that the hexagonal patterns of grid cells with various orientations, grid spacings and phases are necessary for model cells to learn a single spatial field that efficiently tile the entire spatial environment. However, if there is a lack of diversity in any grid parameters or a lack of cells in the network, this will lead to the emergence of place cells that have multiple firing locations. More surprisingly, the model shows that place cells can also emerge even when non-negative sparse coding is used with weakly-tuned MEC cells, instead of MEC grid cells, as the input to place cells. This work suggests that sparse coding may be one of the underlying organizing principles for the navigational system of the brain.",
      "publication_date": "2020-08-13",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c1585013b1dbb7341a8a6821159d8d7681e81503",
      "title": "Predictive grid coding in the medial entorhinal cortex",
      "abstract": "The entorhinal cortex represents allocentric spatial geometry and egocentric speed and heading information required for spatial navigation. However, it remains unclear whether it contributes to the prediction of an animal\u2019s future location. We discovered grid cells in the medial entorhinal cortex (MEC) that have grid fields representing future locations during goal-directed behavior. These predictive grid cells represented prospective spatial information by shifting their grid fields against the direction of travel. Predictive grid cells discharged at the trough phases of the hippocampal CA1 theta oscillation and, together with other types of grid cells, organized sequences of the trajectory from the current to future positions across each theta cycle. Our results suggest that the MEC provides a predictive map that supports forward planning in spatial navigation. Editor\u2019s summary Grid cells in the entorhinal cortex create a coordinate system of the environment for spatial navigation. However, it is not clear whether the entorhinal grid system is also involved in predicting where an animal will be in the next moment. Ouchi and Fujisawa performed high-density neuronal recordings in the entorhinal cortex and hippocampal area CA1 of rats during goal-directed behavior in an open field. They observed neurons in layer 3 of the medial entorhinal cortex that explicitly encode a grid representation of an animal\u2019s future projected location, not the current position, and named them \u201c\u201cpredictive grid cells.\u201d Neuronal assemblies in the medial entorhinal cortex thus organize a predictive cognitive map. \u2014Peter Stern",
      "publication_date": "2024-08-16",
      "venue": "Science",
      "year": 2024,
      "citation_count": 7,
      "authors": [
        "Ayako Ouchi",
        "Shigeyoshi Fujisawa"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "a538579ac50d659ac0bca9824d6446e741c586b3",
      "title": "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization",
      "abstract": "Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits.",
      "publication_date": "2018-02-15",
      "venue": "International Conference on Learning Representations",
      "year": 2018,
      "citation_count": 216,
      "authors": [
        "Christopher J. Cueva",
        "Xue-Xin Wei"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "27bc20a95969cffa29065fcc1beb63ee2386b852",
      "title": "Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells",
      "abstract": "To solve the spatial problems of mapping, localization and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learnt this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, there is no satisfactory synthesis of core principles that lead to multi-modular grid cells in deep recurrent neural networks. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: coding theory, dynamical systems, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, without access to supervised position information or engineering of particular readout representations as needed in previous approaches. We show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks and emergent representations generalize well outside their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.",
      "publication_date": "2023-11-04",
      "venue": "Neural Information Processing Systems",
      "year": 2023,
      "citation_count": 20,
      "authors": [
        "Rylan Schaeffer",
        "Mikail Khona",
        "Tzuhsuan Ma",
        "Cristobal Eyzaguirre",
        "Sanmi Koyejo",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2bf3295a0cec7163c063c0d3967cc0935396bb3e",
      "title": "Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis",
      "abstract": "Many recent models study the downstream projection from grid cells to place cells, while recent data have pointed out the importance of the feedback projection. We thus asked how grid cells are affected by the nature of the input from the place cells. We propose a single-layer neural network with feedforward weights connecting place-like input cells to grid cell outputs. Place-to-grid weights are learned via a generalized Hebbian rule. The architecture of this network highly resembles neural networks used to perform Principal Component Analysis (PCA). Both numerical results and analytic considerations indicate that if the components of the feedforward neural network are non-negative, the output converges to a hexagonal lattice. Without the non-negativity constraint, the output converges to a square lattice. Consistent with experiments, grid spacing ratio between the first two consecutive modules is \u22121.4. Our results express a possible linkage between place cell to grid cell interactions and PCA. DOI: http://dx.doi.org/10.7554/eLife.10094.001",
      "publication_date": "2016-03-08",
      "venue": "eLife",
      "year": 2016,
      "citation_count": 147,
      "authors": [
        "Yedidyah Dordek",
        "Daniel Soudry",
        "R. Meir",
        "D. Derdikman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "cbeb0b647964650757d5e25aa711916e12a896dc",
      "title": "Grid cell firing patterns maintain their hexagonal firing patterns on a circular track",
      "abstract": "In an open two-dimensional environment, grid cells in the medial entorhinal cortex are known to be active in multiple locations, displaying a striking periodic hexagonal firing pattern covering the entire space. Both modeling and experimental data suggest that such periodic spatial representations may emerge from a continuous attractor network. According to this theory, grid cell activity in any stable 1D environment is a slice through an underlying 2D hexagonal pattern, which is supported by some experimental studies but challenged by others. Grid cells are believed to play a fundamental role in path integration, and so understanding their behavior in various environments is crucial for understanding the flow of information through the entorhinal-hippocampal system. To this end, we analyzed the activity of grid cells when rats traversed a circular track. A previous study involving this data set analyzed individual grid cell activity patterns separately, but we found that individual grid cells do not provide sufficient data for determining the under-lying spatial activity pattern. To circumvent this, we compute the population autocorrelation, which pools together population responses from all grid cells within the same module. This novel approach recovers the underlying six-peak hexagonal pattern that was not observable in the individual autocorrelations. We also use the population autocorrelation to infer the spacing and orientation of the population lattice, revealing how the lattice differs across environments. Furthermore, the population autocorrelation of the linearized track reveals that at the level of the population, grid cells have an allocentric code for space. These results are strong support for the attractor network theory for grid cells, and our novel approach can be used to analyze grid cell activity in any undersampled environment.",
      "publication_date": "2023-09-15",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1fe70364ca7c7cf3b5008969518fd3257e62a232",
      "title": "The hippocampus as a predictive map",
      "abstract": "A cognitive map has long been the dominant metaphor for hippocampal function, embracing the idea that place cells encode a geometric representation of space. However, evidence for predictive coding, reward sensitivity and policy dependence in place cells suggests that the representation is not purely spatial. We approach this puzzle from a reinforcement learning perspective: what kind of spatial representation is most useful for maximizing future reward? We show that the answer takes the form of a predictive representation. This representation captures many aspects of place cell responses that fall outside the traditional view of a cognitive map. Furthermore, we argue that entorhinal grid cells encode a low-dimensionality basis set for the predictive representation, useful for suppressing noise in predictions and extracting multiscale structure for hierarchical planning.",
      "publication_date": "2017-06-07",
      "venue": "Nature Neuroscience",
      "year": 2017,
      "citation_count": 757,
      "authors": [
        "Kimberly L. Stachenfeld",
        "M. Botvinick",
        "S. Gershman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "26fc224a118e20e5d6bc98b57711001abc20ec00",
      "title": "Recurrent predictive coding models for associative memory employing covariance learning",
      "abstract": "The computational principles adopted by the hippocampus in associative memory (AM) tasks have been one of the mostly studied topics in computational and theoretical neuroscience. Classical models of the hippocampal network assume that AM is performed via a form of covariance learning, where associations between memorized items are represented by entries in the learned covariance matrix encoded in the recurrent connections in the hippocampal subfield CA3. On the other hand, it has been recently proposed that AM in the hippocampus is achieved through predictive coding. Hierarchical predictive coding models following this theory perform AM, but fail to capture the recurrent hippocampal structure that encodes the covariance in the classical models. Such a dichotomy pose potential difficulties for developing a unitary theory of how memory is formed and recalled in the hippocampus. Earlier predictive coding models that learn the covariance information of inputs explicitly seem to be a solution to this dichotomy. Here, we show that although these models can perform AM, they do it in an implausible and numerically unstable way. Instead, we propose alternatives to these earlier covariance-learning predictive coding networks, which learn the covariance information implicitly and plausibly, and can use dendritic structures to encode prediction errors. We show analytically that our proposed models are perfectly equivalent to the earlier predictive coding model learning covariance explicitly, and encounter no numerical issues when performing AM tasks in practice. We further show that our models can be combined with hierarchical predictive coding networks to model the hippocampo-neocortical interactions. Our models provide a biologically plausible approach to modelling the hippocampal network, pointing to a potential computational mechanism employed by the hippocampus during memory formation and recall, which unifies predictive coding and covariance learning based on the recurrent network structure. Author summary The hippocampus and adjacent cortical areas have long been considered essential for the formation of associative memories. Earlier theoretical works have assumed that the hippocampus stores in its recurrent connections statistical regularities embedded in the sensory inputs. On the other hand, it has been recently suggested that the hippocampus retrieves memory by generating predictions of ongoing sensory inputs. Computational models have thus been proposed to account for this predictive nature of the hippocampal network using predictive coding, a general theory of information processing in the cortex. However, these hierarchical predictive coding models of the hippocampus did not describe how it stores the statistical regularities that play a key role for associative memory in the classical hippocampal models, hindering a unified understanding of the underlying computational principles employed by the hippocampus. To address this dichotomy, here we present a family of predictive coding models that also learn the statistical information needed for associative memory. Our models can stably perform associative memory tasks in a biologically plausible manner, even with large structured data such as natural scenes. Our work provides a possible mechanism of how the recurrent hippocampal network may employ various computational principles concurrently to perform associative memory.",
      "publication_date": "2022-11-09",
      "venue": "bioRxiv",
      "year": 2022,
      "citation_count": 25,
      "authors": [
        "Mufeng Tang",
        "Tommaso Salvatori",
        "Beren Millidge",
        "Yuhang Song",
        "Thomas Lukasiewicz",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "20d6f4241af85a9c4fe5f0fcb583c818441282c4",
      "title": "Learning place cells, grid cells and invariances: A unifying model",
      "abstract": "Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction. The underlying circuit mechanisms are not fully resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, a place cell is highly selective to location, but typically invariant to head direction. Here, we propose that all observed spatial tuning patterns \u2013 in both their selectivity and their invariance \u2013 are a consequence of the same mechanism: Excitatory and inhibitory synaptic plasticity that is driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. The model is robust to changes in parameters, develops patterns on behavioral time scales and makes distinctive experimental predictions. Our results suggest that the interaction of excitatory and inhibitory plasticity is a general principle for the formation of neural representations.",
      "publication_date": "2017-02-17",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6a77b5266b492c69dec62163cd692f86b9b72860",
      "title": "Emergent elasticity in the neural code for space",
      "abstract": "Significance We develop a theoretical model, grounded in known properties of neural dynamics and synaptic plasticity, that can fuse information gathered from the past history of velocity and sequence of encountered landmarks during exploratory behavior, to construct a self-consistent internal representation of space. Moreover, through model reduction techniques, we obtain conceptual insights into how consistent internal spatial representations naturally emerge through an elastic relaxation process in an effective spring\u2013particle system. We verify several experimentally testable predictions of our model involving the spatial behavior of grid cells in the medial entorhinal cortex, as well as suggest additional experiments. Upon encountering a novel environment, an animal must construct a consistent environmental map, as well as an internal estimate of its position within that map, by combining information from two distinct sources: self-motion cues and sensory landmark cues. How do known aspects of neural circuit dynamics and synaptic plasticity conspire to accomplish this feat? Here we show analytically how a neural attractor model that combines path integration of self-motion cues with Hebbian plasticity in synaptic weights from landmark cells can self-organize a consistent map of space as the animal explores an environment. Intriguingly, the emergence of this map can be understood as an elastic relaxation process between landmark cells mediated by the attractor network. Moreover, our model makes several experimentally testable predictions, including (i) systematic path-dependent shifts in the firing fields of grid cells toward the most recently encountered landmark, even in a fully learned environment; (ii) systematic deformations in the firing fields of grid cells in irregular environments, akin to elastic deformations of solids forced into irregular containers; and (iii) the creation of topological defects in grid cell firing patterns through specific environmental manipulations. Taken together, our results conceptually link known aspects of neurons and synapses to an emergent solution of a fundamental computational problem in navigation, while providing a unified account of disparate experimental observations.",
      "publication_date": "2018-05-21",
      "venue": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": 2018,
      "citation_count": 70,
      "authors": [
        "Samuel A. Ocko",
        "Kiah Hardcastle",
        "Lisa M. Giocomo",
        "S. Ganguli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8a8f7953894fef09d32c5f35fd79cfbff025e56f",
      "title": "Sequential Memory with Temporal Predictive Coding",
      "abstract": "Forming accurate memory of sequential stimuli is a fundamental function of biological agents. However, the computational mechanism underlying sequential memory in the brain remains unclear. Inspired by neuroscience theories and recent successes in applying predictive coding (PC) to static memory tasks, in this work we propose a novel PC-based model for sequential memory, called temporal predictive coding (tPC). We show that our tPC models can memorize and retrieve sequential inputs accurately with a biologically plausible neural implementation. Importantly, our analytical study reveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN) with an implicit statistical whitening process, which leads to more stable performance in sequential memory tasks of structured inputs. Moreover, we find that tPC exhibits properties consistent with behavioral observations and theories in neuroscience, thereby strengthening its biological relevance. Our work establishes a possible computational mechanism underlying sequential memory in the brain that can also be theoretically interpreted using existing memory model frameworks.",
      "publication_date": "2023-05-19",
      "venue": "Neural Information Processing Systems",
      "year": 2023,
      "citation_count": 19,
      "authors": [
        "Mufeng Tang",
        "Helen C. Barron",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "138a75cab26100df70bdb66dd1a28e68612db8ff",
      "title": "Learning place cells, grid cells and invariances with excitatory and inhibitory plasticity",
      "abstract": "Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction, and the underlying circuit mechanisms are not yet resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, place cells are typically invariant to head direction. We propose that all observed spatial tuning patterns \u2013 in both their selectivity and their invariance \u2013 arise from the same mechanism: Excitatory and inhibitory synaptic plasticity driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. Our proposed model is robust to changes in parameters, develops patterns on behavioral timescales and makes distinctive experimental predictions.",
      "publication_date": "2018-02-21",
      "venue": "eLife",
      "year": 2018,
      "citation_count": 50,
      "authors": [
        "Simon Nikolaus Weber",
        "Henning Sprekeler"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "35ce44e06802d9f778eb02bf03a6fc5aa657bd56",
      "title": "Inferring neural activity before plasticity as a foundation for learning beyond backpropagation",
      "abstract": "This paper introduces \u2018prospective configuration\u2019, a new principle for learning in neural networks, which differs from backpropagation and is more efficient in learning and more consistent with data on neural activity and behavior. For both humans and machines, the essence of learning is to pinpoint which components in its information processing pipeline are responsible for an error in its output, a challenge that is known as \u2018credit assignment\u2019. It has long been assumed that credit assignment is best solved by backpropagation, which is also the foundation of modern machine learning. Here, we set out a fundamentally different principle on credit assignment called \u2018prospective configuration\u2019. In prospective configuration, the network first infers the pattern of neural activity that should result from learning, and then the synaptic weights are modified to consolidate the change in neural activity. We demonstrate that this distinct mechanism, in contrast to backpropagation, (1) underlies learning in a well-established family of models of cortical circuits, (2) enables learning that is more efficient and effective in many contexts faced by biological organisms and (3) reproduces surprising patterns of neural activity and behavior observed in diverse human and rat learning experiments.",
      "publication_date": "2024-01-03",
      "venue": "Nature Neuroscience",
      "year": 2024,
      "citation_count": 16,
      "authors": [
        "Yuhang Song",
        "Beren Millidge",
        "Tommaso Salvatori",
        "Thomas Lukasiewicz",
        "Zhenghua Xu",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "870d2b9c71c4448498a47870f2622f0a2db44b20",
      "title": "Predictive coding networks for temporal prediction",
      "abstract": "One of the key problems the brain faces is inferring the state of the world from a sequence of dynamically changing stimuli, and it is not yet clear how the sensory system achieves this task. A well-established computational framework for describing perceptual processes in the brain is provided by the theory of predictive coding. Although the original proposals of predictive coding have discussed temporal prediction, later work developing this theory mostly focused on static stimuli, and key questions on neural implementation and computational properties of temporal predictive coding networks remain open. Here, we address these questions and present a formulation of the temporal predictive coding model that can be naturally implemented in recurrent networks, in which activity dynamics rely only on local inputs to the neurons, and learning only utilises local Hebbian plasticity. Additionally, we show that temporal predictive coding networks can approximate the performance of the Kalman filter in predicting behaviour of linear systems, and behave as a variant of a Kalman filter which does not track its own subjective posterior variance. Importantly, temporal predictive coding networks can achieve similar accuracy as the Kalman filter without performing complex mathematical operations, but just employing simple computations that can be implemented by biological networks. Moreover, when trained with natural dynamic inputs, we found that temporal predictive coding can produce Gabor-like, motion-sensitive receptive fields resembling those observed in real neurons in visual areas. In addition, we demonstrate how the model can be effectively generalized to nonlinear systems. Overall, models presented in this paper show how biologically plausible circuits can predict future stimuli and may guide research on understanding specific neural circuits in brain areas involved in temporal prediction. Author summary While significant advances have been made in the neuroscience of how the brain processes static stimuli, the time dimension has often been relatively neglected. However, time is crucial since the stimuli perceived by our senses typically dynamically vary in time, and the cortex needs to make sense of these changing inputs. This paper describes a computational model of cortical networks processing temporal stimuli. This model is able to infer and track the state of the environment based on noisy inputs, and predict future sensory stimuli. By ensuring that these predictions match the incoming stimuli, the model is able to learn the structure and statistics of its temporal inputs and produces responses of neurons resembling those in the brain. The model may help in further understanding neural circuits in sensory cortical areas.",
      "publication_date": "2024-03-09",
      "venue": "bioRxiv",
      "year": 2024,
      "citation_count": 20,
      "authors": [
        "Beren Millidge",
        "Mufeng Tang",
        "Mahyar Osanlouy",
        "N. Harper",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "838c48b300fd1be40014c97edd79233b1fb52f69",
      "title": "Hippocampal Spike-Timing Correlations Lead to Hexagonal Grid Fields.",
      "abstract": "Space is represented in the mammalian brain by the activity of hippocampal place cells, as well as in their spike-timing correlations. Here, we propose a theory for how this temporal code is transformed to spatial firing rate patterns via spike-timing-dependent synaptic plasticity. The resulting dynamics of synaptic weights resembles well-known pattern formation models in which a lateral inhibition mechanism gives rise to a Turing instability. We identify parameter regimes in which hexagonal firing patterns develop as they have been found in medial entorhinal cortex.",
      "publication_date": "2017-06-21",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d09c79f6b7420c8c8a28ecab0836ec9bacb8f8cf",
      "title": "A non-spatial account of place and grid cells based on clustering models of concept learning",
      "abstract": "One view is that conceptual knowledge is organized using the circuitry in the medial temporal lobe (MTL) that supports spatial processing and navigation. In contrast, we find that a domain-general learning algorithm explains key findings in both spatial and conceptual domains. When the clustering model is applied to spatial navigation tasks, so-called place and grid cell-like representations emerge because of the relatively uniform distribution of possible inputs in these tasks. The same mechanism applied to conceptual tasks, where the overall space can be higher-dimensional and sampling sparser, leading to representations more aligned with human conceptual knowledge. Although the types of memory supported by the MTL are superficially dissimilar, the information processing steps appear shared. Our account suggests that the MTL uses a general-purpose algorithm to learn and organize context-relevant information in a useful format, rather than relying on navigation-specific neural circuitry. Spatial maps in the medial temporal lobe (MTL) have been proposed to map abstract conceptual knowledge. Rather than grounding abstract knowledge in a spatial map, the authors propose a general-purpose clustering algorithm that explains how both spatial (including place and grid cells) and higher-dimensional conceptual representations arise during learning.",
      "publication_date": "2018-09-19",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4af94a163a695f14597ac101a9562a68772010ae",
      "title": "Probabilistic Learning by Rodent Grid Cells",
      "abstract": "Mounting evidence shows mammalian brains are probabilistic computers, but the specific cells involved remain elusive. Parallel research suggests that grid cells of the mammalian hippocampal formation are fundamental to spatial cognition but their diverse response properties still defy explanation. No plausible model exists which explains stable grids in darkness for twenty minutes or longer, despite being one of the first results ever published on grid cells. Similarly, no current explanation can tie together grid fragmentation and grid rescaling, which show very different forms of flexibility in grid responses when the environment is varied. Other properties such as attractor dynamics and grid anisotropy seem to be at odds with one another unless additional properties are assumed such as a varying velocity gain. Modelling efforts have largely ignored the breadth of response patterns, while also failing to account for the disastrous effects of sensory noise during spatial learning and recall, especially in darkness. Here, published electrophysiological evidence from a range of experiments are reinterpreted using a novel probabilistic learning model, which shows that grid cell responses are accurately predicted by a probabilistic learning process. Diverse response properties of probabilistic grid cells are statistically indistinguishable from rat grid cells across key manipulations. A simple coherent set of probabilistic computations explains stable grid fields in darkness, partial grid rescaling in resized arenas, low-dimensional attractor grid cell dynamics, and grid fragmentation in hairpin mazes. The same computations also reconcile oscillatory dynamics at the single cell level with attractor dynamics at the cell ensemble level. Additionally, a clear functional role for boundary cells is proposed for spatial learning. These findings provide a parsimonious and unified explanation of grid cell function, and implicate grid cells as an accessible neuronal population readout of a set of probabilistic spatial computations.",
      "publication_date": "2016-10-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "bc7417738a7f7186ce9b06933072c4ddd39ce4b1",
      "title": "Actionable Neural Representations: Grid Cells from Minimal Constraints",
      "abstract": "To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won't have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an `actionable representation'. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode.",
      "publication_date": "2022-09-30",
      "venue": "International Conference on Learning Representations",
      "year": 2022,
      "citation_count": 23,
      "authors": [
        "W. Dorrell",
        "P. Latham",
        "T. Behrens",
        "James C. R. Whittington"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "abb5aa3ff57e043bbaf59239ab27150a2545aee5",
      "title": "Associative Memories via Predictive Coding",
      "abstract": "Associative memories in the brain receive and store patterns of activity registered by the sensory neurons, and are able to retrieve them when necessary. Due to their importance in human intelligence, computational models of associative memories have been developed for several decades now. In this paper, we present a novel neural model for realizing associative memories, which is based on a hierarchical generative network that receives external stimuli via sensory neurons. It is trained using predictive coding, an error-based learning algorithm inspired by information processing in the cortex. To test the model's capabilities, we perform multiple retrieval experiments from both corrupted and incomplete data points. In an extensive comparison, we show that this new model outperforms in retrieval accuracy and robustness popular associative memory models, such as autoencoders trained via backpropagation, and modern Hopfield networks. In particular, in completing partial data points, our model achieves remarkable results on natural image datasets, such as ImageNet, with a surprisingly high accuracy, even when only a tiny fraction of pixels of the original images is presented. Our model provides a plausible framework to study learning and retrieval of memories in the brain, as it closely mimics the behavior of the hippocampus as a memory index and generative model.",
      "publication_date": "2021-09-16",
      "venue": "Neural Information Processing Systems",
      "year": 2021,
      "citation_count": 64,
      "authors": [
        "Tommaso Salvatori",
        "Yuhang Song",
        "Yujian Hong",
        "Simon Frieder",
        "Lei Sha",
        "Zhenghua Xu",
        "R. Bogacz",
        "Thomas Lukasiewicz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "c124a6aec4b1833e4e86092e20a782183349d57e",
      "title": "An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity",
      "abstract": "Abstract To efficiently learn from feedback, cortical networks need to update synaptic weights on multiple levels of cortical hierarchy. An effective and well-known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm. However, in this algorithm, the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons. Several models have been proposed that approximate the backpropagation algorithm with local synaptic plasticity, but these models require complex external control over the network or relatively complex plasticity rules. Here we show that a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple local Hebbian plasticity. Furthermore, for certain parameters, the weight change in the predictive coding model converges to that of the backpropagation algorithm. This suggests that it is possible for cortical networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output.",
      "publication_date": "2017-03-23",
      "venue": "Neural Computation",
      "year": 2017,
      "citation_count": 284,
      "authors": [
        "James C. R. Whittington",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "5374052df7a9a69499586b5acfc433f8a8e20e66",
      "title": "On Conformal Isometry of Grid Cells: Learning Distance-Preserving Position Embedding",
      "abstract": "This paper investigates the conformal isometry hypothesis as a potential explanation for the hexagonal periodic patterns in grid cell response maps. We posit that grid cell activities form a high-dimensional vector in neural space, encoding the agent's position in 2D physical space. As the agent moves, this vector rotates within a 2D manifold in the neural space, driven by a recurrent neural network. The conformal hypothesis proposes that this neural manifold is a conformal isometric embedding of 2D physical space, where local physical distance is preserved by the embedding up to a scaling factor (or unit of metric). Such distance-preserving position embedding is indispensable for path planning in navigation, especially planning local straight path segments. We conduct numerical experiments to show that this hypothesis leads to the hexagonal grid firing patterns by learning maximally distance-preserving position embedding, agnostic to the choice of the recurrent neural network. Furthermore, we present a theoretical explanation of why hexagon periodic patterns emerge by minimizing our loss function by showing that hexagon flat torus is maximally distance preserving.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0a292908d132abc2bf5fc5413d8c121600b5288e",
      "title": "A Computational Model of Grid Cells based on Dendritic Self-organized Learning",
      "abstract": "In this paper we present a new computational model for grid cells. These cells are neurons in the entorhinal cortex of the hippocampal region that encode allocentric spatial information. They possess a peculiar, triangular firing pattern that spans the entire environment with a virtual lattice. We show that such a firing pattern can emerge from a dendritic, self-organized learning process. A key aspect of the proposed model is the hypothesis that the dendritic tree of a grid cell can behave like a sparse self organizing map that tries to cover its input space as best as possible. We argue, that the encoding scheme used by grid cells is possibly not limited to the description of spatial information and may represent a general principle on how complex information is encoded in higher level brain areas like the hippocampal region.",
      "publication_date": null,
      "venue": "",
      "year": 2013,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "34a9b0c7ddc9012b8657c4bc1de5f14d45e646b4",
      "title": "Hybrid predictive coding: Inferring, fast and slow",
      "abstract": "Predictive coding is an influential model of cortical neural activity. It proposes that perceptual beliefs are furnished by sequentially minimising \u201cprediction errors\u201d\u2014the differences between predicted and observed data. Implicit in this proposal is the idea that successful perception requires multiple cycles of neural activity. This is at odds with evidence that several aspects of visual perception\u2014including complex forms of object recognition\u2014arise from an initial \u201cfeedforward sweep\u201d that occurs on fast timescales which preclude substantial recurrent activity. Here, we propose that the feedforward sweep can be understood as performing amortized inference (applying a learned function that maps directly from data to beliefs) and recurrent processing can be understood as performing iterative inference (sequentially updating neural activity in order to improve the accuracy of beliefs). We propose a hybrid predictive coding network that combines both iterative and amortized inference in a principled manner by describing both in terms of a dual optimization of a single objective function. We show that the resulting scheme can be implemented in a biologically plausible neural architecture that approximates Bayesian inference utilising local Hebbian update rules. We demonstrate that our hybrid predictive coding model combines the benefits of both amortized and iterative inference\u2014obtaining rapid and computationally cheap perceptual inference for familiar data while maintaining the context-sensitivity, precision, and sample efficiency of iterative inference schemes. Moreover, we show how our model is inherently sensitive to its uncertainty and adaptively balances iterative and amortized inference to obtain accurate beliefs using minimum computational expense. Hybrid predictive coding offers a new perspective on the functional relevance of the feedforward and recurrent activity observed during visual perception and offers novel insights into distinct aspects of visual phenomenology.",
      "publication_date": "2022-04-05",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2c0203ff41fbe8f3fc2a0e706ecb3ecf806f2108",
      "title": "Predictive Coding Approximates Backprop Along Arbitrary Computation Graphs",
      "abstract": "Abstract Backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. Recently it has been shown that backprop in multilayer perceptrons (MLPs) can be approximated using predictive coding, a biologically plausible process theory of cortical computation that relies solely on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs but in the concept of automatic differentiation, which allows for the optimization of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice, rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding convolutional neural networks, recurrent neural networks, and the more complex long short-term memory, which include a nonlayer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks while using only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry and may also contribute to the development of completely distributed neuromorphic architectures.",
      "publication_date": "2020-06-07",
      "venue": "Neural Computation",
      "year": 2020,
      "citation_count": 119,
      "authors": [
        "Beren Millidge",
        "Alexander Tschantz",
        "C. Buckley"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "ad367b44f3434b9ba6b46b41ab083210f6827a9f",
      "title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning",
      "abstract": "While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (\"PredNet\") architecture that is inspired by the concept of \"predictive coding\" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.",
      "publication_date": "2016-05-25",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "998628588f7850d533a172c883872057a9198d82",
      "title": "Predictive Coding: a Theoretical and Experimental Review",
      "abstract": "Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.",
      "publication_date": "2021-07-27",
      "venue": "arXiv.org",
      "year": 2021,
      "citation_count": 127,
      "authors": [
        "Beren Millidge",
        "A. Seth",
        "C. Buckley"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "25b422756cbddb3193002c9a49c844641fff2127",
      "title": "No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit",
      "abstract": "Research in Neuroscience, as in many scientific disciplines, is undergoing a renaissance based on deep learning. Unique to Neuroscience, deep learning models can be used not only as a tool but interpreted as models of the brain. The central claims of recent deep learning-based models of brain circuits are that they make novel predictions about neural phenomena or shed light on the fundamental functions being optimized. We show, through the case-study of grid cells in the entorhinal-hippocampal circuit, that one may get neither. We begin by reviewing the principles of grid cell mechanism and function obtained from first-principles modeling efforts, then rigorously examine the claims of deep learning models of grid cells. Using large-scale architectural and hyperparameter sweeps and theory-driven experimentation, we demonstrate that the results of such models may be more strongly driven by particular, non-fundamental, and post-hoc implementation choices than fundamental truths about neural circuits or the loss function(s) they might optimize. We discuss why these models cannot be expected to produce accurate models of the brain without the addition of substantial amounts of inductive bias, an informal No Free Lunch result for Neuroscience. Based on first principles work, we provide hypotheses for what additional loss functions will produce grid cells more robustly. In conclusion, circumspection and transparency, together with biological knowledge, are warranted in building and interpreting deep learning models in Neuroscience.",
      "publication_date": "2023-08-15",
      "venue": "bioRxiv",
      "year": 2023,
      "citation_count": 57,
      "authors": [
        "Rylan Schaeffer",
        "Mikail Khona",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6bcdf260d7927d8fe4ff030c20ee1db974d0c969",
      "title": "Predictive Coding: Towards a Future of Deep Learning beyond Backpropagation?",
      "abstract": "The backpropagation of error algorithm (BP) used to train deep neural networks has been fundamental to the successes of deep learning. However, it requires sequential backwards updates and non-local computations which make it challenging to parallelize at scale and is unlike how learning works in the brain. Neuroscience-inspired learning algorithms, however, such as \\emph{predictive coding} which utilize local learning have the potential to overcome these limitations and advance beyond deep learning technologies in the future. While predictive coding originated in theoretical neuroscience as a model of information processing in the cortex, recent work has developed the idea into a general-purpose algorithm able to train neural networks using only local computations. In this survey, we review works that have contributed to this perspective and demonstrate the close connection between predictive coding and backpropagation in terms of generalization quality, as well as works that highlight the multiple advantages of using predictive coding models over backprop-trained neural networks. Specifically, we show the substantially greater flexibility of predictive coding networks against equivalent deep neural networks, which can function as classifiers, generators, and associative memories simultaneously, and can be defined on arbitrary graph topologies. Finally, we review direct benchmarks of predictive coding networks on machine learning classification tasks, as well as its close connections to control theory and applications in robotics.",
      "publication_date": "2022-02-18",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "66c286df54551baba7351a1ed44019367e5aa7ea",
      "title": "Evidence of a predictive coding hierarchy in the human brain listening to speech",
      "abstract": "Current machine learning language algorithms make adjacent word-level predictions. In this work, Caucheteux et al. show that the human brain probably uses long-range and hierarchical predictions, taking into account up to eight possible words into the future. Considerable progress has recently been made in natural language processing: deep learning algorithms are increasingly able to generate, summarize, translate and classify texts. Yet, these language models still fail to match the language abilities of humans. Predictive coding theory offers a tentative explanation to this discrepancy: while language models are optimized to predict nearby words, the human brain would continuously predict a hierarchy of representations that spans multiple timescales. To test this hypothesis, we analysed the functional magnetic resonance imaging brain signals of 304 participants listening to short stories. First, we confirmed that the activations of modern language models linearly map onto the brain responses to speech. Second, we showed that enhancing these algorithms with predictions that span multiple timescales improves this brain mapping. Finally, we showed that these predictions are organized hierarchically: frontoparietal cortices predict higher-level, longer-range and more contextual representations than temporal cortices. Overall, these results strengthen the role of hierarchical predictive coding in language processing and illustrate how the synergy between neuroscience and artificial intelligence can unravel the computational bases of human cognition.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "f4649234d52d1848495dcebd9f0cc9042e636bff",
      "title": "A Spin Glass Model of Path Integration in Rat Medial Entorhinal Cortex",
      "abstract": "Electrophysiological recording studies in the dorsocaudal region of medial entorhinal cortex (dMEC) of the rat reveal cells whose spatial firing fields show a remarkably regular hexagonal grid pattern (Fyhn et al., 2004; Hafting et al., 2005). We describe a symmetric, locally connected neural network, or spin glass model, that spontaneously produces a hexagonal grid of activity bumps on a two-dimensional sheet of units. The spatial firing fields of the simulated cells closely resemble those of dMEC cells. A collection of grids with different scales and/or orientations forms a basis set for encoding position. Simulations show that the animal\u2019s location can easily be determined from the population activity pattern. Introducing an asymmetry in the model allows the activity bumps to be shifted in any direction, at a rate proportional to velocity, to achieve path integration. Furthermore, information about the structure of the environment can be superimposed on the spatial position signal by modulation of the bump activity levels without significantly interfering with the hexagonal periodicity of firing fields. Our results support the conjecture of Hafting et al. (2005) that an attractor network in dMEC may be the source of path integration information afferent to hippocampus.",
      "publication_date": "2006-04-19",
      "venue": "Journal of Neuroscience",
      "year": 2006,
      "citation_count": 617,
      "authors": [
        "Mark C. Fuhs",
        "D. Touretzky"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "da741827ab6f5bf9762d30b91fd2409ac9e0e272",
      "title": "World models and predictive coding for cognitive and developmental robotics: frontiers and challenges",
      "abstract": "ABSTRACT Creating autonomous robots that can actively explore the environment, acquire knowledge and learn skills continuously is the ultimate achievement envisioned in cognitive and developmental robotics. Importantly, if the aim is to create robots that can continuously develop through interactions with their environment, their learning processes should be based on interactions with their physical and social world in the manner of human learning and cognitive development. Based on this context, in this paper, we focus on the two concepts of world models and predictive coding. Recently, world models have attracted renewed attention as a topic of considerable interest in artificial intelligence. Cognitive systems learn world models to better predict future sensory observations and optimize their policies, i.e. controllers. Alternatively, in neuroscience, predictive coding proposes that the brain continuously predicts its inputs and adapts to model its own dynamics and control behavior in its environment. Both ideas may be considered as underpinning the cognitive development of robots and humans capable of continual or lifelong learning. Although many studies have been conducted on predictive coding in cognitive robotics and neurorobotics, the relationship between world model-based approaches in AI and predictive coding in robotics has rarely been discussed. Therefore, in this paper, we clarify the definitions, relationships, and status of current research on these topics, as well as missing pieces of world models and predictive coding in conjunction with crucially related concepts such as the free-energy principle and active inference in the context of cognitive and developmental robotics. Furthermore, we outline the frontiers and challenges involved in world models and predictive coding toward the further integration of AI and robotics, as well as the creation of robots with real cognitive and developmental capabilities in the future. GRAPHICAL ABSTRACT",
      "publication_date": "2023-01-14",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b820ad4a35a6587b44ab03c0e70672a2ed5e9c5f",
      "title": "Accurate Path Integration in Continuous Attractor Network Models of Grid Cells",
      "abstract": "Grid cells in the rat entorhinal cortex display strikingly regular firing responses to the animal's position in 2-D space and have been hypothesized to form the neural substrate for dead-reckoning. However, errors accumulate rapidly when velocity inputs are integrated in existing models of grid cell activity. To produce grid-cell-like responses, these models would require frequent resets triggered by external sensory cues. Such inadequacies, shared by various models, cast doubt on the dead-reckoning potential of the grid cell system. Here we focus on the question of accurate path integration, specifically in continuous attractor models of grid cell activity. We show, in contrast to previous models, that continuous attractor models can generate regular triangular grid responses, based on inputs that encode only the rat's velocity and heading direction. We consider the role of the network boundary in the integration performance of the network and show that both periodic and aperiodic networks are capable of accurate path integration, despite important differences in their attractor manifolds. We quantify the rate at which errors in the velocity integration accumulate as a function of network size and intrinsic noise within the network. With a plausible range of parameters and the inclusion of spike variability, our model networks can accurately integrate velocity inputs over a maximum of \u223c10\u2013100 meters and \u223c1\u201310 minutes. These findings form a proof-of-concept that continuous attractor dynamics may underlie velocity integration in the dorsolateral medial entorhinal cortex. The simulations also generate pertinent upper bounds on the accuracy of integration that may be achieved by continuous attractor dynamics in the grid cell network. We suggest experiments to test the continuous attractor model and differentiate it from models in which single cells establish their responses independently of each other.",
      "publication_date": "2008-11-12",
      "venue": "PLoS Comput. Biol.",
      "year": 2008,
      "citation_count": 703,
      "authors": [
        "Y. Burak",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "115806ae388f43e1e25062ce88d2415c64c580dc",
      "title": "Grid cell firing fields in a volumetric space",
      "abstract": "We investigated how entorhinal grid cells represent volumetric (three-dimensional) space. On a flat surface, grid cell firing fields are circular and arranged in a close-packed hexagonal array. In three dimensions, theoretical and computational work suggests that the most efficient configuration would be a regular close packing of spherical fields. We report that in rats exploring a cubic lattice, grid cells were spatially stable and maintained normal directional modulation, theta modulation and spike dynamics. However, while the majority of grid fields were spherical, they were irregularly arranged, even when only fields abutting the lower surface (equivalent to the floor) were considered. Thus, grid organization is shaped by the environment\u2019s movement affordances, and may not need to be regular to support spatial computations. One Sentence Summary In rats exploring a volumetric space, grid cells are spatially modulated but their firing fields are irregularly arranged.",
      "publication_date": "2020-12-07",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "48fa2392ceb51aa5fc57766d513efe1fa9bf3de0",
      "title": "Spatial periodicity in grid cell firing is explained by a neural sequence code of 2-D trajectories",
      "abstract": "Spatial periodicity in grid cell firing has been interpreted as a neural metric for space providing animals with a coordinate system in navigating physical and mental spaces. However, the specific computational problem being solved by grid cells has remained elusive. Here, we provide mathematical proof that spatial periodicity in grid cell firing is the only possible solution to a neural sequence code of 2-D trajectories and that the hexagonal firing pattern of grid cells is the most parsimonious solution to such a sequence code. We thereby provide a likely teleological cause for the existence of grid cells and reveal the underlying nature of the global geometric organization in grid maps as a direct consequence of a simple local sequence code. A sequence code by grid cells provides intuitive explanations for many previously puzzling experimental observations and may transform our thinking about grid cells.",
      "publication_date": "2023-05-30",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5be0bfaa322272f3c70a203bcc0aef87a6a6dc13",
      "title": "A Brain-Inspired Adaptive Space Representation Model Based on Grid Cells and Place Cells",
      "abstract": "Grid cells and place cells are important neurons in the animal brain. The information transmission between them provides the basis for the spatial representation and navigation of animals and also provides reference for the research on the autonomous navigation mechanism of intelligent agents. Grid cells are important information source of place cells. The supervised learning and unsupervised learning models can be used to simulate the generation of place cells from grid cell inputs. However, the existing models preset the firing characteristics of grid cell. In this paper, we propose a united generation model of grid cells and place cells. First, the visual place cells with nonuniform distribution generate the visual grid cells with regional firing field through feedforward network. Second, the visual grid cells and the self-motion information generate the united grid cells whose firing fields extend to the whole space through genetic algorithm. Finally, the visual place cells and the united grid cells generate the united place cells with uniform distribution through supervised fuzzy adaptive resonance theory (ART) network. Simulation results show that this model has stronger environmental adaptability and can provide reference for the research on spatial representation model and brain-inspired navigation mechanism of intelligent agents under the condition of nonuniform environmental information.",
      "publication_date": "2020-08-11",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e83816c0e7ed01ee48d579568ba97db72931d979",
      "title": "Learning on Arbitrary Graph Topologies via Predictive Coding",
      "abstract": "Training with backpropagation (BP) in standard deep learning consists of two main steps: a forward pass that maps a data point to its prediction, and a backward pass that propagates the error of this prediction back through the network. This process is highly effective when the goal is to minimize a specific objective function. However, it does not allow training on networks with cyclic or backward connections. This is an obstacle to reaching brain-like capabilities, as the highly complex heterarchical structure of the neural connections in the neocortex are potentially fundamental for its effectiveness. In this paper, we show how predictive coding (PC), a theory of information processing in the cortex, can be used to perform inference and learning on arbitrary graph topologies. We experimentally show how this formulation, called PC graphs, can be used to flexibly perform different tasks with the same network by simply stimulating specific neurons. This enables the model to be queried on stimuli with different structures, such as partial images, images with labels, or images without labels. We conclude by investigating how the topology of the graph influences the final performance, and comparing against simple baselines trained with BP.",
      "publication_date": "2022-01-31",
      "venue": "Neural Information Processing Systems",
      "year": 2022,
      "citation_count": 34,
      "authors": [
        "Tommaso Salvatori",
        "Luca Pinchetti",
        "Beren Millidge",
        "Yuhang Song",
        "R. Bogacz",
        "Thomas Lukasiewicz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "e96016d5e3c6492230aa172efcf733596dc64b6e",
      "title": "Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells",
      "abstract": "Unsupervised text encoding models have recently fueled substantial progress in Natural Language Processing (NLP). The key idea is to use neural networks to convert words in texts to vector space representations (embeddings) based on word positions in a sentence and their contexts. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as Points of Interest (POIs) into models. A general space encoding method is valuable for a multitude of tasks such asPOI search, land use classification, point-based spatial interpolation and locationaware image classification. However, no such general model exists to date beyond simply applying discretizing or feed forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for encoding space and are critical for recognizing places and for path-integration. Inspired by this research, wepropose a representation learning model called Space2vec to encode the absolutepositions and spatial relationships of places. We conduct experiments on realworld geographic data and predict types of POIs at given positions based on their1) locations and 2) nearby POIs. Results show that because of its multi-scale representations Space2vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed forward nets, and tile embedding approaches.",
      "publication_date": "2020-02-16",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "de629192ca5b0b7195355149dad23bd4124bf3df",
      "title": "Irregular distribution of grid cell firing fields in rats exploring a 3D volumetric space",
      "abstract": "We investigated how entorhinal grid cells encode volumetric space. On a horizontal surface, grid cells usually produce multiple, spatially focal, approximately circular firing fields that are evenly sized and spaced to form a regular, close-packed, hexagonal array. This spatial regularity has been suggested to underlie navigational computations. In three dimensions, theoretically the equivalent firing pattern would be a regular, hexagonal close packing of evenly sized spherical fields. In the present study, we report that, in rats foraging in a cubic lattice, grid cells maintained normal temporal firing characteristics and produced spatially stable firing fields. However, although most grid fields were ellipsoid, they were sparser, larger, more variably sized and irregularly arranged, even when only fields abutting the lower surface (equivalent to the floor) were considered. Thus, grid self-organization is shaped by the environment\u2019s structure and/or movement affordances, and grids may not need to be regular to support spatial computations. Grieves et al. show that when rats explore a 3D space, grid cells in the entorhinal cortex exchange their usual spatially regular firing patterns for more irregular ones, suggesting that 3D space is mapped differently than previously thought.",
      "publication_date": "2021-08-11",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "79be3dcc0cd53613e5386425e387b2664c9c9ab0",
      "title": "Untethered firing fields and intermittent silences: Why grid\u2010cell discharge is so variable",
      "abstract": "Grid cells in medial entorhinal cortex are notoriously variable in their responses, despite the striking hexagonal arrangement of their spatial firing fields. Indeed, when the animal moves through a firing field, grid cells often fire much more vigorously than predicted or do not fire at all. The source of this trial\u2010to\u2010trial variability is not completely understood. By analyzing grid\u2010cell spike trains from mice running in open arenas and on linear tracks, we characterize the phenomenon of \u201cmissed\u201d firing fields using the statistical theory of zero inflation. We find that one major cause of grid\u2010cell variability lies in the spatial representation itself: firing fields are not as strongly anchored to spatial location as the averaged grid suggests. In addition, grid fields from different cells drift together from trial to trial, regardless of whether the environment is real or virtual, or whether the animal moves in light or darkness. Spatial realignment across trials sharpens the grid representation, yielding firing fields that are more pronounced and significantly narrower. These findings indicate that ensembles of grid cells encode relative position more reliably than absolute position.",
      "publication_date": "2020-02-11",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b9ea31c9c8719a504dd349feffbd79c7c8d18e23",
      "title": "Coupled Noisy Spiking Neurons as Velocity-Controlled Oscillators in a Model of Grid Cell Spatial Firing",
      "abstract": "One of the two primary classes of models of grid cell spatial firing uses interference between oscillators at dynamically modulated frequencies. Generally, these models are presented in terms of idealized oscillators (modeled as sinusoids), which differ from biological oscillators in multiple important ways. Here we show that two more realistic, noisy neural models (Izhikevich's simple model and a biophysical model of an entorhinal cortex stellate cell) can be successfully used as oscillators in a model of this type. When additive noise is included in the models such that uncoupled or sparsely coupled cells show realistic interspike interval variance, both synaptic and gap-junction coupling can synchronize networks of cells to produce comparatively less variable network-level oscillations. We show that the frequency of these oscillatory networks can be controlled sufficiently well to produce stable grid cell spatial firing on the order of at least 2\u20135 min, despite the high noise level. Our results suggest that the basic principles of oscillatory interference models work with more realistic models of noisy neurons. Nevertheless, a number of simplifications were still made and future work should examine increasingly realistic models.",
      "publication_date": "2010-10-13",
      "venue": "Journal of Neuroscience",
      "year": 2010,
      "citation_count": 108,
      "authors": [
        "Eric A. Zilli",
        "M. Hasselmo"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2abc9d603a87b300a251a3796a12b8a2d21746df",
      "title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion",
      "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multiplication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.",
      "publication_date": "2018-09-27",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "d40cb3f74f7e4a0858bc63760ec983f15de8125c",
      "title": "Grid-Cell Activity on Linear Tracks Indicates Purely Translational Remapping of 2D Firing Patterns at Movement Turning Points",
      "abstract": "Grid cells in rodent medial entorhinal cortex are thought to play a critical role for spatial navigation. When the animal is freely moving in an open arena the firing fields of each grid cell tend to form a hexagonal lattice spanning the environment. For movements along a linear track the cells seem to respond differently. They show multiple firing fields that are not periodically arranged and whose shape and position change when the running direction is reversed. In addition, peak firing rates vary widely from field to field. Measured along one running direction only, firing fields are, however, compatible with a slice through a two-dimensional (2D) hexagonal pattern. It is an open question, whether this is also true if leftward and rightward runs are jointly considered. By analyzing data from 15 male Long\u2013Evans rats, we show that a single hexagonal firing pattern explains the linear-track data if translational shifts of the pattern are allowed at the movement turning points. A rotation or scaling of the grid is not required. The agreement is further improved if the peak firing rates of the underlying 2D grid fields can vary from field to field, as suggested by recent studies. These findings have direct consequences for experiments using linear tracks in virtual reality. SIGNIFICANCE STATEMENT Various types of neurons support spatial navigation. Their response properties are often studied in reduced settings and might change when the animal can freely explore its environment. Grid cells in rodents, for example, exhibit seemingly irregular firing fields when animal movement is restricted to a linear track but highly regular patterns in two-dimensional (2D) arenas. We show that linear-track responses of a cell for both leftward and rightward running directions can be explained as cuts through a single hexagonal pattern if translational remapping is allowed at movement turning points; neither rotations nor scale transformations are needed. These results provide a basis to quantify grid-cell activity in 1D virtual reality and could help to detect and categorize grid cells without experiments in 2D environments.",
      "publication_date": "2018-07-05",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0c48611df1f03ca7cec7f98a1ffaadf95dec1260",
      "title": "Grids from bands, or bands from grids? An examination of the effects of single unit contamination on grid cell firing fields.",
      "abstract": "Neural recording technology is improving rapidly, allowing for the detection of spikes from hundreds of cells simultaneously. The limiting step in multielectrode electrophysiology continues to be single cell isolation. However, this step is crucial to the interpretation of data from putative single neurons. We present here, in simulation, an illustration of possibly erroneous conclusions that may be reached when poorly isolated single cell data are analyzed. Grid cells are neurons recorded in rodents, and bats, that spike in equally spaced locations in a hexagonal pattern. One theory states that grid firing patterns arise from a combination of band firing patterns. However, we show here that summing the grid firing patterns of two poorly resolved neurons can result in spurious band-like patterns. Thus, evidence of neurons spiking in band patterns must undergo extreme scrutiny before it is accepted. Toward this aim, we discuss single cell isolation methods and metrics.",
      "publication_date": "2016-02-01",
      "venue": "Journal of Neurophysiology",
      "year": 2016,
      "citation_count": 21,
      "authors": [
        "Zaneta Navratilova",
        "Zaneta Navratilova",
        "Keith B. Godfrey",
        "B. McNaughton",
        "B. McNaughton"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "36ec1bf8a3a21321b7887caaeee9c6ee6cfc0db8",
      "title": "A predictive coding model of the N400",
      "abstract": "The N400 event-related component has been widely used to investigate the neural mechanisms underlying real-time language comprehension. However, despite decades of research, there is still no unifying theory that can explain both its temporal dynamics and functional properties. In this work, we show that predictive coding \u2013 a biologically plausible algorithm for approximating Bayesian inference \u2013 offers a promising framework for characterizing the N400. Using an implemented predictive coding computational model, we demonstrate how the N400 can be formalized as the lexico-semantic prediction error produced as the brain infers meaning from linguistic form of incoming words. We show that the magnitude of lexico-semantic prediction error mirrors the functional sensitivity of the N400 to various lexical variables, priming, contextual effects, as well as their higher-order interactions. We further show that the dynamics of the predictive coding algorithm provide a natural explanation for the temporal dynamics of the N400, and a biologically plausible link to neural activity. Together, these findings directly situate the N400 within the broader context of predictive coding research, and suggest that the brain may use the same computational mechanism for inference across linguistic and non-linguistic domains.",
      "publication_date": "2023-04-11",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "202c79bbb45ab6524141feacc81caacc4ba00401",
      "title": "Memory-augmented Dense Predictive Coding for Video Representation Learning",
      "abstract": "The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.",
      "publication_date": "2020-08-03",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "858549b00245aadc92f91a2540f01398f5f389ae",
      "title": "A solution to the learning dilemma for recurrent networks of spiking neurons",
      "abstract": "Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method\u2013called e-prop\u2013approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence. Bellec et al. present a mathematically founded approximation for gradient descent training of recurrent neural networks without backwards propagation in time. This enables biologically plausible training of spike-based neural network models with working memory and supports on-chip training of neuromorphic hardware.",
      "publication_date": "2019-08-19",
      "venue": "Nature Communications",
      "year": 2019,
      "citation_count": 466,
      "authors": [
        "G. Bellec",
        "Franz Scherr",
        "Anand Subramoney",
        "Elias Hajek",
        "Darjan Salaj",
        "R. Legenstein",
        "W. Maass"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "62f16717f8359d414513cc77ccdd363b508dae35",
      "title": "Brain-Inspired Spatial Representation Based on Grid Cells with Finite-Spacing Firing Field",
      "abstract": "Grid cell is a kind of important neuron cell related to spatial cognition and navigation in animal brain. It has a hexagonal firing field extending to the whole space. Grid cells exist in the form of modules, whose number is finite, and the grid cells between adjacent modules have discrete firing- field spacing with constant ratio. The existing models can simulate the constant ratio discrete of firing-field spacing between modules, but the finite characteristic of firing-field spacing cannot be simulated. We propose a recurrent attractor network model of grid cell, which can generate grid cells with finite firing-field spacing by analyzing the connections between grid cells in a single network. Then, the grid cells in different modules could possess discrete firing-field spacing with constant ratio through the competition among the recurrent networks. The recurrent attractor network model can provide reference for the construction of brain-inspired navigation system of unmanned platform.",
      "publication_date": "2019-11-01",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "554a5385b955e3be4a0621b643b476c0a5514d46",
      "title": "What Does the Anatomical Organization of the Entorhinal Cortex Tell Us?",
      "abstract": "The entorhinal cortex is commonly perceived as a major input and output structure of the hippocampal formation, entertaining the role of the nodal point of cortico-hippocampal circuits. Superficial layers receive convergent cortical information, which is relayed to structures in the hippocampus, and hippocampal output reaches deep layers of entorhinal cortex, that project back to the cortex. The finding of the grid cells in all layers and reports on interactions between deep and superficial layers indicate that this rather simplistic perception may be at fault. Therefore, an integrative approach on the entorhinal cortex, that takes into account recent additions to our knowledge database on entorhinal connectivity, is timely. We argue that layers in entorhinal cortex show different functional characteristics most likely not on the basis of strikingly different inputs or outputs, but much more likely on the basis of differences in intrinsic organization, combined with very specific sets of inputs. Here, we aim to summarize recent anatomical data supporting the notion that the traditional description of the entorhinal cortex as a layered input-output structure for the hippocampal formation does not give the deserved credit to what this structure might be contributing to the overall functions of cortico-hippocampal networks.",
      "publication_date": "2008-08-28",
      "venue": "Journal of Neural Transplantation and Plasticity",
      "year": 2008,
      "citation_count": 407,
      "authors": [
        "Cathrin B. Canto",
        "F. Wouterlood",
        "M. Witter"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "b227f3e4c0dc96e5ac5426b85485a70f2175a205",
      "title": "Representation Learning with Contrastive Predictive Coding",
      "abstract": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
      "publication_date": "2018-07-10",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9f6eb784ffc460cba56077b4b8739d4eb9115f18",
      "title": "StockFormer: Learning Hybrid Trading Machines with Predictive Coding",
      "abstract": "Typical RL-for-finance solutions directly optimize trading policies over the noisy market data, such as stock prices and trading volumes, without explicitly considering the future trends and correlations of different investment assets as we humans do. In this paper, we present StockFormer, a hybrid trading machine that integrates the forward modeling capabilities of predictive coding with the advantages of RL agents in policy flexibility. The predictive coding part consists of three Transformer branches with modified structures, which respectively extract effective latent states of long-/short-term future dynamics and asset relations. The RL agent adaptively fuses these states and then executes an actor-critic algorithm in the unified state space. The entire model is jointly trained by propagating the critic's gradients back to the predictive coding module. StockFormer significantly outperforms existing approaches across three publicly available financial datasets in terms of portfolio returns and Sharpe ratios.",
      "publication_date": "2023-08-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "917922f63a9920b940c05fa802deb3d7d83dd6b0",
      "title": "Using Convolutional Neural Networks to Build a Lightweight Flood Height Prediction Model with Grad-Cam for the Selection of Key Grid Cells in Radar Echo Maps",
      "abstract": "Recent climate change has brought extremely heavy rains and widescale flooding to many areas around the globe. However, previous flood prediction methods usually require a lot of computation to obtain the prediction results and impose a heavy burden on the unit cost of the prediction. This paper proposes the use of a deep learning model (DLM) to overcome these problems. We alleviated the high computational overhead of this approach by developing a novel framework for the construction of lightweight DLMs. The proposed scheme involves training a convolutional neural network (CNN) by using a radar echo map in conjunction with historical flood records at target sites and using Grad-Cam to extract key grid cells from these maps (representing regions with the greatest impact on flooding) for use as inputs in another DLM. Finally, we used real radar echo maps of five locations and the flood heights record to verify the validity of the method proposed in this paper. The experimental results show that our proposed lightweight model can achieve similar or even better prediction accuracy at all locations with only about 5~15% of the operation time and about 30~35% of the memory space of the CNN.",
      "publication_date": "2022-01-07",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9480a64d9520f4b8857ed291fd42db11e46594a7",
      "title": "Contrastive Difference Predictive Coding",
      "abstract": "Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \\times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20 \\times$ more sample efficient than the successor representation and $1500 \\times$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.",
      "publication_date": "2023-10-31",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c2228c6e59fdb4e63f2e873077f1690e16fa39f8",
      "title": "Dynamic Occupancy Grid Map with Semantic Information Using Deep Learning-Based BEVFusion Method with Camera and LiDAR Fusion",
      "abstract": "In the field of robotics and autonomous driving, dynamic occupancy grid maps (DOGMs) are typically used to represent the position and velocity information of objects. Although three-dimensional light detection and ranging (LiDAR) sensor-based DOGMs have been actively researched, they have limitations, as they cannot classify types of objects. Therefore, in this study, a deep learning-based camera\u2013LiDAR sensor fusion technique is employed as input to DOGMs. Consequently, not only the position and velocity information of objects but also their class information can be updated, expanding the application areas of DOGMs. Moreover, unclassified LiDAR point measurements contribute to the formation of a map of the surrounding environment, improving the reliability of perception by registering objects that were not classified by deep learning. To achieve this, we developed update rules on the basis of the Dempster\u2013Shafer evidence theory, incorporating class information and the uncertainty of objects occupying grid cells. Furthermore, we analyzed the accuracy of the velocity estimation using two update models. One assigns the occupancy probability only to the edges of the oriented bounding box, whereas the other assigns the occupancy probability to the entire area of the box. The performance of the developed perception technique is evaluated using the public nuScenes dataset. The developed DOGM with object class information will help autonomous vehicles to navigate in complex urban driving environments by providing them with rich information, such as the class and velocity of nearby obstacles.",
      "publication_date": "2024-04-29",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1cae417456711c4da184f5efcd1b7464a7a0661a",
      "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding",
      "abstract": "Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.",
      "publication_date": "2019-05-22",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "46e2af3d7e8dd50675b0062fe9e4fce5cd5a0508",
      "title": "Predictive and Adaptive Deep Coding for Wireless Image Transmission in Semantic Communication",
      "abstract": "Semantic communication is a newly emerged communication paradigm that exploits deep learning (DL) models to realize communication processes like source coding and channel coding. Recent advances have demonstrated that DL-based joint source-channel coding (DeepJSCC) can achieve exciting data compression and noise-resiliency performances for wireless image transmission tasks, especially in environments with low channel signal-to-noises (SNRs). However, existing DeepJSCC-based semantic communication frameworks still cannot achieve adaptive code rates for different channel SNRs and image contents, which reduces its flexibility and bandwidth efficiency. In this paper, we propose a predictive and adaptive deep coding (PADC) framework for realizing flexible code rate optimization with a given target transmission quality requirement. PADC is realized by a variable code length enabled DeepJSCC (DeepJSCC-V) model for realizing flexible code length adjustment, an Oracle Network (OraNet) model for predicting peak-signal-to-noise (PSNR) value for an image transmission task according to its contents, channel signal to noise ratio (SNR) and the compression ratio (CR) value, and a CR optimizer aims at finding the minimal data-level or instance-level CR with a PSNR quality constraint. By using the above three modules, PADC can transmit the image data with minimal CR, which greatly increases bandwidth efficiency. Simulation results demonstrate that the proposed DeepJSCC-V model can achieve similar PSNR performances compared with the state-of-the-art Attention-based DeepJSCC (ADJSCC) model, and the proposed OraNet model is able to predict high-quality PSNR values with an average error lower than 0.5dB. Results also demonstrate that the proposed PADC can use nearly minimal bandwidth consumption for wireless image transmission tasks with different channel SNR and image contents, at the same time guaranteeing the PSNR constraint for each image data.",
      "publication_date": "2023-08-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ae47b8bef62e47285f5acc7d81ea3e265171e2ef",
      "title": "The contributions of entorhinal cortex and hippocampus to error driven learning",
      "abstract": "Computational models proposed that the medial temporal lobe (MTL) contributes importantly to error-driven learning, though little direct in-vivo evidence for this hypothesis exists. To test this, we recorded in the entorhinal cortex (EC) and hippocampus (HPC) as macaques performed an associative learning task using an error-driven learning strategy, defined as better performance after error relative to correct trials. Error-detection signals were more prominent in the EC relative to HPC. Early in learning hippocampal but not EC neurons signaled error-driven learning by increasing their population stimulus-selectivity following error trials. This same pattern was not seen in another task where error-driven learning was not used. After learning, different populations of cells in both the EC and HPC signaled long-term memory of newly learned associations with enhanced stimulus-selective responses. These results suggest prominent but differential contributions of EC and HPC to learning from errors and a particularly important role of the EC in error-detection. Ku et al. recorded in the entorhinal cortex (EC) and hippocampus (HPC) of macaques during associative learning tasks in order to test the computational model prediction that they contribute to error-driven learning. They demonstrate that the EC and HPC have prominent but differential contributions to learning from errors, with the EC having a particularly prominent role in error-detection.",
      "publication_date": "2020-09-29",
      "venue": "Communications Biology",
      "year": 2020,
      "citation_count": 7,
      "authors": [
        "S-P Ku",
        "E. Hargreaves",
        "S. Wirth",
        "W. Suzuki"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3bc48861d66b2ee2968ed26c0d96c9b2e1ab06e8",
      "title": "Path integration maintains spatial periodicity of grid cell firing in a 1D circular track",
      "abstract": "Entorhinal grid cells are thought to provide a 2D spatial metric of the environment. In this study we demonstrate that in a familiar 1D circular track (i.e., a continuous space) grid cells display a novel 1D equidistant firing pattern based on integrated distance rather than travelled distance or time. In addition, field spacing is increased compared to a 2D open field, probably due to a reduced access to the visual cue in the track. This metrical modification is accompanied by a change in LFP theta oscillations, but no change in intrinsic grid cell rhythmicity, or firing activity of entorhinal speed and head-direction cells. These results suggest that in a 1D circular space grid cell spatial selectivity is shaped by path integration processes, while grid scale relies on external information. In an open field, the preferential firing of grid cells on a hexagonal lattice is formed by integrating external as well as self-motion cues. Here, the authors show that on a 1D circular track, path integration cues shape the spatial selectivity of grid cells while external cues determine the scale of the grid.",
      "publication_date": "2019-02-19",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "23e04389f8728a5736382d3662341a1a2a25e171",
      "title": "Predictive Coding or Just Feature Discovery? An Alternative Account of Why Language Models Fit Brain Data",
      "abstract": "Abstract Many recent studies have shown that representations drawn from neural network language models are extremely effective at predicting brain responses to natural language. But why do these models work so well? One proposed explanation is that language models and brains are similar because they have the same objective: to predict upcoming words before they are perceived. This explanation is attractive because it lends support to the popular theory of predictive coding. We provide several analyses that cast doubt on this claim. First, we show that the ability to predict future words does not uniquely (or even best) explain why some representations are a better match to the brain than others. Second, we show that within a language model, representations that are best at predicting future words are strictly worse brain models than other representations. Finally, we argue in favor of an alternative explanation for the success of language models in neuroscience: These models are effective at predicting brain responses because they generally capture a wide variety of linguistic phenomena.",
      "publication_date": "2022-11-03",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0174d263d3a77bf03fce831a9a5ce2678e1959f0",
      "title": "Video Representation Learning by Dense Predictive Coding",
      "abstract": "The objective of this paper is self-supervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We make three contributions: First, we introduce the Dense Predictive Coding (DPC) framework for self-supervised representation learning on videos. This learns a dense encoding of spatio-temporal blocks by recurrently predicting future representations; Second, we propose a curriculum training scheme to predict further into the future with progressively less temporal context. This encourages the model to only encode slowly varying spatial-temporal signals, therefore leading to semantic representations; Third, we evaluate the approach by first training the DPC model on the Kinetics-400 dataset with self-supervised learning, and then finetuning the representation on a downstream task, i.e. action recognition. With single stream (RGB only), DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet.",
      "publication_date": "2019-09-10",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "39201db4005cfccd9290bf96e1d2cb3132db1e16",
      "title": "Development of a Combined Maxwell's Equations and Magnetic Equivalent Circuit Solution for Induction Machines in Electric Vehicle Applications",
      "abstract": "This study introduces a novel analytical technique for analyzing the magnetic field in induction motors (IM). This method combines Maxwell's equations with the magnetic equivalent circuit (MEC) framework to determine flux densities in various regions of the motor. By integrating flux sources from Maxwell's equations into the MEC's network of reluctances, this method accurately considers the distribution of main and leakage magnetic fluxes within the motor. The approach significantly reduces analysis time while maintaining high accuracy and reliability when compared to numerical methods. Furthermore, it provides important motor features such as the radial and tangential components of the flux density in the air-gap, stator and rotor areas, as well as the induced voltage. Finally, to validate the accuracy of the proposed method, the analytical results are compared to the case that no leakage fluxes are included in the analytical model and to the finite-element method (FEM) in terms of computation time and accuracy. As such, this method is suggested to serve as a valuable analytical tool during the design process of IMs.",
      "publication_date": "2024-05-05",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2ddc7239d99b4b3c5c5407bb62c47b6d48ad8161",
      "title": "Armature Reaction Analysis and Suppression of Voice Coil Actuator Based on an Improved Magnetic Equivalent Circuit Model",
      "abstract": "The high thrust density voice coil actuator (VCA) is a key component for the deployers to determine the CubeSats\u2019 separation velocity in orbit directly. To improve the VCA's thrust density and power consumption, it is essential to analyze the armature reaction under heavy current work conditions. This article proposes a dynamic splitting method of the magnetic equivalent circuit (MEC) model for analyzing the armature reaction of the VCA. This MEC model can effectively reveal the interaction mechanism between the dynamic winding field and the permanent magnet (PM) field, which provides an approach to readily and quickly analyze the armature reaction. Based on the understanding of the mechanism, we found an effective way to suppress the armature reaction and propose a new VCA with double PM flux circuits. The electromagnetic performances of the new configuration are evaluated by comparing it with the conventional one. Both the theoretical and experimental results show that the new structure achieves lower magnetic saturation, lower armature reaction, and higher thrust density. A research prototype was constructed and experimentally tested to verify the theoretical results of the armature reaction, magnetic field, thrust, separation velocity, and other performances. The new VCA realizes a wide and accurate modulation of separation velocity for the CubeSats.",
      "publication_date": "2024-07-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e294c3cc0071c5cd34273455342b75182b621908",
      "title": "Analysis and Compensation of the Longitudinal End Effect in Variable Reluctance Linear Resolvers Using Magnetic Equivalent Circuit Model",
      "abstract": "This article focused on the longitudinal end effect (LEE) in linear variable reluctance (VR) resolvers. The LEE can significantly decrease the accuracy of the linear sensor. Therefore, a novel method based on a magnetic equivalent circuit (MEC) model is proposed for accurate modeling of this phenomenon and used in an optimization routine to suppress that. The LEE is modeled by adding three extra permeances into the conventional MEC model without increasing the complexity and simulation time. Besides, the method of calculating these permeances is described. After that, some techniques are presented to decrease the computational burden of the MEC model. Then, the model is used to compensate the LEE by optimizing the turn number of coils. Comparing the results of the proposed model and the finite-element method (FEM) shows that the deviation of the model in the prediction of different inductances is less than 5%, while the simulation time is about 31 times less than that of FEM. Besides, the comparison between the initial design and the optimal design shows the usefulness of the model in the optimization process, in which the accuracy of the sensor improved by 74.1%. Finally, experimental tests on the prototype sensor verify the success of the analysis.",
      "publication_date": "2023-09-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "569043747e04038c86b73c28f3d269d01e31d7ff",
      "title": "Magnetic Equivalent Circuit and Lagrange Interpolation Function Modeling of Induction Machines Under Broken Bar Faults",
      "abstract": "This article introduces a mesh-based magnetic equivalent circuit (MEC) modeling technique for induction machines (IMs) in healthy and broken rotor bars conditions. The MEC model is presented as a highly accurate and computationally efficient alternative to finite element (FE) models. By incorporating modifications to the air gap coupling method, including a new Lagrange interpolation function, and utilizing a harmonic MEC model, the accuracy of the solution is improved while reducing electrical and mechanical transients. Compared to experiments and 2-D FE models, this model achieves precise results for electromagnetic torque, rotational speed, and forces across various conditions. The Lagrange interpolation function forms the basis for the air gap coupling between stator and rotor flux densities. The results demonstrate the MEC model\u2019s exceptional accuracy in predicting speed oscillations, calculating forces, and analyzing current harmonics in faulty IMs. Furthermore, the MEC model performs over 30 times faster than the 2-D FE models.",
      "publication_date": "2024-03-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ed185db19b0b1025817bcde562b7e9b7b2ba1f7e",
      "title": "A Novel Modeling Technique via Coupled Magnetic Equivalent Circuit With Vector Hysteresis Characteristics of Laminated Steels",
      "abstract": "This paper proposes a method to include the anisotropic hysteresis characteristics of soft-magnetic laminated steels in the magnetic equivalent circuit (MEC) modeling. The loop-based MEC formulation is improved to handle the nonlinearity of the anisotropic magnetic hysteresis, including the dynamic classical eddy-current and excess fields. The developed MEC model is coupled with both the single-valued $B$-$H$ curve (SVC) in magnetostatic and the dynamic vector hysteresis model (VHM) in transient analysis. Results with a single elementary MEC element show that an alternating magnetic field in a single direction with a peak value smaller than 300 A/m causes a discrepancy of more than 10% between the magnetic flux densities calculated by the VHM and SVC at 50 and 200 Hz excitation frequencies. Moreover, the proposed modeling technique is verified experimentally using the laminated transformer core of TEAM problem 32. The induced voltage calculated by the MEC model with the VHM demonstrates a good agreement with the measurements, while the MEC model with the SVC calculates inaccurate voltage waveforms. Lastly, the total iron loss dissipated in the transformer's iron core is investigated to verify the proposed technique under different excitation levels and frequencies up to 500 Hz. It is observed that the proposed MEC model with the vector hysteresis characteristics of laminated steels is able to calculate the iron loss accurately, while the conventional single-valued curve method fails to estimate the iron loss.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e73959ee703c09e2b05531cf5ef1bff51598abe3",
      "title": "Bioelectrochemical enhancement of methane production from exhausted vine shoot fermentation broth by integration of MEC with anaerobic digestion",
      "abstract": "A microbial electrolysis cell integrated in an anaerobic digestion system (MEC-AD) is an efficient configuration to produce methane from an exhausted vine shoot fermentation broth (EVS). The cell worked in a single-chamber two-electrode configuration at an applied potential of 1\u00a0V with a feeding ratio of 30/70 (30% EVS to 70% synthetic medium). In addition, an identical cell operated in an open circuit was used as a control reactor. Experimental results showed similar behavior in terms of carbon removal (70\u201376%), while the specific averaged methane production from cycle 7 was more stable and higher in the connected cell (MECAD) compared with the unpolarized one (OCAD) accounting for 403.7\u2009\u00b1\u200933.6 L CH4\u00b7kg VS\u22121 and 121.3\u2009\u00b1\u200949.7 L CH4\u00b7kg VS\u22121, respectively. In addition, electrochemical impedance spectroscopy revealed that the electrical capacitance of the bioanode in MECAD was twice the capacitance shown by OCAD. The bacterial community in both cells was similar but a clear adaptation of Methanosarcina Archaea was exhibited in MECAD, which could explain the increased yields in CH4 production. In summary, the results reported here confirm the advantages of integrating MEC-AD for the treatment of real organic liquid waste instead of traditional AD treatment.",
      "publication_date": "2022-06-08",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "78c169ad1636ec9eeea8c02e5c26452272727ede",
      "title": "MEC: An Open-source Fine-grained Mapping Equivalence Checking Tool for FPGA",
      "abstract": "Technology mapping is an essential step in EDA flow. However, the function of the circuit may be changed after technology mapping, and equivalence checking (EC) based verification is highly necessary. The traditional EC method has significant time and resource constraints, making it only feasible to carry out at a coarse-grained level. To make it efficient for technology mapping, we propose a fine-grained method called MEC, which leverages a combination of two approaches to significantly reduce the time cost of verification. The local block verification approach performs fast verification and the global graph cover approach guarantees correctness. The proposed method is rigorously tested and compared to three EC tools, and the results show that MEC technique offers a substantial improvement in speed. MEC not only offers a faster and more efficient way of performing EC on technology mapping but also opens up new opportunities for more fine-grained verification in the future.",
      "publication_date": "2023-05-08",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5336257faf7eeaceb742b55f81d4154f89998e42",
      "title": "Mesh-Based 3D MEC Modeling of a Novel Hybrid Claw Pole Generator",
      "abstract": "A brushless parallel hybrid excitation claw pole generator (HECPG) is proposed for electric vehicle (EV) application. Permanent magnet (PM) excitation method can reduce the volume of the machine and improve the power density and efficiency. Moreover, the voltage regulation can be ensured by field excitation. The flux path of the proposed HECPG is complex, and it will take a long time for 3D finite element analysis (FEA) to process it. To reduce simulation time, the mathematical model of the generator is given by a mesh-based 3D magnet equivalent circuit (MEC) network method considering radial and axial flux, magnetic saturation, and magnetic flux leakage. The performance of the generator is analyzed by FEA and prototype experiment. Finally, the results of 3D MEC, FEA, and experiment are compared. There is little difference between the three results, so 3D MEC can ensure the accuracy and significantly reduce the simulation time. The efficiency of the proposed HECPG is 90%, and the DC-Bus voltage can be modulated by changing the amplitude of field current.",
      "publication_date": "2022-02-24",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5e2fdf42d985b3eb9eeb8387e1f7d093ade81525",
      "title": "Vector-based navigation using grid-like representations in artificial agents",
      "abstract": null,
      "publication_date": "2018-05-09",
      "venue": "Nature",
      "year": 2018,
      "citation_count": 612,
      "authors": [
        "Andrea Banino",
        "C. Barry",
        "Benigno Uria",
        "C. Blundell",
        "T. Lillicrap",
        "Piotr Wojciech Mirowski",
        "A. Pritzel",
        "M. Chadwick",
        "T. Degris",
        "Joseph Modayil",
        "Greg Wayne",
        "Hubert Soyer",
        "Fabio Viola",
        "Brian Zhang",
        "Ross Goroshin",
        "Neil C. Rabinowitz",
        "Razvan Pascanu",
        "Charlie Beattie",
        "Stig Petersen",
        "Amir Sadik",
        "Stephen Gaffney",
        "Helen King",
        "K. Kavukcuoglu",
        "D. Hassabis",
        "R. Hadsell",
        "D. Kumaran"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "fb3dee071c90c2e860cd4023f682f2b11b8955c1",
      "title": "3D Rotor Position-Dependant MEC Modelling of Different Claw Pole Machine Topologies",
      "abstract": "The paper is aimed at a 3D magnetic equivalent circuit (MEC)-based modelling of claw pole synchronous machine topologies. Beyond the magnetic saturation and the armature magnetic reaction, the proposed modelling approach takes into consideration the rotor position variation, yielding the so-called: rotor position-dependant MEC. Accounting for the complexity of the magnetic circuit of claw pole topologies, specific assumptions are adopted prior a general analytical derivation of their MEC models. The developed analytical approach focuses on the air gap reluctance under variable rotor position considering a simplified geometry of the claw. A dedicated numerical procedure based on the Newton-Raphson algorithm is proposed for the resolution of the designed rotor position-dependant MEC. The proposed approach is applied to three claw pole topologies. The two first ones are equipped with a single source of excitation achieved by a field. Their analytically-predicted features are validated by experiments. The third topology has a dual excitation achieved by a field and permanent magnets (PMs) in the rotor. Its analytically-predicted features are validated by 3D finite element analysis (FEA). It is found that both experimental and FEA results are in quite good agreement with the analytical predictions yielded by the proposed rotor position-dependant MEC.",
      "publication_date": null,
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1b34f47656f7d490bc433aeb0d4875f2247cb7d3",
      "title": "Improved Configuration Proposal for Axial Reluctance Resolver Using 3-D Magnetic Equivalent Circuit Model and Winding Function Approach",
      "abstract": "Resolver is a position sensor that is widely employed in motor servo systems under harsh working conditions. In this article, a novel axial flux resolver with a fractional-slot sinusoidal distributed winding configuration is developed, which has the advantages of a simple structure and convenient mass production. Sinusoidal distribution winding expands the optional range of pole\u2013slot combinations, and its harmonic suppression capability is proposed and verified by the winding function approach (WFA). Furthermore, the magnetic equivalent circuit (MEC) model is formulated to analyze the impact of structural parameters on the induced voltage envelope, through which the optimal parameters on the resolver are developed. In terms of computation time and accuracy, the results of the MEC model under healthy and eccentric conditions are compared with the finite-element method (FEM). In addition, the antiaxial offset capability of the half-wave resolver is illustrated and verified. The effectiveness and superiority of the proposed resolver are illustrated and verified based on numerical simulations and experimental tests.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "aa8ab6a377cdcc5f0614b8f8beb7f267782319ac",
      "title": "Control of hexagonal plate-like microstructure of in-situ calcium hexaluminate in monolithic refractories",
      "abstract": "ABSTRACT The self-toughening property of calcium hexaluminate (CaO\u20226Al2O3, CA6) in engineering ceramics causes a remarkable interest in the in-situ CA6 in monolithic refractory. The determination of significant factors which controls the formation and microstructure of CA6 in monolithic refractory is mainly discussed in this study. Samples were prepared by using calcium aluminate cement and sintered alumina and the chemical composition was fixed at the molar ratio of CaO: Al2O3 = 1:6. In order to evaluate the suitable sintering condition and SiO2 content for CA6 phase formation, the variation of firing temperature and holding time was first modified at 1400\u20131500\u00b0C for 1\u20135 h. The second factor was done by adding 2\u201310 mass% of SiO2 into the primary mixture. In comparison, the results were confirmed that high firing temperature at 1500\u00b0C provided the highest quantity of CA6 phase and plate-like microstructure. In addition, longer proceeding time contributed to the grain growth of CA6, especially, within 5 h of holding time. SiO2 importantly helped to enhance hexagonal plate-like microstructure due to the ability of ion mobility in low-melting phase of gehlenite but the suitable content of SiO2 should not exceed 2 mass% for the better control ability of CA6 formation and microstructure.",
      "publication_date": "2018-07-03",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5b66541e9cc6708be8f70405b6f51b1c41fa34dc",
      "title": "Features of formation of the celsian phase during firing of heat-resistant ceramics in the system BaO\u2013Al2O3\u2013SiO2",
      "abstract": "In the synthesis of heat-resistant ceramics in the system BaO\u2013Al2O3\u2013SiO2 (BAS), it is important to find ways of intensifying the process of transition of hexagonal celsian to monoclinic celsian without an increase in sintering temperature. Monoclinic form of celsian is characterized by higher thermal, electro-insulating, and mechanical properties. This paper deals with the features of formation of the phase composition of celsian ceramics when using BAS glass of eutectic composition and glass in the system Li2O\u2013Al2O3\u2013B2O3\u2013SiO2 (LABS) of spodumene composition as modifying components. It is shown that monoclinic celsian is the final crystalline phase formed in ceramics synthesized on the basis of barium carbonate and kaolin. Monoclinic celsian is formed stepwise; and the hexagonal celsian appears first. The complete transition hexagonal celsian\uf0aemonoclinic celsian occurs only in the process of high temperature firing at 12500C. Notably, the degree of ceramic sintering remains low (water absorption is 11.0%). Introduction of BAS glass contributes to the complete transition of hexagonal celsian to monoclinic celsian at a reduced temperature of 11000C. Maximum effect in the formation of monoclinic celsian is achieved by the introduction of LABS glass. As a result, the temperature of formation of this modification maximally decreases to 8000C. In this case, complete sintering of celsian ceramics is achieved at the temperature of 12500\u0421.",
      "publication_date": "2022-06-01",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ddf9626f10f4a5fd38e2332f1720c3f505783632",
      "title": "Influence of Firing Atmosphere on the Cubic-Hexagonal Transition and the Chemical State of Titanium in BaTiO3",
      "abstract": "The effect of reducing gas mixture, H2-N2 or CO-N2, on the cubic-hexagonal transition in BaTiO3 has been investigated by X-ray diffraction, ESR, ESCA, PAS and chemical analysis. The firing temperatures of BaTiO3 were 1380\u00b0C and 1500\u00b0C, and the transition temperature in air is already known to be 1460\u00b0C. The fractional conversion to the hexagonal phase increased with increasing concentrations of reducing gases, larger for samples fired in H2-N2 than those fired in CO-N2 for the same concentrations of the reducing gases. The hexagonal phase in all samples increased proportionally to the amount of Ti3+ produced by reduction of Ti4+ ions, regardless of the reducing gas species and firing temperatures. It was found that the minimum amount of Ti3+ ions to stabilize the hexagonal BaTiO3 at room temperature was 0.3% of total Ti ions. ESCA spectra due to Ti3+ were observed in the sample fired in 100% H2 at 1380\u00b0C.",
      "publication_date": null,
      "venue": "",
      "year": 1987,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4a5f9b6232a0ac08c09ef473c419e691295e8a23",
      "title": "Effect of Firing Atmosphere on the Cubic-Hexagonal Transition in Ba0.99Sr0.01TiO3",
      "abstract": "Ba0.99Sr0.01Tio3 ceramics were fired for 1 hour at a temperature from 1410\u00b0 to 1460\u00b0C in H2-N2 atmospheres. A pure hexagonal phase, which was not formed by calcination in air, was obtained by firing for 1 hour at 1430\u00b0C in H2 atmosphere. A part of the oxygen in Ba0.99Sr0.01TiO3 was removed under the reducing condition, which led to the formation of the hexagonal phase. The amount of the hexagonal phase of Ba0.99Sr0.01TiO3-x increased proportionally with an increase in oxygen deficiency (x). The value of x required for stabilizing the hexagonal phase at room temperature was found to be larger than 0.008. The transition temperature from cubic to hexagonal phase of Ba0.99Sr0.01TiO3-x was higher than that of BaTiO3-x, and oxygen deficiency required for stabilizing the hexagonal phase was larger than that for BaTiO3-x. These results indicated that Ba0.99Sr0.01TiO3-x does not easily transformed into the hexagonal phase.",
      "publication_date": "1990-08-01",
      "venue": "",
      "year": 1990,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "618e2830b9f1c67b265d25125a6b48520f1a4ac3",
      "title": "Utilizing an Efficient Magnetic Equivalent Circuit Model and Manifold Mapping Method for Two-Level Optimization of Axial Flux Machine",
      "abstract": "In this paper, a two-level optimization algorithm is developed based on rapid magnetic equivalent circuit (MEC) model and manifold mapping (MM) method. The performance of an axial flux permanent magnet machine (AFPM) is optimized using this approach. The rapid MEC model based on grid division exhibits significant computational efficiency, while reducing the influence of artificial factors in modeling process. Further, quantitative comparison of the error performance under different combinations of divisions are carried out. Subsequently, coarse and fine models are selected, respectively. The iterative process indicates that the modeling approach enables the coarse model to correctly follow the trends of fine model as input changes. As a result, the fast convergence of the MM is successfully achieved.",
      "publication_date": "2024-06-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9c1b89701dbf1bb6094536248c0948949012c63c",
      "title": "A Novel Nonlinear Magnetic Equivalent Circuit Model for Magnetic Flux Leakage System",
      "abstract": "To ensure efficient inspection using the magnetic flux leakage (MFL) method, generating a flux density near the saturation level within the tested material is essential. This requirement brings high flux density conditions in the system\u2019s pole regions. Hence, leakage flux within the slot is excessively triggered, leading to distortion of the defect signal. In this context, the system dimensions stand out as one of the most significant factors affecting the mentioned flux distributions. Therefore, various alternative solutions with different system dimensions arise in the design process of the MFL system. This study proposes a magnetic equivalent circuit (MEC) model to achieve optimal system design. The proposed MEC model is designed considering the nonlinear behavior of the material, leakage flux, and fringing effects. Verification results demonstrate that the MEC model consistently tracks the finite element analysis (FEA) results in calculating the flux densities. Furthermore, the relative errors in the flux density calculations of the tested material are at a maximum level of 10.2% and an average of 5.2% compared to the FEA. These findings indicate that the proposed MEC model can be effectively utilized in rapid prototyping and optimization procedures of MFL system design by providing fast solutions with reasonable accuracy.",
      "publication_date": "2024-05-10",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ab7b9e504d427abbe7798e441442566ed7ea967a",
      "title": "Joint Computation Offloading and Radio Resource Allocation in MEC-Based Wireless-Powered Backscatter Communication Networks",
      "abstract": "The multi-access edge computing (MEC)-based wireless-powered backscatter communication networks (WP-BackComNets) allow wireless devices (WDs) to offload computation resources to lightweight and widely deployed MEC servers with the assistance of backscatter devices (BDs), which have substantial application prospects for the emerging Internet-of-Things applications. However, the limited battery capacity of WDs is one of the bottlenecks restricting its further development. Reducing the energy consumption and the computation burden of WDs while ensuring the quality-of-service requirements is an urgent issue. To this end, a joint computation offloading and radio resource allocation problem is formulated to minimize the total energy consumption of WDs for an MEC-based WP-BackComNet by jointly optimizing user association, the transmit power and transmission time of WDs, the computational offloading coefficient of each task, and the reflection coefficients of BDs, where the circuit power consumption of BDs, the computational capabilities of WDs, and the task execution delay budgets are considered. To handle this non-convex problem, we propose an efficient algorithm to obtain a suboptimal solution. Simulation results demonstrate that the proposed scheme can effectively decrease the energy consumption compared with the benchmarks.",
      "publication_date": "2021-05-03",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4771f1a1f4e723e7efdc1b7747b00ff17b26ec6e",
      "title": "Two-Segment Magnet Transverse Flux Ferrite PM Generator for Direct-Drive Wind Turbine Applications: Nonlinear 3-D MEC Modeling and Experimental Validation",
      "abstract": "Transverse flux PM generator (TFPMG) is a capable option for direct-drive wind turbine (DDWT) applications due to its high-power characteristics at low speeds. NdFeB-based TFPMGs may suffer from a higher total cost and lower thermal capabilities compared to the ferrite-based TFPMGs. Not yet covered in the existing literature, in this paper a transverse flux ferrite PM generator (TFFPMG) is proposed, designed, and modeled, which can also resolve the unipolar flux generation and even-order harmonics in the flux linkage of the conventional TFPMGs that occurred in conventional TFPMGs through its innovative two-segment trapezoidal shape magnet structure. Due to the 3-D flux path nature in the proposed TFFPMG, either 3-D finite element analysis (FEA) or 3-D magnetic equivalent circuit (MEC) modeling should be used. As a computationally efficient modeling method, a nonlinear 3-D MEC is established to model the entire structure of the TFFPMG while the core saturation and nonlinear permeances are also fully considered to improve the modeling accuracy. The electromagnetic performance of the proposed TFFPMG modeled by 3-D MEC in various loading conditions is validated by both 3-D FEA and experimental results using a 2.5 kW TFFPMG. A close agreement between 3-D MEC modeling predictions, 3-D FEA, and test results confirms the reliability of the MEC modeling for the proposed TFFPMG.",
      "publication_date": "2022-09-01",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "57eb4d347212a9123ff2cfc17839ccd231f6b4b9",
      "title": "Computational Models of Grid Cells",
      "abstract": null,
      "publication_date": "2011-08-25",
      "venue": "Neuron",
      "year": 2011,
      "citation_count": 218,
      "authors": [
        "Lisa M. Giocomo",
        "M. Moser",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "fad77fa5f1b84eac354b6d6f7e011259092ce87d",
      "title": "What do grid cells contribute to place cell firing?",
      "abstract": null,
      "publication_date": "2014-03-01",
      "venue": "Trends in Neurosciences",
      "year": 2014,
      "citation_count": 151,
      "authors": [
        "D. Bush",
        "C. Barry",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2495e0b51367beb828d68abbd00999b4bcdf3c44",
      "title": "Evidence for grid cells in a human memory network",
      "abstract": null,
      "publication_date": "2010-02-04",
      "venue": "Nature",
      "year": 2010,
      "citation_count": 706,
      "authors": [
        "Christian F. Doeller",
        "C. Barry",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "84c888d94d93137b767ec982c58e6829b10ed88a",
      "title": "Grid cells in mice",
      "abstract": null,
      "publication_date": "2008-12-01",
      "venue": "Hippocampus",
      "year": 2008,
      "citation_count": 174,
      "authors": [
        "M. Fyhn",
        "T. Hafting",
        "M. Witter",
        "E. Moser",
        "M. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "b0004097eac896ee4bf8b45ba3a7d9f101782542",
      "title": "Microstructure of a spatial map in the entorhinal cortex",
      "abstract": null,
      "publication_date": "2005-08-11",
      "venue": "Nature",
      "year": 2005,
      "citation_count": 3671,
      "authors": [
        "T. Hafting",
        "M. Fyhn",
        "S. Molden",
        "M. Moser",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3edd190cc7d541f6ae410e5be3d82868532a39f6",
      "title": "Development of the Spatial Representation System in the Rat",
      "abstract": null,
      "publication_date": "2010-06-18",
      "venue": "Science",
      "year": 2010,
      "citation_count": 600,
      "authors": [
        "R. Langston",
        "J. Ainge",
        "J. J. Couey",
        "Cathrin B. Canto",
        "T. Bjerknes",
        "M. Witter",
        "E. Moser",
        "M. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6aa68970ec381a78a75fc30555884d4e8f1cb672",
      "title": "Place units in the hippocampus of the freely moving rat",
      "abstract": null,
      "publication_date": "1976-12-31",
      "venue": "Experimental Neurology",
      "year": 1976,
      "citation_count": 1807,
      "authors": [
        "J. O\u2019Keefe"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6ea99443ff4d599ee67b42a5afd5c7bf6729c2c5",
      "title": "Neural Representations of Location Composed of Spatially Periodic Bands",
      "abstract": null,
      "publication_date": "2012-08-17",
      "venue": "Science",
      "year": 2012,
      "citation_count": 175,
      "authors": [
        "J. Krupic",
        "N. Burgess",
        "J. O\u2019Keefe"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "de76cd6c61199445e2688402cabbb3119dadc877",
      "title": "Dual phase and rate coding in hippocampal place cells: Theoretical significance and relationship to entorhinal grid cells",
      "abstract": null,
      "publication_date": null,
      "venue": "Hippocampus",
      "year": 2005,
      "citation_count": 521,
      "authors": [
        "J. O\u2019Keefe",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "14a90fb87242fd7033eca54671f9f1f7c09c05e1",
      "title": "Grid cell firing may arise from interference of theta frequency membrane potential oscillations in single neurons",
      "abstract": null,
      "publication_date": "2007-12-01",
      "venue": "Hippocampus",
      "year": 2007,
      "citation_count": 265,
      "authors": [
        "M. Hasselmo",
        "Lisa M. Giocomo",
        "Eric A. Zilli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "72823437d781996a412428b097b4b562fe846716",
      "title": "Prediction and memory: A predictive coding account",
      "abstract": null,
      "publication_date": "2020-05-21",
      "venue": "Progress in neurobiology",
      "year": 2020,
      "citation_count": 144,
      "authors": [
        "Helen C. Barron",
        "R. Auksztulewicz",
        "Karl J. Friston"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "57070167d647bfa23bc5c5895623af13576db66b",
      "title": "A tutorial on the free-energy framework for modelling perception and learning",
      "abstract": null,
      "publication_date": "2017-02-01",
      "venue": "Journal of Mathematical Psychology",
      "year": 2017,
      "citation_count": 269,
      "authors": [
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "198acc8504895b1ba3211ea3f1e8fe0cf106b753",
      "title": "Impression learning: Online representation learning with synaptic plasticity",
      "abstract": null,
      "publication_date": null,
      "venue": "Neural Information Processing Systems",
      "year": 2021,
      "citation_count": 9,
      "authors": [
        "C. Bredenberg",
        "Benjamin Lyo",
        "Eero P. Simoncelli",
        "Cristina Savin"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "28c3c3d07d27b854babfac7b109ba15fca09437c",
      "title": "Evidence for area CA1 as a match/mismatch detector: A high\u2010resolution fMRI study of the human hippocampus",
      "abstract": null,
      "publication_date": "2012-03-01",
      "venue": "Hippocampus",
      "year": 2012,
      "citation_count": 238,
      "authors": [
        "Katherine Duncan",
        "Nicholas A. Ketz",
        "S. Inati",
        "L. Davachi"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8fcbc38e7196b0dc8748f04cd6101e71f92c158e",
      "title": "A theory of cortical responses",
      "abstract": null,
      "publication_date": "2005-04-29",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences",
      "year": 2005,
      "citation_count": 3972,
      "authors": [
        "Karl J. Friston"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "252b2d7ab977ed1d1dfc659294318bb2394410d7",
      "title": "Basket-like interneurones in layer II of the entorhinal cortex exhibit a powerful NMDA-mediated synaptic excitation",
      "abstract": null,
      "publication_date": "1993-01-04",
      "venue": "Neuroscience Letters",
      "year": 1993,
      "citation_count": 185,
      "authors": [
        "R. Jones",
        "E. H. B\u00fchl"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1c9852b8cbeb0668cfd5e3aeef01f9ce9a123843",
      "title": "The emergence of grid cells: Intelligent design or just adaptation?",
      "abstract": null,
      "publication_date": "2008-12-01",
      "venue": "Hippocampus",
      "year": 2008,
      "citation_count": 245,
      "authors": [
        "E. Kropff",
        "A. Treves"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "42ce761c85bdb0d422917b03751ab9cbc72a3417",
      "title": "Backpropagation through time and the brain",
      "abstract": null,
      "publication_date": "2019-03-07",
      "venue": "Current Opinion in Neurobiology",
      "year": 2019,
      "citation_count": 127,
      "authors": [
        "T. Lillicrap",
        "Adam Santoro"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "c04713059cfdbff7f3cce0b10a5fd6fc7cdee43e",
      "title": "Relating Hippocampal Circuitry to Function Recall of Memory Sequences by Reciprocal Dentate\u2013CA3 Interactions",
      "abstract": null,
      "publication_date": "1999-02-01",
      "venue": "Neuron",
      "year": 1999,
      "citation_count": 632,
      "authors": [
        "J. Lisman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
      "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
      "abstract": null,
      "publication_date": "1996-06-13",
      "venue": "Nature",
      "year": 1996,
      "citation_count": 6169,
      "authors": [
        "B. Olshausen",
        "D. Field"
      ],
      "novel": null,
      "cited_paper": true
    }
  ],
  "purpose_ranking": [
    {
      "paper_id": "c1585013b1dbb7341a8a6821159d8d7681e81503",
      "title": "Predictive grid coding in the medial entorhinal cortex",
      "abstract": "The entorhinal cortex represents allocentric spatial geometry and egocentric speed and heading information required for spatial navigation. However, it remains unclear whether it contributes to the prediction of an animal\u2019s future location. We discovered grid cells in the medial entorhinal cortex (MEC) that have grid fields representing future locations during goal-directed behavior. These predictive grid cells represented prospective spatial information by shifting their grid fields against the direction of travel. Predictive grid cells discharged at the trough phases of the hippocampal CA1 theta oscillation and, together with other types of grid cells, organized sequences of the trajectory from the current to future positions across each theta cycle. Our results suggest that the MEC provides a predictive map that supports forward planning in spatial navigation. Editor\u2019s summary Grid cells in the entorhinal cortex create a coordinate system of the environment for spatial navigation. However, it is not clear whether the entorhinal grid system is also involved in predicting where an animal will be in the next moment. Ouchi and Fujisawa performed high-density neuronal recordings in the entorhinal cortex and hippocampal area CA1 of rats during goal-directed behavior in an open field. They observed neurons in layer 3 of the medial entorhinal cortex that explicitly encode a grid representation of an animal\u2019s future projected location, not the current position, and named them \u201c\u201cpredictive grid cells.\u201d Neuronal assemblies in the medial entorhinal cortex thus organize a predictive cognitive map. \u2014Peter Stern",
      "publication_date": "2024-08-16",
      "venue": "Science",
      "year": 2024,
      "citation_count": 7,
      "authors": [
        "Ayako Ouchi",
        "Shigeyoshi Fujisawa"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "cd98854d3482e276b7124712c02e20ed48f56a0f",
      "title": "A generative model of the hippocampal formation trained with theta driven local learning rules",
      "abstract": "Advances in generative models have recently revolutionised machine learning. Meanwhile, in neuroscience, generative models have long been thought fundamental to animal intelligence. Understanding the biological mechanisms that support these processes promises to shed light on the relationship between biological and artificial intelligence. In animals, the hippocampal formation is thought to learn and use a generative model to support its role in spatial and non-spatial memory. Here we introduce a biologically plausible model of the hippocampal formation tantamount to a Helmholtz machine that we apply to a temporal stream of inputs. A novel component of our model is that fast theta-band oscillations (5-10 Hz) gate the direction of information flow throughout the network, training it akin to a high-frequency wake-sleep algorithm. Our model accurately infers the latent state of high-dimensional sensory environments and generates realistic sensory predictions. Furthermore, it can learn to path integrate by developing a ring attractor connectivity structure matching previous theoretical proposals and flexibly transfer this structure between environments. Whereas many models trade-off biological plausibility with generality, our model captures a variety of hippocampal cognitive functions under one biologically plausible local learning rule.",
      "publication_date": "2023-12-13",
      "venue": "bioRxiv",
      "year": 2023,
      "citation_count": 9,
      "authors": [
        "Tom M George",
        "C. Barry",
        "K. Stachenfeld",
        "C. Clopath",
        "T. Fukai"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2510c7bcbd6278b50dc73e02d127546ef70141bd",
      "title": "Learning an efficient place cell map from grid cells using non-negative sparse coding",
      "abstract": "Experimental studies of grid cells in the Medial Entorhinal Cortex (MEC) have shown that they are selective to an array of spatial locations in the environment that form a hexagonal grid. However, in a small environment, place cells in the hippocampus are only selective to a single-location of the environment while granule cells in the dentate gyrus of the hippocampus have multiple discrete firing locations, but lack spatial periodicity. Given the anatomical connection from MEC to the hippocampus, previous feedforward models of grid-to-place have been proposed. Here, we propose a unified learning model that can describe the spatial tuning properties of both hippocampal place cells and dentate gyrus granule cells based on non-negative sparse coding. Sparse coding plays an important role in many cortical areas and is proposed here to have a key role in the navigational system of the brain in the hippocampus. Our results show that the hexagonal patterns of grid cells with various orientations, grid spacings and phases are necessary for model cells to learn a single spatial field that efficiently tile the entire spatial environment. However, if there is a lack of diversity in any grid parameters or a lack of cells in the network, this will lead to the emergence of place cells that have multiple firing locations. More surprisingly, the model shows that place cells can also emerge even when non-negative sparse coding is used with weakly-tuned MEC cells, instead of MEC grid cells, as the input to place cells. This work suggests that sparse coding may be one of the underlying organizing principles for the navigational system of the brain.",
      "publication_date": "2020-08-13",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "bc7417738a7f7186ce9b06933072c4ddd39ce4b1",
      "title": "Actionable Neural Representations: Grid Cells from Minimal Constraints",
      "abstract": "To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won't have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an `actionable representation'. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode.",
      "publication_date": "2022-09-30",
      "venue": "International Conference on Learning Representations",
      "year": 2022,
      "citation_count": 23,
      "authors": [
        "W. Dorrell",
        "P. Latham",
        "T. Behrens",
        "James C. R. Whittington"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d40cb3f74f7e4a0858bc63760ec983f15de8125c",
      "title": "Grid-Cell Activity on Linear Tracks Indicates Purely Translational Remapping of 2D Firing Patterns at Movement Turning Points",
      "abstract": "Grid cells in rodent medial entorhinal cortex are thought to play a critical role for spatial navigation. When the animal is freely moving in an open arena the firing fields of each grid cell tend to form a hexagonal lattice spanning the environment. For movements along a linear track the cells seem to respond differently. They show multiple firing fields that are not periodically arranged and whose shape and position change when the running direction is reversed. In addition, peak firing rates vary widely from field to field. Measured along one running direction only, firing fields are, however, compatible with a slice through a two-dimensional (2D) hexagonal pattern. It is an open question, whether this is also true if leftward and rightward runs are jointly considered. By analyzing data from 15 male Long\u2013Evans rats, we show that a single hexagonal firing pattern explains the linear-track data if translational shifts of the pattern are allowed at the movement turning points. A rotation or scaling of the grid is not required. The agreement is further improved if the peak firing rates of the underlying 2D grid fields can vary from field to field, as suggested by recent studies. These findings have direct consequences for experiments using linear tracks in virtual reality. SIGNIFICANCE STATEMENT Various types of neurons support spatial navigation. Their response properties are often studied in reduced settings and might change when the animal can freely explore its environment. Grid cells in rodents, for example, exhibit seemingly irregular firing fields when animal movement is restricted to a linear track but highly regular patterns in two-dimensional (2D) arenas. We show that linear-track responses of a cell for both leftward and rightward running directions can be explained as cuts through a single hexagonal pattern if translational remapping is allowed at movement turning points; neither rotations nor scale transformations are needed. These results provide a basis to quantify grid-cell activity in 1D virtual reality and could help to detect and categorize grid cells without experiments in 2D environments.",
      "publication_date": "2018-07-05",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "a538579ac50d659ac0bca9824d6446e741c586b3",
      "title": "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization",
      "abstract": "Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits.",
      "publication_date": "2018-02-15",
      "venue": "International Conference on Learning Representations",
      "year": 2018,
      "citation_count": 216,
      "authors": [
        "Christopher J. Cueva",
        "Xue-Xin Wei"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "de629192ca5b0b7195355149dad23bd4124bf3df",
      "title": "Irregular distribution of grid cell firing fields in rats exploring a 3D volumetric space",
      "abstract": "We investigated how entorhinal grid cells encode volumetric space. On a horizontal surface, grid cells usually produce multiple, spatially focal, approximately circular firing fields that are evenly sized and spaced to form a regular, close-packed, hexagonal array. This spatial regularity has been suggested to underlie navigational computations. In three dimensions, theoretically the equivalent firing pattern would be a regular, hexagonal close packing of evenly sized spherical fields. In the present study, we report that, in rats foraging in a cubic lattice, grid cells maintained normal temporal firing characteristics and produced spatially stable firing fields. However, although most grid fields were ellipsoid, they were sparser, larger, more variably sized and irregularly arranged, even when only fields abutting the lower surface (equivalent to the floor) were considered. Thus, grid self-organization is shaped by the environment\u2019s structure and/or movement affordances, and grids may not need to be regular to support spatial computations. Grieves et al. show that when rats explore a 3D space, grid cells in the entorhinal cortex exchange their usual spatially regular firing patterns for more irregular ones, suggesting that 3D space is mapped differently than previously thought.",
      "publication_date": "2021-08-11",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "138a75cab26100df70bdb66dd1a28e68612db8ff",
      "title": "Learning place cells, grid cells and invariances with excitatory and inhibitory plasticity",
      "abstract": "Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction, and the underlying circuit mechanisms are not yet resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, place cells are typically invariant to head direction. We propose that all observed spatial tuning patterns \u2013 in both their selectivity and their invariance \u2013 arise from the same mechanism: Excitatory and inhibitory synaptic plasticity driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. Our proposed model is robust to changes in parameters, develops patterns on behavioral timescales and makes distinctive experimental predictions.",
      "publication_date": "2018-02-21",
      "venue": "eLife",
      "year": 2018,
      "citation_count": 50,
      "authors": [
        "Simon Nikolaus Weber",
        "Henning Sprekeler"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "20d6f4241af85a9c4fe5f0fcb583c818441282c4",
      "title": "Learning place cells, grid cells and invariances: A unifying model",
      "abstract": "Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction. The underlying circuit mechanisms are not fully resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, a place cell is highly selective to location, but typically invariant to head direction. Here, we propose that all observed spatial tuning patterns \u2013 in both their selectivity and their invariance \u2013 are a consequence of the same mechanism: Excitatory and inhibitory synaptic plasticity that is driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. The model is robust to changes in parameters, develops patterns on behavioral time scales and makes distinctive experimental predictions. Our results suggest that the interaction of excitatory and inhibitory plasticity is a general principle for the formation of neural representations.",
      "publication_date": "2017-02-17",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2bf3295a0cec7163c063c0d3967cc0935396bb3e",
      "title": "Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis",
      "abstract": "Many recent models study the downstream projection from grid cells to place cells, while recent data have pointed out the importance of the feedback projection. We thus asked how grid cells are affected by the nature of the input from the place cells. We propose a single-layer neural network with feedforward weights connecting place-like input cells to grid cell outputs. Place-to-grid weights are learned via a generalized Hebbian rule. The architecture of this network highly resembles neural networks used to perform Principal Component Analysis (PCA). Both numerical results and analytic considerations indicate that if the components of the feedforward neural network are non-negative, the output converges to a hexagonal lattice. Without the non-negativity constraint, the output converges to a square lattice. Consistent with experiments, grid spacing ratio between the first two consecutive modules is \u22121.4. Our results express a possible linkage between place cell to grid cell interactions and PCA. DOI: http://dx.doi.org/10.7554/eLife.10094.001",
      "publication_date": "2016-03-08",
      "venue": "eLife",
      "year": 2016,
      "citation_count": 147,
      "authors": [
        "Yedidyah Dordek",
        "Daniel Soudry",
        "R. Meir",
        "D. Derdikman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "d09c79f6b7420c8c8a28ecab0836ec9bacb8f8cf",
      "title": "A non-spatial account of place and grid cells based on clustering models of concept learning",
      "abstract": "One view is that conceptual knowledge is organized using the circuitry in the medial temporal lobe (MTL) that supports spatial processing and navigation. In contrast, we find that a domain-general learning algorithm explains key findings in both spatial and conceptual domains. When the clustering model is applied to spatial navigation tasks, so-called place and grid cell-like representations emerge because of the relatively uniform distribution of possible inputs in these tasks. The same mechanism applied to conceptual tasks, where the overall space can be higher-dimensional and sampling sparser, leading to representations more aligned with human conceptual knowledge. Although the types of memory supported by the MTL are superficially dissimilar, the information processing steps appear shared. Our account suggests that the MTL uses a general-purpose algorithm to learn and organize context-relevant information in a useful format, rather than relying on navigation-specific neural circuitry. Spatial maps in the medial temporal lobe (MTL) have been proposed to map abstract conceptual knowledge. Rather than grounding abstract knowledge in a spatial map, the authors propose a general-purpose clustering algorithm that explains how both spatial (including place and grid cells) and higher-dimensional conceptual representations arise during learning.",
      "publication_date": "2018-09-19",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6a77b5266b492c69dec62163cd692f86b9b72860",
      "title": "Emergent elasticity in the neural code for space",
      "abstract": "Significance We develop a theoretical model, grounded in known properties of neural dynamics and synaptic plasticity, that can fuse information gathered from the past history of velocity and sequence of encountered landmarks during exploratory behavior, to construct a self-consistent internal representation of space. Moreover, through model reduction techniques, we obtain conceptual insights into how consistent internal spatial representations naturally emerge through an elastic relaxation process in an effective spring\u2013particle system. We verify several experimentally testable predictions of our model involving the spatial behavior of grid cells in the medial entorhinal cortex, as well as suggest additional experiments. Upon encountering a novel environment, an animal must construct a consistent environmental map, as well as an internal estimate of its position within that map, by combining information from two distinct sources: self-motion cues and sensory landmark cues. How do known aspects of neural circuit dynamics and synaptic plasticity conspire to accomplish this feat? Here we show analytically how a neural attractor model that combines path integration of self-motion cues with Hebbian plasticity in synaptic weights from landmark cells can self-organize a consistent map of space as the animal explores an environment. Intriguingly, the emergence of this map can be understood as an elastic relaxation process between landmark cells mediated by the attractor network. Moreover, our model makes several experimentally testable predictions, including (i) systematic path-dependent shifts in the firing fields of grid cells toward the most recently encountered landmark, even in a fully learned environment; (ii) systematic deformations in the firing fields of grid cells in irregular environments, akin to elastic deformations of solids forced into irregular containers; and (iii) the creation of topological defects in grid cell firing patterns through specific environmental manipulations. Taken together, our results conceptually link known aspects of neurons and synapses to an emergent solution of a fundamental computational problem in navigation, while providing a unified account of disparate experimental observations.",
      "publication_date": "2018-05-21",
      "venue": "Proceedings of the National Academy of Sciences of the United States of America",
      "year": 2018,
      "citation_count": 70,
      "authors": [
        "Samuel A. Ocko",
        "Kiah Hardcastle",
        "Lisa M. Giocomo",
        "S. Ganguli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "da741827ab6f5bf9762d30b91fd2409ac9e0e272",
      "title": "World models and predictive coding for cognitive and developmental robotics: frontiers and challenges",
      "abstract": "ABSTRACT Creating autonomous robots that can actively explore the environment, acquire knowledge and learn skills continuously is the ultimate achievement envisioned in cognitive and developmental robotics. Importantly, if the aim is to create robots that can continuously develop through interactions with their environment, their learning processes should be based on interactions with their physical and social world in the manner of human learning and cognitive development. Based on this context, in this paper, we focus on the two concepts of world models and predictive coding. Recently, world models have attracted renewed attention as a topic of considerable interest in artificial intelligence. Cognitive systems learn world models to better predict future sensory observations and optimize their policies, i.e. controllers. Alternatively, in neuroscience, predictive coding proposes that the brain continuously predicts its inputs and adapts to model its own dynamics and control behavior in its environment. Both ideas may be considered as underpinning the cognitive development of robots and humans capable of continual or lifelong learning. Although many studies have been conducted on predictive coding in cognitive robotics and neurorobotics, the relationship between world model-based approaches in AI and predictive coding in robotics has rarely been discussed. Therefore, in this paper, we clarify the definitions, relationships, and status of current research on these topics, as well as missing pieces of world models and predictive coding in conjunction with crucially related concepts such as the free-energy principle and active inference in the context of cognitive and developmental robotics. Furthermore, we outline the frontiers and challenges involved in world models and predictive coding toward the further integration of AI and robotics, as well as the creation of robots with real cognitive and developmental capabilities in the future. GRAPHICAL ABSTRACT",
      "publication_date": "2023-01-14",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "8a8f7953894fef09d32c5f35fd79cfbff025e56f",
      "title": "Sequential Memory with Temporal Predictive Coding",
      "abstract": "Forming accurate memory of sequential stimuli is a fundamental function of biological agents. However, the computational mechanism underlying sequential memory in the brain remains unclear. Inspired by neuroscience theories and recent successes in applying predictive coding (PC) to static memory tasks, in this work we propose a novel PC-based model for sequential memory, called temporal predictive coding (tPC). We show that our tPC models can memorize and retrieve sequential inputs accurately with a biologically plausible neural implementation. Importantly, our analytical study reveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN) with an implicit statistical whitening process, which leads to more stable performance in sequential memory tasks of structured inputs. Moreover, we find that tPC exhibits properties consistent with behavioral observations and theories in neuroscience, thereby strengthening its biological relevance. Our work establishes a possible computational mechanism underlying sequential memory in the brain that can also be theoretically interpreted using existing memory model frameworks.",
      "publication_date": "2023-05-19",
      "venue": "Neural Information Processing Systems",
      "year": 2023,
      "citation_count": 19,
      "authors": [
        "Mufeng Tang",
        "Helen C. Barron",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "4af94a163a695f14597ac101a9562a68772010ae",
      "title": "Probabilistic Learning by Rodent Grid Cells",
      "abstract": "Mounting evidence shows mammalian brains are probabilistic computers, but the specific cells involved remain elusive. Parallel research suggests that grid cells of the mammalian hippocampal formation are fundamental to spatial cognition but their diverse response properties still defy explanation. No plausible model exists which explains stable grids in darkness for twenty minutes or longer, despite being one of the first results ever published on grid cells. Similarly, no current explanation can tie together grid fragmentation and grid rescaling, which show very different forms of flexibility in grid responses when the environment is varied. Other properties such as attractor dynamics and grid anisotropy seem to be at odds with one another unless additional properties are assumed such as a varying velocity gain. Modelling efforts have largely ignored the breadth of response patterns, while also failing to account for the disastrous effects of sensory noise during spatial learning and recall, especially in darkness. Here, published electrophysiological evidence from a range of experiments are reinterpreted using a novel probabilistic learning model, which shows that grid cell responses are accurately predicted by a probabilistic learning process. Diverse response properties of probabilistic grid cells are statistically indistinguishable from rat grid cells across key manipulations. A simple coherent set of probabilistic computations explains stable grid fields in darkness, partial grid rescaling in resized arenas, low-dimensional attractor grid cell dynamics, and grid fragmentation in hairpin mazes. The same computations also reconcile oscillatory dynamics at the single cell level with attractor dynamics at the cell ensemble level. Additionally, a clear functional role for boundary cells is proposed for spatial learning. These findings provide a parsimonious and unified explanation of grid cell function, and implicate grid cells as an accessible neuronal population readout of a set of probabilistic spatial computations.",
      "publication_date": "2016-10-01",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "27bc20a95969cffa29065fcc1beb63ee2386b852",
      "title": "Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells",
      "abstract": "To solve the spatial problems of mapping, localization and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learnt this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, there is no satisfactory synthesis of core principles that lead to multi-modular grid cells in deep recurrent neural networks. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: coding theory, dynamical systems, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, without access to supervised position information or engineering of particular readout representations as needed in previous approaches. We show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks and emergent representations generalize well outside their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.",
      "publication_date": "2023-11-04",
      "venue": "Neural Information Processing Systems",
      "year": 2023,
      "citation_count": 20,
      "authors": [
        "Rylan Schaeffer",
        "Mikail Khona",
        "Tzuhsuan Ma",
        "Cristobal Eyzaguirre",
        "Sanmi Koyejo",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "791686d4fa91082e225797a13060adeeb0fddf13",
      "title": "A unified theory for the computational and mechanistic origins of grid cells",
      "abstract": "The discovery of entorhinal grid cells has generated considerable interest in how and why hexagonal firing fields might mechanistically emerge in a generic manner from neural circuits, and what their computational significance might be. Here we forge an intimate link between the computational problem of path-integration and the existence of hexagonal grids, by demonstrating that such grids arise generically in biologically plausible neural networks trained to path integrate. Moreover, we develop a unifying theory for why hexagonal grids are so ubiquitous in path-integrator circuits. Such trained networks also yield powerful mechanistic hypotheses, exhibiting realistic levels of biological variability not captured by hand-designed models. We furthermore develop methods to analyze the connectome and activity maps of our trained networks to elucidate fundamental mechanisms underlying path integration. These methods provide an instructive roadmap to go from connectomic and physiological measurements to conceptual understanding in a manner that might be generalizable to other settings.",
      "publication_date": "2020-12-30",
      "venue": "Neuron",
      "year": 2020,
      "citation_count": 83,
      "authors": [
        "Ben Sorscher",
        "Gabriel C. Mel",
        "Samuel A. Ocko",
        "Lisa M. Giocomo",
        "S. Ganguli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "26fc224a118e20e5d6bc98b57711001abc20ec00",
      "title": "Recurrent predictive coding models for associative memory employing covariance learning",
      "abstract": "The computational principles adopted by the hippocampus in associative memory (AM) tasks have been one of the mostly studied topics in computational and theoretical neuroscience. Classical models of the hippocampal network assume that AM is performed via a form of covariance learning, where associations between memorized items are represented by entries in the learned covariance matrix encoded in the recurrent connections in the hippocampal subfield CA3. On the other hand, it has been recently proposed that AM in the hippocampus is achieved through predictive coding. Hierarchical predictive coding models following this theory perform AM, but fail to capture the recurrent hippocampal structure that encodes the covariance in the classical models. Such a dichotomy pose potential difficulties for developing a unitary theory of how memory is formed and recalled in the hippocampus. Earlier predictive coding models that learn the covariance information of inputs explicitly seem to be a solution to this dichotomy. Here, we show that although these models can perform AM, they do it in an implausible and numerically unstable way. Instead, we propose alternatives to these earlier covariance-learning predictive coding networks, which learn the covariance information implicitly and plausibly, and can use dendritic structures to encode prediction errors. We show analytically that our proposed models are perfectly equivalent to the earlier predictive coding model learning covariance explicitly, and encounter no numerical issues when performing AM tasks in practice. We further show that our models can be combined with hierarchical predictive coding networks to model the hippocampo-neocortical interactions. Our models provide a biologically plausible approach to modelling the hippocampal network, pointing to a potential computational mechanism employed by the hippocampus during memory formation and recall, which unifies predictive coding and covariance learning based on the recurrent network structure. Author summary The hippocampus and adjacent cortical areas have long been considered essential for the formation of associative memories. Earlier theoretical works have assumed that the hippocampus stores in its recurrent connections statistical regularities embedded in the sensory inputs. On the other hand, it has been recently suggested that the hippocampus retrieves memory by generating predictions of ongoing sensory inputs. Computational models have thus been proposed to account for this predictive nature of the hippocampal network using predictive coding, a general theory of information processing in the cortex. However, these hierarchical predictive coding models of the hippocampus did not describe how it stores the statistical regularities that play a key role for associative memory in the classical hippocampal models, hindering a unified understanding of the underlying computational principles employed by the hippocampus. To address this dichotomy, here we present a family of predictive coding models that also learn the statistical information needed for associative memory. Our models can stably perform associative memory tasks in a biologically plausible manner, even with large structured data such as natural scenes. Our work provides a possible mechanism of how the recurrent hippocampal network may employ various computational principles concurrently to perform associative memory.",
      "publication_date": "2022-11-09",
      "venue": "bioRxiv",
      "year": 2022,
      "citation_count": 25,
      "authors": [
        "Mufeng Tang",
        "Tommaso Salvatori",
        "Beren Millidge",
        "Yuhang Song",
        "Thomas Lukasiewicz",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1fe70364ca7c7cf3b5008969518fd3257e62a232",
      "title": "The hippocampus as a predictive map",
      "abstract": "A cognitive map has long been the dominant metaphor for hippocampal function, embracing the idea that place cells encode a geometric representation of space. However, evidence for predictive coding, reward sensitivity and policy dependence in place cells suggests that the representation is not purely spatial. We approach this puzzle from a reinforcement learning perspective: what kind of spatial representation is most useful for maximizing future reward? We show that the answer takes the form of a predictive representation. This representation captures many aspects of place cell responses that fall outside the traditional view of a cognitive map. Furthermore, we argue that entorhinal grid cells encode a low-dimensionality basis set for the predictive representation, useful for suppressing noise in predictions and extracting multiscale structure for hierarchical planning.",
      "publication_date": "2017-06-07",
      "venue": "Nature Neuroscience",
      "year": 2017,
      "citation_count": 757,
      "authors": [
        "Kimberly L. Stachenfeld",
        "M. Botvinick",
        "S. Gershman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "66c286df54551baba7351a1ed44019367e5aa7ea",
      "title": "Evidence of a predictive coding hierarchy in the human brain listening to speech",
      "abstract": "Current machine learning language algorithms make adjacent word-level predictions. In this work, Caucheteux et al. show that the human brain probably uses long-range and hierarchical predictions, taking into account up to eight possible words into the future. Considerable progress has recently been made in natural language processing: deep learning algorithms are increasingly able to generate, summarize, translate and classify texts. Yet, these language models still fail to match the language abilities of humans. Predictive coding theory offers a tentative explanation to this discrepancy: while language models are optimized to predict nearby words, the human brain would continuously predict a hierarchy of representations that spans multiple timescales. To test this hypothesis, we analysed the functional magnetic resonance imaging brain signals of 304 participants listening to short stories. First, we confirmed that the activations of modern language models linearly map onto the brain responses to speech. Second, we showed that enhancing these algorithms with predictions that span multiple timescales improves this brain mapping. Finally, we showed that these predictions are organized hierarchically: frontoparietal cortices predict higher-level, longer-range and more contextual representations than temporal cortices. Overall, these results strengthen the role of hierarchical predictive coding in language processing and illustrate how the synergy between neuroscience and artificial intelligence can unravel the computational bases of human cognition.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b820ad4a35a6587b44ab03c0e70672a2ed5e9c5f",
      "title": "Accurate Path Integration in Continuous Attractor Network Models of Grid Cells",
      "abstract": "Grid cells in the rat entorhinal cortex display strikingly regular firing responses to the animal's position in 2-D space and have been hypothesized to form the neural substrate for dead-reckoning. However, errors accumulate rapidly when velocity inputs are integrated in existing models of grid cell activity. To produce grid-cell-like responses, these models would require frequent resets triggered by external sensory cues. Such inadequacies, shared by various models, cast doubt on the dead-reckoning potential of the grid cell system. Here we focus on the question of accurate path integration, specifically in continuous attractor models of grid cell activity. We show, in contrast to previous models, that continuous attractor models can generate regular triangular grid responses, based on inputs that encode only the rat's velocity and heading direction. We consider the role of the network boundary in the integration performance of the network and show that both periodic and aperiodic networks are capable of accurate path integration, despite important differences in their attractor manifolds. We quantify the rate at which errors in the velocity integration accumulate as a function of network size and intrinsic noise within the network. With a plausible range of parameters and the inclusion of spike variability, our model networks can accurately integrate velocity inputs over a maximum of \u223c10\u2013100 meters and \u223c1\u201310 minutes. These findings form a proof-of-concept that continuous attractor dynamics may underlie velocity integration in the dorsolateral medial entorhinal cortex. The simulations also generate pertinent upper bounds on the accuracy of integration that may be achieved by continuous attractor dynamics in the grid cell network. We suggest experiments to test the continuous attractor model and differentiate it from models in which single cells establish their responses independently of each other.",
      "publication_date": "2008-11-12",
      "venue": "PLoS Comput. Biol.",
      "year": 2008,
      "citation_count": 703,
      "authors": [
        "Y. Burak",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "79be3dcc0cd53613e5386425e387b2664c9c9ab0",
      "title": "Untethered firing fields and intermittent silences: Why grid\u2010cell discharge is so variable",
      "abstract": "Grid cells in medial entorhinal cortex are notoriously variable in their responses, despite the striking hexagonal arrangement of their spatial firing fields. Indeed, when the animal moves through a firing field, grid cells often fire much more vigorously than predicted or do not fire at all. The source of this trial\u2010to\u2010trial variability is not completely understood. By analyzing grid\u2010cell spike trains from mice running in open arenas and on linear tracks, we characterize the phenomenon of \u201cmissed\u201d firing fields using the statistical theory of zero inflation. We find that one major cause of grid\u2010cell variability lies in the spatial representation itself: firing fields are not as strongly anchored to spatial location as the averaged grid suggests. In addition, grid fields from different cells drift together from trial to trial, regardless of whether the environment is real or virtual, or whether the animal moves in light or darkness. Spatial realignment across trials sharpens the grid representation, yielding firing fields that are more pronounced and significantly narrower. These findings indicate that ensembles of grid cells encode relative position more reliably than absolute position.",
      "publication_date": "2020-02-11",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "115806ae388f43e1e25062ce88d2415c64c580dc",
      "title": "Grid cell firing fields in a volumetric space",
      "abstract": "We investigated how entorhinal grid cells represent volumetric (three-dimensional) space. On a flat surface, grid cell firing fields are circular and arranged in a close-packed hexagonal array. In three dimensions, theoretical and computational work suggests that the most efficient configuration would be a regular close packing of spherical fields. We report that in rats exploring a cubic lattice, grid cells were spatially stable and maintained normal directional modulation, theta modulation and spike dynamics. However, while the majority of grid fields were spherical, they were irregularly arranged, even when only fields abutting the lower surface (equivalent to the floor) were considered. Thus, grid organization is shaped by the environment\u2019s movement affordances, and may not need to be regular to support spatial computations. One Sentence Summary In rats exploring a volumetric space, grid cells are spatially modulated but their firing fields are irregularly arranged.",
      "publication_date": "2020-12-07",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "48fa2392ceb51aa5fc57766d513efe1fa9bf3de0",
      "title": "Spatial periodicity in grid cell firing is explained by a neural sequence code of 2-D trajectories",
      "abstract": "Spatial periodicity in grid cell firing has been interpreted as a neural metric for space providing animals with a coordinate system in navigating physical and mental spaces. However, the specific computational problem being solved by grid cells has remained elusive. Here, we provide mathematical proof that spatial periodicity in grid cell firing is the only possible solution to a neural sequence code of 2-D trajectories and that the hexagonal firing pattern of grid cells is the most parsimonious solution to such a sequence code. We thereby provide a likely teleological cause for the existence of grid cells and reveal the underlying nature of the global geometric organization in grid maps as a direct consequence of a simple local sequence code. A sequence code by grid cells provides intuitive explanations for many previously puzzling experimental observations and may transform our thinking about grid cells.",
      "publication_date": "2023-05-30",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "998628588f7850d533a172c883872057a9198d82",
      "title": "Predictive Coding: a Theoretical and Experimental Review",
      "abstract": "Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.",
      "publication_date": "2021-07-27",
      "venue": "arXiv.org",
      "year": 2021,
      "citation_count": 127,
      "authors": [
        "Beren Millidge",
        "A. Seth",
        "C. Buckley"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "25b422756cbddb3193002c9a49c844641fff2127",
      "title": "No Free Lunch from Deep Learning in Neuroscience: A Case Study through Models of the Entorhinal-Hippocampal Circuit",
      "abstract": "Research in Neuroscience, as in many scientific disciplines, is undergoing a renaissance based on deep learning. Unique to Neuroscience, deep learning models can be used not only as a tool but interpreted as models of the brain. The central claims of recent deep learning-based models of brain circuits are that they make novel predictions about neural phenomena or shed light on the fundamental functions being optimized. We show, through the case-study of grid cells in the entorhinal-hippocampal circuit, that one may get neither. We begin by reviewing the principles of grid cell mechanism and function obtained from first-principles modeling efforts, then rigorously examine the claims of deep learning models of grid cells. Using large-scale architectural and hyperparameter sweeps and theory-driven experimentation, we demonstrate that the results of such models may be more strongly driven by particular, non-fundamental, and post-hoc implementation choices than fundamental truths about neural circuits or the loss function(s) they might optimize. We discuss why these models cannot be expected to produce accurate models of the brain without the addition of substantial amounts of inductive bias, an informal No Free Lunch result for Neuroscience. Based on first principles work, we provide hypotheses for what additional loss functions will produce grid cells more robustly. In conclusion, circumspection and transparency, together with biological knowledge, are warranted in building and interpreting deep learning models in Neuroscience.",
      "publication_date": "2023-08-15",
      "venue": "bioRxiv",
      "year": 2023,
      "citation_count": 57,
      "authors": [
        "Rylan Schaeffer",
        "Mikail Khona",
        "I. Fiete"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "cbeb0b647964650757d5e25aa711916e12a896dc",
      "title": "Grid cell firing patterns maintain their hexagonal firing patterns on a circular track",
      "abstract": "In an open two-dimensional environment, grid cells in the medial entorhinal cortex are known to be active in multiple locations, displaying a striking periodic hexagonal firing pattern covering the entire space. Both modeling and experimental data suggest that such periodic spatial representations may emerge from a continuous attractor network. According to this theory, grid cell activity in any stable 1D environment is a slice through an underlying 2D hexagonal pattern, which is supported by some experimental studies but challenged by others. Grid cells are believed to play a fundamental role in path integration, and so understanding their behavior in various environments is crucial for understanding the flow of information through the entorhinal-hippocampal system. To this end, we analyzed the activity of grid cells when rats traversed a circular track. A previous study involving this data set analyzed individual grid cell activity patterns separately, but we found that individual grid cells do not provide sufficient data for determining the under-lying spatial activity pattern. To circumvent this, we compute the population autocorrelation, which pools together population responses from all grid cells within the same module. This novel approach recovers the underlying six-peak hexagonal pattern that was not observable in the individual autocorrelations. We also use the population autocorrelation to infer the spacing and orientation of the population lattice, revealing how the lattice differs across environments. Furthermore, the population autocorrelation of the linearized track reveals that at the level of the population, grid cells have an allocentric code for space. These results are strong support for the attractor network theory for grid cells, and our novel approach can be used to analyze grid cell activity in any undersampled environment.",
      "publication_date": "2023-09-15",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "35ce44e06802d9f778eb02bf03a6fc5aa657bd56",
      "title": "Inferring neural activity before plasticity as a foundation for learning beyond backpropagation",
      "abstract": "This paper introduces \u2018prospective configuration\u2019, a new principle for learning in neural networks, which differs from backpropagation and is more efficient in learning and more consistent with data on neural activity and behavior. For both humans and machines, the essence of learning is to pinpoint which components in its information processing pipeline are responsible for an error in its output, a challenge that is known as \u2018credit assignment\u2019. It has long been assumed that credit assignment is best solved by backpropagation, which is also the foundation of modern machine learning. Here, we set out a fundamentally different principle on credit assignment called \u2018prospective configuration\u2019. In prospective configuration, the network first infers the pattern of neural activity that should result from learning, and then the synaptic weights are modified to consolidate the change in neural activity. We demonstrate that this distinct mechanism, in contrast to backpropagation, (1) underlies learning in a well-established family of models of cortical circuits, (2) enables learning that is more efficient and effective in many contexts faced by biological organisms and (3) reproduces surprising patterns of neural activity and behavior observed in diverse human and rat learning experiments.",
      "publication_date": "2024-01-03",
      "venue": "Nature Neuroscience",
      "year": 2024,
      "citation_count": 16,
      "authors": [
        "Yuhang Song",
        "Beren Millidge",
        "Tommaso Salvatori",
        "Thomas Lukasiewicz",
        "Zhenghua Xu",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "870d2b9c71c4448498a47870f2622f0a2db44b20",
      "title": "Predictive coding networks for temporal prediction",
      "abstract": "One of the key problems the brain faces is inferring the state of the world from a sequence of dynamically changing stimuli, and it is not yet clear how the sensory system achieves this task. A well-established computational framework for describing perceptual processes in the brain is provided by the theory of predictive coding. Although the original proposals of predictive coding have discussed temporal prediction, later work developing this theory mostly focused on static stimuli, and key questions on neural implementation and computational properties of temporal predictive coding networks remain open. Here, we address these questions and present a formulation of the temporal predictive coding model that can be naturally implemented in recurrent networks, in which activity dynamics rely only on local inputs to the neurons, and learning only utilises local Hebbian plasticity. Additionally, we show that temporal predictive coding networks can approximate the performance of the Kalman filter in predicting behaviour of linear systems, and behave as a variant of a Kalman filter which does not track its own subjective posterior variance. Importantly, temporal predictive coding networks can achieve similar accuracy as the Kalman filter without performing complex mathematical operations, but just employing simple computations that can be implemented by biological networks. Moreover, when trained with natural dynamic inputs, we found that temporal predictive coding can produce Gabor-like, motion-sensitive receptive fields resembling those observed in real neurons in visual areas. In addition, we demonstrate how the model can be effectively generalized to nonlinear systems. Overall, models presented in this paper show how biologically plausible circuits can predict future stimuli and may guide research on understanding specific neural circuits in brain areas involved in temporal prediction. Author summary While significant advances have been made in the neuroscience of how the brain processes static stimuli, the time dimension has often been relatively neglected. However, time is crucial since the stimuli perceived by our senses typically dynamically vary in time, and the cortex needs to make sense of these changing inputs. This paper describes a computational model of cortical networks processing temporal stimuli. This model is able to infer and track the state of the environment based on noisy inputs, and predict future sensory stimuli. By ensuring that these predictions match the incoming stimuli, the model is able to learn the structure and statistics of its temporal inputs and produces responses of neurons resembling those in the brain. The model may help in further understanding neural circuits in sensory cortical areas.",
      "publication_date": "2024-03-09",
      "venue": "bioRxiv",
      "year": 2024,
      "citation_count": 20,
      "authors": [
        "Beren Millidge",
        "Mufeng Tang",
        "Mahyar Osanlouy",
        "N. Harper",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "838c48b300fd1be40014c97edd79233b1fb52f69",
      "title": "Hippocampal Spike-Timing Correlations Lead to Hexagonal Grid Fields.",
      "abstract": "Space is represented in the mammalian brain by the activity of hippocampal place cells, as well as in their spike-timing correlations. Here, we propose a theory for how this temporal code is transformed to spatial firing rate patterns via spike-timing-dependent synaptic plasticity. The resulting dynamics of synaptic weights resembles well-known pattern formation models in which a lateral inhibition mechanism gives rise to a Turing instability. We identify parameter regimes in which hexagonal firing patterns develop as they have been found in medial entorhinal cortex.",
      "publication_date": "2017-06-21",
      "venue": "",
      "year": 2017,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2c0203ff41fbe8f3fc2a0e706ecb3ecf806f2108",
      "title": "Predictive Coding Approximates Backprop Along Arbitrary Computation Graphs",
      "abstract": "Abstract Backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. Recently it has been shown that backprop in multilayer perceptrons (MLPs) can be approximated using predictive coding, a biologically plausible process theory of cortical computation that relies solely on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs but in the concept of automatic differentiation, which allows for the optimization of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice, rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding convolutional neural networks, recurrent neural networks, and the more complex long short-term memory, which include a nonlayer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks while using only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry and may also contribute to the development of completely distributed neuromorphic architectures.",
      "publication_date": "2020-06-07",
      "venue": "Neural Computation",
      "year": 2020,
      "citation_count": 119,
      "authors": [
        "Beren Millidge",
        "Alexander Tschantz",
        "C. Buckley"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "b9ea31c9c8719a504dd349feffbd79c7c8d18e23",
      "title": "Coupled Noisy Spiking Neurons as Velocity-Controlled Oscillators in a Model of Grid Cell Spatial Firing",
      "abstract": "One of the two primary classes of models of grid cell spatial firing uses interference between oscillators at dynamically modulated frequencies. Generally, these models are presented in terms of idealized oscillators (modeled as sinusoids), which differ from biological oscillators in multiple important ways. Here we show that two more realistic, noisy neural models (Izhikevich's simple model and a biophysical model of an entorhinal cortex stellate cell) can be successfully used as oscillators in a model of this type. When additive noise is included in the models such that uncoupled or sparsely coupled cells show realistic interspike interval variance, both synaptic and gap-junction coupling can synchronize networks of cells to produce comparatively less variable network-level oscillations. We show that the frequency of these oscillatory networks can be controlled sufficiently well to produce stable grid cell spatial firing on the order of at least 2\u20135 min, despite the high noise level. Our results suggest that the basic principles of oscillatory interference models work with more realistic models of noisy neurons. Nevertheless, a number of simplifications were still made and future work should examine increasingly realistic models.",
      "publication_date": "2010-10-13",
      "venue": "Journal of Neuroscience",
      "year": 2010,
      "citation_count": 108,
      "authors": [
        "Eric A. Zilli",
        "M. Hasselmo"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "5be0bfaa322272f3c70a203bcc0aef87a6a6dc13",
      "title": "A Brain-Inspired Adaptive Space Representation Model Based on Grid Cells and Place Cells",
      "abstract": "Grid cells and place cells are important neurons in the animal brain. The information transmission between them provides the basis for the spatial representation and navigation of animals and also provides reference for the research on the autonomous navigation mechanism of intelligent agents. Grid cells are important information source of place cells. The supervised learning and unsupervised learning models can be used to simulate the generation of place cells from grid cell inputs. However, the existing models preset the firing characteristics of grid cell. In this paper, we propose a united generation model of grid cells and place cells. First, the visual place cells with nonuniform distribution generate the visual grid cells with regional firing field through feedforward network. Second, the visual grid cells and the self-motion information generate the united grid cells whose firing fields extend to the whole space through genetic algorithm. Finally, the visual place cells and the united grid cells generate the united place cells with uniform distribution through supervised fuzzy adaptive resonance theory (ART) network. Simulation results show that this model has stronger environmental adaptability and can provide reference for the research on spatial representation model and brain-inspired navigation mechanism of intelligent agents under the condition of nonuniform environmental information.",
      "publication_date": "2020-08-11",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "abb5aa3ff57e043bbaf59239ab27150a2545aee5",
      "title": "Associative Memories via Predictive Coding",
      "abstract": "Associative memories in the brain receive and store patterns of activity registered by the sensory neurons, and are able to retrieve them when necessary. Due to their importance in human intelligence, computational models of associative memories have been developed for several decades now. In this paper, we present a novel neural model for realizing associative memories, which is based on a hierarchical generative network that receives external stimuli via sensory neurons. It is trained using predictive coding, an error-based learning algorithm inspired by information processing in the cortex. To test the model's capabilities, we perform multiple retrieval experiments from both corrupted and incomplete data points. In an extensive comparison, we show that this new model outperforms in retrieval accuracy and robustness popular associative memory models, such as autoencoders trained via backpropagation, and modern Hopfield networks. In particular, in completing partial data points, our model achieves remarkable results on natural image datasets, such as ImageNet, with a surprisingly high accuracy, even when only a tiny fraction of pixels of the original images is presented. Our model provides a plausible framework to study learning and retrieval of memories in the brain, as it closely mimics the behavior of the hippocampus as a memory index and generative model.",
      "publication_date": "2021-09-16",
      "venue": "Neural Information Processing Systems",
      "year": 2021,
      "citation_count": 64,
      "authors": [
        "Tommaso Salvatori",
        "Yuhang Song",
        "Yujian Hong",
        "Simon Frieder",
        "Lei Sha",
        "Zhenghua Xu",
        "R. Bogacz",
        "Thomas Lukasiewicz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "46e2af3d7e8dd50675b0062fe9e4fce5cd5a0508",
      "title": "Predictive and Adaptive Deep Coding for Wireless Image Transmission in Semantic Communication",
      "abstract": "Semantic communication is a newly emerged communication paradigm that exploits deep learning (DL) models to realize communication processes like source coding and channel coding. Recent advances have demonstrated that DL-based joint source-channel coding (DeepJSCC) can achieve exciting data compression and noise-resiliency performances for wireless image transmission tasks, especially in environments with low channel signal-to-noises (SNRs). However, existing DeepJSCC-based semantic communication frameworks still cannot achieve adaptive code rates for different channel SNRs and image contents, which reduces its flexibility and bandwidth efficiency. In this paper, we propose a predictive and adaptive deep coding (PADC) framework for realizing flexible code rate optimization with a given target transmission quality requirement. PADC is realized by a variable code length enabled DeepJSCC (DeepJSCC-V) model for realizing flexible code length adjustment, an Oracle Network (OraNet) model for predicting peak-signal-to-noise (PSNR) value for an image transmission task according to its contents, channel signal to noise ratio (SNR) and the compression ratio (CR) value, and a CR optimizer aims at finding the minimal data-level or instance-level CR with a PSNR quality constraint. By using the above three modules, PADC can transmit the image data with minimal CR, which greatly increases bandwidth efficiency. Simulation results demonstrate that the proposed DeepJSCC-V model can achieve similar PSNR performances compared with the state-of-the-art Attention-based DeepJSCC (ADJSCC) model, and the proposed OraNet model is able to predict high-quality PSNR values with an average error lower than 0.5dB. Results also demonstrate that the proposed PADC can use nearly minimal bandwidth consumption for wireless image transmission tasks with different channel SNR and image contents, at the same time guaranteeing the PSNR constraint for each image data.",
      "publication_date": "2023-08-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "34a9b0c7ddc9012b8657c4bc1de5f14d45e646b4",
      "title": "Hybrid predictive coding: Inferring, fast and slow",
      "abstract": "Predictive coding is an influential model of cortical neural activity. It proposes that perceptual beliefs are furnished by sequentially minimising \u201cprediction errors\u201d\u2014the differences between predicted and observed data. Implicit in this proposal is the idea that successful perception requires multiple cycles of neural activity. This is at odds with evidence that several aspects of visual perception\u2014including complex forms of object recognition\u2014arise from an initial \u201cfeedforward sweep\u201d that occurs on fast timescales which preclude substantial recurrent activity. Here, we propose that the feedforward sweep can be understood as performing amortized inference (applying a learned function that maps directly from data to beliefs) and recurrent processing can be understood as performing iterative inference (sequentially updating neural activity in order to improve the accuracy of beliefs). We propose a hybrid predictive coding network that combines both iterative and amortized inference in a principled manner by describing both in terms of a dual optimization of a single objective function. We show that the resulting scheme can be implemented in a biologically plausible neural architecture that approximates Bayesian inference utilising local Hebbian update rules. We demonstrate that our hybrid predictive coding model combines the benefits of both amortized and iterative inference\u2014obtaining rapid and computationally cheap perceptual inference for familiar data while maintaining the context-sensitivity, precision, and sample efficiency of iterative inference schemes. Moreover, we show how our model is inherently sensitive to its uncertainty and adaptively balances iterative and amortized inference to obtain accurate beliefs using minimum computational expense. Hybrid predictive coding offers a new perspective on the functional relevance of the feedforward and recurrent activity observed during visual perception and offers novel insights into distinct aspects of visual phenomenology.",
      "publication_date": "2022-04-05",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "3bc48861d66b2ee2968ed26c0d96c9b2e1ab06e8",
      "title": "Path integration maintains spatial periodicity of grid cell firing in a 1D circular track",
      "abstract": "Entorhinal grid cells are thought to provide a 2D spatial metric of the environment. In this study we demonstrate that in a familiar 1D circular track (i.e., a continuous space) grid cells display a novel 1D equidistant firing pattern based on integrated distance rather than travelled distance or time. In addition, field spacing is increased compared to a 2D open field, probably due to a reduced access to the visual cue in the track. This metrical modification is accompanied by a change in LFP theta oscillations, but no change in intrinsic grid cell rhythmicity, or firing activity of entorhinal speed and head-direction cells. These results suggest that in a 1D circular space grid cell spatial selectivity is shaped by path integration processes, while grid scale relies on external information. In an open field, the preferential firing of grid cells on a hexagonal lattice is formed by integrating external as well as self-motion cues. Here, the authors show that on a 1D circular track, path integration cues shape the spatial selectivity of grid cells while external cues determine the scale of the grid.",
      "publication_date": "2019-02-19",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6bcdf260d7927d8fe4ff030c20ee1db974d0c969",
      "title": "Predictive Coding: Towards a Future of Deep Learning beyond Backpropagation?",
      "abstract": "The backpropagation of error algorithm (BP) used to train deep neural networks has been fundamental to the successes of deep learning. However, it requires sequential backwards updates and non-local computations which make it challenging to parallelize at scale and is unlike how learning works in the brain. Neuroscience-inspired learning algorithms, however, such as \\emph{predictive coding} which utilize local learning have the potential to overcome these limitations and advance beyond deep learning technologies in the future. While predictive coding originated in theoretical neuroscience as a model of information processing in the cortex, recent work has developed the idea into a general-purpose algorithm able to train neural networks using only local computations. In this survey, we review works that have contributed to this perspective and demonstrate the close connection between predictive coding and backpropagation in terms of generalization quality, as well as works that highlight the multiple advantages of using predictive coding models over backprop-trained neural networks. Specifically, we show the substantially greater flexibility of predictive coding networks against equivalent deep neural networks, which can function as classifiers, generators, and associative memories simultaneously, and can be defined on arbitrary graph topologies. Finally, we review direct benchmarks of predictive coding networks on machine learning classification tasks, as well as its close connections to control theory and applications in robotics.",
      "publication_date": "2022-02-18",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0a292908d132abc2bf5fc5413d8c121600b5288e",
      "title": "A Computational Model of Grid Cells based on Dendritic Self-organized Learning",
      "abstract": "In this paper we present a new computational model for grid cells. These cells are neurons in the entorhinal cortex of the hippocampal region that encode allocentric spatial information. They possess a peculiar, triangular firing pattern that spans the entire environment with a virtual lattice. We show that such a firing pattern can emerge from a dendritic, self-organized learning process. A key aspect of the proposed model is the hypothesis that the dendritic tree of a grid cell can behave like a sparse self organizing map that tries to cover its input space as best as possible. We argue, that the encoding scheme used by grid cells is possibly not limited to the description of spatial information and may represent a general principle on how complex information is encoded in higher level brain areas like the hippocampal region.",
      "publication_date": null,
      "venue": "",
      "year": 2013,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5374052df7a9a69499586b5acfc433f8a8e20e66",
      "title": "On Conformal Isometry of Grid Cells: Learning Distance-Preserving Position Embedding",
      "abstract": "This paper investigates the conformal isometry hypothesis as a potential explanation for the hexagonal periodic patterns in grid cell response maps. We posit that grid cell activities form a high-dimensional vector in neural space, encoding the agent's position in 2D physical space. As the agent moves, this vector rotates within a 2D manifold in the neural space, driven by a recurrent neural network. The conformal hypothesis proposes that this neural manifold is a conformal isometric embedding of 2D physical space, where local physical distance is preserved by the embedding up to a scaling factor (or unit of metric). Such distance-preserving position embedding is indispensable for path planning in navigation, especially planning local straight path segments. We conduct numerical experiments to show that this hypothesis leads to the hexagonal grid firing patterns by learning maximally distance-preserving position embedding, agnostic to the choice of the recurrent neural network. Furthermore, we present a theoretical explanation of why hexagon periodic patterns emerge by minimizing our loss function by showing that hexagon flat torus is maximally distance preserving.",
      "publication_date": "2024-05-27",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "917922f63a9920b940c05fa802deb3d7d83dd6b0",
      "title": "Using Convolutional Neural Networks to Build a Lightweight Flood Height Prediction Model with Grad-Cam for the Selection of Key Grid Cells in Radar Echo Maps",
      "abstract": "Recent climate change has brought extremely heavy rains and widescale flooding to many areas around the globe. However, previous flood prediction methods usually require a lot of computation to obtain the prediction results and impose a heavy burden on the unit cost of the prediction. This paper proposes the use of a deep learning model (DLM) to overcome these problems. We alleviated the high computational overhead of this approach by developing a novel framework for the construction of lightweight DLMs. The proposed scheme involves training a convolutional neural network (CNN) by using a radar echo map in conjunction with historical flood records at target sites and using Grad-Cam to extract key grid cells from these maps (representing regions with the greatest impact on flooding) for use as inputs in another DLM. Finally, we used real radar echo maps of five locations and the flood heights record to verify the validity of the method proposed in this paper. The experimental results show that our proposed lightweight model can achieve similar or even better prediction accuracy at all locations with only about 5~15% of the operation time and about 30~35% of the memory space of the CNN.",
      "publication_date": "2022-01-07",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e96016d5e3c6492230aa172efcf733596dc64b6e",
      "title": "Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells",
      "abstract": "Unsupervised text encoding models have recently fueled substantial progress in Natural Language Processing (NLP). The key idea is to use neural networks to convert words in texts to vector space representations (embeddings) based on word positions in a sentence and their contexts. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as Points of Interest (POIs) into models. A general space encoding method is valuable for a multitude of tasks such asPOI search, land use classification, point-based spatial interpolation and locationaware image classification. However, no such general model exists to date beyond simply applying discretizing or feed forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for encoding space and are critical for recognizing places and for path-integration. Inspired by this research, wepropose a representation learning model called Space2vec to encode the absolutepositions and spatial relationships of places. We conduct experiments on realworld geographic data and predict types of POIs at given positions based on their1) locations and 2) nearby POIs. Results show that because of its multi-scale representations Space2vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed forward nets, and tile embedding approaches.",
      "publication_date": "2020-02-16",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e83816c0e7ed01ee48d579568ba97db72931d979",
      "title": "Learning on Arbitrary Graph Topologies via Predictive Coding",
      "abstract": "Training with backpropagation (BP) in standard deep learning consists of two main steps: a forward pass that maps a data point to its prediction, and a backward pass that propagates the error of this prediction back through the network. This process is highly effective when the goal is to minimize a specific objective function. However, it does not allow training on networks with cyclic or backward connections. This is an obstacle to reaching brain-like capabilities, as the highly complex heterarchical structure of the neural connections in the neocortex are potentially fundamental for its effectiveness. In this paper, we show how predictive coding (PC), a theory of information processing in the cortex, can be used to perform inference and learning on arbitrary graph topologies. We experimentally show how this formulation, called PC graphs, can be used to flexibly perform different tasks with the same network by simply stimulating specific neurons. This enables the model to be queried on stimuli with different structures, such as partial images, images with labels, or images without labels. We conclude by investigating how the topology of the graph influences the final performance, and comparing against simple baselines trained with BP.",
      "publication_date": "2022-01-31",
      "venue": "Neural Information Processing Systems",
      "year": 2022,
      "citation_count": 34,
      "authors": [
        "Tommaso Salvatori",
        "Luca Pinchetti",
        "Beren Millidge",
        "Yuhang Song",
        "R. Bogacz",
        "Thomas Lukasiewicz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "202c79bbb45ab6524141feacc81caacc4ba00401",
      "title": "Memory-augmented Dense Predictive Coding for Video Representation Learning",
      "abstract": "The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.",
      "publication_date": "2020-08-03",
      "venue": "",
      "year": 2020,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "858549b00245aadc92f91a2540f01398f5f389ae",
      "title": "A solution to the learning dilemma for recurrent networks of spiking neurons",
      "abstract": "Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method\u2013called e-prop\u2013approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence. Bellec et al. present a mathematically founded approximation for gradient descent training of recurrent neural networks without backwards propagation in time. This enables biologically plausible training of spike-based neural network models with working memory and supports on-chip training of neuromorphic hardware.",
      "publication_date": "2019-08-19",
      "venue": "Nature Communications",
      "year": 2019,
      "citation_count": 466,
      "authors": [
        "G. Bellec",
        "Franz Scherr",
        "Anand Subramoney",
        "Elias Hajek",
        "Darjan Salaj",
        "R. Legenstein",
        "W. Maass"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "62f16717f8359d414513cc77ccdd363b508dae35",
      "title": "Brain-Inspired Spatial Representation Based on Grid Cells with Finite-Spacing Firing Field",
      "abstract": "Grid cell is a kind of important neuron cell related to spatial cognition and navigation in animal brain. It has a hexagonal firing field extending to the whole space. Grid cells exist in the form of modules, whose number is finite, and the grid cells between adjacent modules have discrete firing- field spacing with constant ratio. The existing models can simulate the constant ratio discrete of firing-field spacing between modules, but the finite characteristic of firing-field spacing cannot be simulated. We propose a recurrent attractor network model of grid cell, which can generate grid cells with finite firing-field spacing by analyzing the connections between grid cells in a single network. Then, the grid cells in different modules could possess discrete firing-field spacing with constant ratio through the competition among the recurrent networks. The recurrent attractor network model can provide reference for the construction of brain-inspired navigation system of unmanned platform.",
      "publication_date": "2019-11-01",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "f4649234d52d1848495dcebd9f0cc9042e636bff",
      "title": "A Spin Glass Model of Path Integration in Rat Medial Entorhinal Cortex",
      "abstract": "Electrophysiological recording studies in the dorsocaudal region of medial entorhinal cortex (dMEC) of the rat reveal cells whose spatial firing fields show a remarkably regular hexagonal grid pattern (Fyhn et al., 2004; Hafting et al., 2005). We describe a symmetric, locally connected neural network, or spin glass model, that spontaneously produces a hexagonal grid of activity bumps on a two-dimensional sheet of units. The spatial firing fields of the simulated cells closely resemble those of dMEC cells. A collection of grids with different scales and/or orientations forms a basis set for encoding position. Simulations show that the animal\u2019s location can easily be determined from the population activity pattern. Introducing an asymmetry in the model allows the activity bumps to be shifted in any direction, at a rate proportional to velocity, to achieve path integration. Furthermore, information about the structure of the environment can be superimposed on the spatial position signal by modulation of the bump activity levels without significantly interfering with the hexagonal periodicity of firing fields. Our results support the conjecture of Hafting et al. (2005) that an attractor network in dMEC may be the source of path integration information afferent to hippocampus.",
      "publication_date": "2006-04-19",
      "venue": "Journal of Neuroscience",
      "year": 2006,
      "citation_count": 617,
      "authors": [
        "Mark C. Fuhs",
        "D. Touretzky"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "554a5385b955e3be4a0621b643b476c0a5514d46",
      "title": "What Does the Anatomical Organization of the Entorhinal Cortex Tell Us?",
      "abstract": "The entorhinal cortex is commonly perceived as a major input and output structure of the hippocampal formation, entertaining the role of the nodal point of cortico-hippocampal circuits. Superficial layers receive convergent cortical information, which is relayed to structures in the hippocampus, and hippocampal output reaches deep layers of entorhinal cortex, that project back to the cortex. The finding of the grid cells in all layers and reports on interactions between deep and superficial layers indicate that this rather simplistic perception may be at fault. Therefore, an integrative approach on the entorhinal cortex, that takes into account recent additions to our knowledge database on entorhinal connectivity, is timely. We argue that layers in entorhinal cortex show different functional characteristics most likely not on the basis of strikingly different inputs or outputs, but much more likely on the basis of differences in intrinsic organization, combined with very specific sets of inputs. Here, we aim to summarize recent anatomical data supporting the notion that the traditional description of the entorhinal cortex as a layered input-output structure for the hippocampal formation does not give the deserved credit to what this structure might be contributing to the overall functions of cortico-hippocampal networks.",
      "publication_date": "2008-08-28",
      "venue": "Journal of Neural Transplantation and Plasticity",
      "year": 2008,
      "citation_count": 407,
      "authors": [
        "Cathrin B. Canto",
        "F. Wouterlood",
        "M. Witter"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "c124a6aec4b1833e4e86092e20a782183349d57e",
      "title": "An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity",
      "abstract": "Abstract To efficiently learn from feedback, cortical networks need to update synaptic weights on multiple levels of cortical hierarchy. An effective and well-known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm. However, in this algorithm, the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons. Several models have been proposed that approximate the backpropagation algorithm with local synaptic plasticity, but these models require complex external control over the network or relatively complex plasticity rules. Here we show that a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple local Hebbian plasticity. Furthermore, for certain parameters, the weight change in the predictive coding model converges to that of the backpropagation algorithm. This suggests that it is possible for cortical networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output.",
      "publication_date": "2017-03-23",
      "venue": "Neural Computation",
      "year": 2017,
      "citation_count": 284,
      "authors": [
        "James C. R. Whittington",
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "2abc9d603a87b300a251a3796a12b8a2d21746df",
      "title": "Learning Grid Cells as Vector Representation of Self-Position Coupled with Matrix Representation of Self-Motion",
      "abstract": "This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multiplication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.",
      "publication_date": "2018-09-27",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9f6eb784ffc460cba56077b4b8739d4eb9115f18",
      "title": "StockFormer: Learning Hybrid Trading Machines with Predictive Coding",
      "abstract": "Typical RL-for-finance solutions directly optimize trading policies over the noisy market data, such as stock prices and trading volumes, without explicitly considering the future trends and correlations of different investment assets as we humans do. In this paper, we present StockFormer, a hybrid trading machine that integrates the forward modeling capabilities of predictive coding with the advantages of RL agents in policy flexibility. The predictive coding part consists of three Transformer branches with modified structures, which respectively extract effective latent states of long-/short-term future dynamics and asset relations. The RL agent adaptively fuses these states and then executes an actor-critic algorithm in the unified state space. The entire model is jointly trained by propagating the critic's gradients back to the predictive coding module. StockFormer significantly outperforms existing approaches across three publicly available financial datasets in terms of portfolio returns and Sharpe ratios.",
      "publication_date": "2023-08-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "618e2830b9f1c67b265d25125a6b48520f1a4ac3",
      "title": "Utilizing an Efficient Magnetic Equivalent Circuit Model and Manifold Mapping Method for Two-Level Optimization of Axial Flux Machine",
      "abstract": "In this paper, a two-level optimization algorithm is developed based on rapid magnetic equivalent circuit (MEC) model and manifold mapping (MM) method. The performance of an axial flux permanent magnet machine (AFPM) is optimized using this approach. The rapid MEC model based on grid division exhibits significant computational efficiency, while reducing the influence of artificial factors in modeling process. Further, quantitative comparison of the error performance under different combinations of divisions are carried out. Subsequently, coarse and fine models are selected, respectively. The iterative process indicates that the modeling approach enables the coarse model to correctly follow the trends of fine model as input changes. As a result, the fast convergence of the MM is successfully achieved.",
      "publication_date": "2024-06-02",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "b227f3e4c0dc96e5ac5426b85485a70f2175a205",
      "title": "Representation Learning with Contrastive Predictive Coding",
      "abstract": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
      "publication_date": "2018-07-10",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1cae417456711c4da184f5efcd1b7464a7a0661a",
      "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding",
      "abstract": "Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.",
      "publication_date": "2019-05-22",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9480a64d9520f4b8857ed291fd42db11e46594a7",
      "title": "Contrastive Difference Predictive Coding",
      "abstract": "Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \\times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20 \\times$ more sample efficient than the successor representation and $1500 \\times$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.",
      "publication_date": "2023-10-31",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0c48611df1f03ca7cec7f98a1ffaadf95dec1260",
      "title": "Grids from bands, or bands from grids? An examination of the effects of single unit contamination on grid cell firing fields.",
      "abstract": "Neural recording technology is improving rapidly, allowing for the detection of spikes from hundreds of cells simultaneously. The limiting step in multielectrode electrophysiology continues to be single cell isolation. However, this step is crucial to the interpretation of data from putative single neurons. We present here, in simulation, an illustration of possibly erroneous conclusions that may be reached when poorly isolated single cell data are analyzed. Grid cells are neurons recorded in rodents, and bats, that spike in equally spaced locations in a hexagonal pattern. One theory states that grid firing patterns arise from a combination of band firing patterns. However, we show here that summing the grid firing patterns of two poorly resolved neurons can result in spurious band-like patterns. Thus, evidence of neurons spiking in band patterns must undergo extreme scrutiny before it is accepted. Toward this aim, we discuss single cell isolation methods and metrics.",
      "publication_date": "2016-02-01",
      "venue": "Journal of Neurophysiology",
      "year": 2016,
      "citation_count": 21,
      "authors": [
        "Zaneta Navratilova",
        "Zaneta Navratilova",
        "Keith B. Godfrey",
        "B. McNaughton",
        "B. McNaughton"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "ae47b8bef62e47285f5acc7d81ea3e265171e2ef",
      "title": "The contributions of entorhinal cortex and hippocampus to error driven learning",
      "abstract": "Computational models proposed that the medial temporal lobe (MTL) contributes importantly to error-driven learning, though little direct in-vivo evidence for this hypothesis exists. To test this, we recorded in the entorhinal cortex (EC) and hippocampus (HPC) as macaques performed an associative learning task using an error-driven learning strategy, defined as better performance after error relative to correct trials. Error-detection signals were more prominent in the EC relative to HPC. Early in learning hippocampal but not EC neurons signaled error-driven learning by increasing their population stimulus-selectivity following error trials. This same pattern was not seen in another task where error-driven learning was not used. After learning, different populations of cells in both the EC and HPC signaled long-term memory of newly learned associations with enhanced stimulus-selective responses. These results suggest prominent but differential contributions of EC and HPC to learning from errors and a particularly important role of the EC in error-detection. Ku et al. recorded in the entorhinal cortex (EC) and hippocampus (HPC) of macaques during associative learning tasks in order to test the computational model prediction that they contribute to error-driven learning. They demonstrate that the EC and HPC have prominent but differential contributions to learning from errors, with the EC having a particularly prominent role in error-detection.",
      "publication_date": "2020-09-29",
      "venue": "Communications Biology",
      "year": 2020,
      "citation_count": 7,
      "authors": [
        "S-P Ku",
        "E. Hargreaves",
        "S. Wirth",
        "W. Suzuki"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "36ec1bf8a3a21321b7887caaeee9c6ee6cfc0db8",
      "title": "A predictive coding model of the N400",
      "abstract": "The N400 event-related component has been widely used to investigate the neural mechanisms underlying real-time language comprehension. However, despite decades of research, there is still no unifying theory that can explain both its temporal dynamics and functional properties. In this work, we show that predictive coding \u2013 a biologically plausible algorithm for approximating Bayesian inference \u2013 offers a promising framework for characterizing the N400. Using an implemented predictive coding computational model, we demonstrate how the N400 can be formalized as the lexico-semantic prediction error produced as the brain infers meaning from linguistic form of incoming words. We show that the magnitude of lexico-semantic prediction error mirrors the functional sensitivity of the N400 to various lexical variables, priming, contextual effects, as well as their higher-order interactions. We further show that the dynamics of the predictive coding algorithm provide a natural explanation for the temporal dynamics of the N400, and a biologically plausible link to neural activity. Together, these findings directly situate the N400 within the broader context of predictive coding research, and suggest that the brain may use the same computational mechanism for inference across linguistic and non-linguistic domains.",
      "publication_date": "2023-04-11",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "c2228c6e59fdb4e63f2e873077f1690e16fa39f8",
      "title": "Dynamic Occupancy Grid Map with Semantic Information Using Deep Learning-Based BEVFusion Method with Camera and LiDAR Fusion",
      "abstract": "In the field of robotics and autonomous driving, dynamic occupancy grid maps (DOGMs) are typically used to represent the position and velocity information of objects. Although three-dimensional light detection and ranging (LiDAR) sensor-based DOGMs have been actively researched, they have limitations, as they cannot classify types of objects. Therefore, in this study, a deep learning-based camera\u2013LiDAR sensor fusion technique is employed as input to DOGMs. Consequently, not only the position and velocity information of objects but also their class information can be updated, expanding the application areas of DOGMs. Moreover, unclassified LiDAR point measurements contribute to the formation of a map of the surrounding environment, improving the reliability of perception by registering objects that were not classified by deep learning. To achieve this, we developed update rules on the basis of the Dempster\u2013Shafer evidence theory, incorporating class information and the uncertainty of objects occupying grid cells. Furthermore, we analyzed the accuracy of the velocity estimation using two update models. One assigns the occupancy probability only to the edges of the oriented bounding box, whereas the other assigns the occupancy probability to the entire area of the box. The performance of the developed perception technique is evaluated using the public nuScenes dataset. The developed DOGM with object class information will help autonomous vehicles to navigate in complex urban driving environments by providing them with rich information, such as the class and velocity of nearby obstacles.",
      "publication_date": "2024-04-29",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ad367b44f3434b9ba6b46b41ab083210f6827a9f",
      "title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning",
      "abstract": "While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (\"PredNet\") architecture that is inspired by the concept of \"predictive coding\" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.",
      "publication_date": "2016-05-25",
      "venue": "",
      "year": 2016,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "23e04389f8728a5736382d3662341a1a2a25e171",
      "title": "Predictive Coding or Just Feature Discovery? An Alternative Account of Why Language Models Fit Brain Data",
      "abstract": "Abstract Many recent studies have shown that representations drawn from neural network language models are extremely effective at predicting brain responses to natural language. But why do these models work so well? One proposed explanation is that language models and brains are similar because they have the same objective: to predict upcoming words before they are perceived. This explanation is attractive because it lends support to the popular theory of predictive coding. We provide several analyses that cast doubt on this claim. First, we show that the ability to predict future words does not uniquely (or even best) explain why some representations are a better match to the brain than others. Second, we show that within a language model, representations that are best at predicting future words are strictly worse brain models than other representations. Finally, we argue in favor of an alternative explanation for the success of language models in neuroscience: These models are effective at predicting brain responses because they generally capture a wide variety of linguistic phenomena.",
      "publication_date": "2022-11-03",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "0174d263d3a77bf03fce831a9a5ce2678e1959f0",
      "title": "Video Representation Learning by Dense Predictive Coding",
      "abstract": "The objective of this paper is self-supervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We make three contributions: First, we introduce the Dense Predictive Coding (DPC) framework for self-supervised representation learning on videos. This learns a dense encoding of spatio-temporal blocks by recurrently predicting future representations; Second, we propose a curriculum training scheme to predict further into the future with progressively less temporal context. This encourages the model to only encode slowly varying spatial-temporal signals, therefore leading to semantic representations; Third, we evaluate the approach by first training the DPC model on the Kinetics-400 dataset with self-supervised learning, and then finetuning the representation on a downstream task, i.e. action recognition. With single stream (RGB only), DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet.",
      "publication_date": "2019-09-10",
      "venue": "",
      "year": 2019,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2495e0b51367beb828d68abbd00999b4bcdf3c44",
      "title": "Evidence for grid cells in a human memory network",
      "abstract": null,
      "publication_date": "2010-02-04",
      "venue": "Nature",
      "year": 2010,
      "citation_count": 706,
      "authors": [
        "Christian F. Doeller",
        "C. Barry",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "6ea99443ff4d599ee67b42a5afd5c7bf6729c2c5",
      "title": "Neural Representations of Location Composed of Spatially Periodic Bands",
      "abstract": null,
      "publication_date": "2012-08-17",
      "venue": "Science",
      "year": 2012,
      "citation_count": 175,
      "authors": [
        "J. Krupic",
        "N. Burgess",
        "J. O\u2019Keefe"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "de76cd6c61199445e2688402cabbb3119dadc877",
      "title": "Dual phase and rate coding in hippocampal place cells: Theoretical significance and relationship to entorhinal grid cells",
      "abstract": null,
      "publication_date": null,
      "venue": "Hippocampus",
      "year": 2005,
      "citation_count": 521,
      "authors": [
        "J. O\u2019Keefe",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "b0004097eac896ee4bf8b45ba3a7d9f101782542",
      "title": "Microstructure of a spatial map in the entorhinal cortex",
      "abstract": null,
      "publication_date": "2005-08-11",
      "venue": "Nature",
      "year": 2005,
      "citation_count": 3671,
      "authors": [
        "T. Hafting",
        "M. Fyhn",
        "S. Molden",
        "M. Moser",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "252b2d7ab977ed1d1dfc659294318bb2394410d7",
      "title": "Basket-like interneurones in layer II of the entorhinal cortex exhibit a powerful NMDA-mediated synaptic excitation",
      "abstract": null,
      "publication_date": "1993-01-04",
      "venue": "Neuroscience Letters",
      "year": 1993,
      "citation_count": 185,
      "authors": [
        "R. Jones",
        "E. H. B\u00fchl"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "14a90fb87242fd7033eca54671f9f1f7c09c05e1",
      "title": "Grid cell firing may arise from interference of theta frequency membrane potential oscillations in single neurons",
      "abstract": null,
      "publication_date": "2007-12-01",
      "venue": "Hippocampus",
      "year": 2007,
      "citation_count": 265,
      "authors": [
        "M. Hasselmo",
        "Lisa M. Giocomo",
        "Eric A. Zilli"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "fad77fa5f1b84eac354b6d6f7e011259092ce87d",
      "title": "What do grid cells contribute to place cell firing?",
      "abstract": null,
      "publication_date": "2014-03-01",
      "venue": "Trends in Neurosciences",
      "year": 2014,
      "citation_count": 151,
      "authors": [
        "D. Bush",
        "C. Barry",
        "N. Burgess"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "57eb4d347212a9123ff2cfc17839ccd231f6b4b9",
      "title": "Computational Models of Grid Cells",
      "abstract": null,
      "publication_date": "2011-08-25",
      "venue": "Neuron",
      "year": 2011,
      "citation_count": 218,
      "authors": [
        "Lisa M. Giocomo",
        "M. Moser",
        "E. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "78c169ad1636ec9eeea8c02e5c26452272727ede",
      "title": "MEC: An Open-source Fine-grained Mapping Equivalence Checking Tool for FPGA",
      "abstract": "Technology mapping is an essential step in EDA flow. However, the function of the circuit may be changed after technology mapping, and equivalence checking (EC) based verification is highly necessary. The traditional EC method has significant time and resource constraints, making it only feasible to carry out at a coarse-grained level. To make it efficient for technology mapping, we propose a fine-grained method called MEC, which leverages a combination of two approaches to significantly reduce the time cost of verification. The local block verification approach performs fast verification and the global graph cover approach guarantees correctness. The proposed method is rigorously tested and compared to three EC tools, and the results show that MEC technique offers a substantial improvement in speed. MEC not only offers a faster and more efficient way of performing EC on technology mapping but also opens up new opportunities for more fine-grained verification in the future.",
      "publication_date": "2023-05-08",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "569043747e04038c86b73c28f3d269d01e31d7ff",
      "title": "Magnetic Equivalent Circuit and Lagrange Interpolation Function Modeling of Induction Machines Under Broken Bar Faults",
      "abstract": "This article introduces a mesh-based magnetic equivalent circuit (MEC) modeling technique for induction machines (IMs) in healthy and broken rotor bars conditions. The MEC model is presented as a highly accurate and computationally efficient alternative to finite element (FE) models. By incorporating modifications to the air gap coupling method, including a new Lagrange interpolation function, and utilizing a harmonic MEC model, the accuracy of the solution is improved while reducing electrical and mechanical transients. Compared to experiments and 2-D FE models, this model achieves precise results for electromagnetic torque, rotational speed, and forces across various conditions. The Lagrange interpolation function forms the basis for the air gap coupling between stator and rotor flux densities. The results demonstrate the MEC model\u2019s exceptional accuracy in predicting speed oscillations, calculating forces, and analyzing current harmonics in faulty IMs. Furthermore, the MEC model performs over 30 times faster than the 2-D FE models.",
      "publication_date": "2024-03-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e294c3cc0071c5cd34273455342b75182b621908",
      "title": "Analysis and Compensation of the Longitudinal End Effect in Variable Reluctance Linear Resolvers Using Magnetic Equivalent Circuit Model",
      "abstract": "This article focused on the longitudinal end effect (LEE) in linear variable reluctance (VR) resolvers. The LEE can significantly decrease the accuracy of the linear sensor. Therefore, a novel method based on a magnetic equivalent circuit (MEC) model is proposed for accurate modeling of this phenomenon and used in an optimization routine to suppress that. The LEE is modeled by adding three extra permeances into the conventional MEC model without increasing the complexity and simulation time. Besides, the method of calculating these permeances is described. After that, some techniques are presented to decrease the computational burden of the MEC model. Then, the model is used to compensate the LEE by optimizing the turn number of coils. Comparing the results of the proposed model and the finite-element method (FEM) shows that the deviation of the model in the prediction of different inductances is less than 5%, while the simulation time is about 31 times less than that of FEM. Besides, the comparison between the initial design and the optimal design shows the usefulness of the model in the optimization process, in which the accuracy of the sensor improved by 74.1%. Finally, experimental tests on the prototype sensor verify the success of the analysis.",
      "publication_date": "2023-09-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "2ddc7239d99b4b3c5c5407bb62c47b6d48ad8161",
      "title": "Armature Reaction Analysis and Suppression of Voice Coil Actuator Based on an Improved Magnetic Equivalent Circuit Model",
      "abstract": "The high thrust density voice coil actuator (VCA) is a key component for the deployers to determine the CubeSats\u2019 separation velocity in orbit directly. To improve the VCA's thrust density and power consumption, it is essential to analyze the armature reaction under heavy current work conditions. This article proposes a dynamic splitting method of the magnetic equivalent circuit (MEC) model for analyzing the armature reaction of the VCA. This MEC model can effectively reveal the interaction mechanism between the dynamic winding field and the permanent magnet (PM) field, which provides an approach to readily and quickly analyze the armature reaction. Based on the understanding of the mechanism, we found an effective way to suppress the armature reaction and propose a new VCA with double PM flux circuits. The electromagnetic performances of the new configuration are evaluated by comparing it with the conventional one. Both the theoretical and experimental results show that the new structure achieves lower magnetic saturation, lower armature reaction, and higher thrust density. A research prototype was constructed and experimentally tested to verify the theoretical results of the armature reaction, magnetic field, thrust, separation velocity, and other performances. The new VCA realizes a wide and accurate modulation of separation velocity for the CubeSats.",
      "publication_date": "2024-07-01",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "9c1b89701dbf1bb6094536248c0948949012c63c",
      "title": "A Novel Nonlinear Magnetic Equivalent Circuit Model for Magnetic Flux Leakage System",
      "abstract": "To ensure efficient inspection using the magnetic flux leakage (MFL) method, generating a flux density near the saturation level within the tested material is essential. This requirement brings high flux density conditions in the system\u2019s pole regions. Hence, leakage flux within the slot is excessively triggered, leading to distortion of the defect signal. In this context, the system dimensions stand out as one of the most significant factors affecting the mentioned flux distributions. Therefore, various alternative solutions with different system dimensions arise in the design process of the MFL system. This study proposes a magnetic equivalent circuit (MEC) model to achieve optimal system design. The proposed MEC model is designed considering the nonlinear behavior of the material, leakage flux, and fringing effects. Verification results demonstrate that the MEC model consistently tracks the finite element analysis (FEA) results in calculating the flux densities. Furthermore, the relative errors in the flux density calculations of the tested material are at a maximum level of 10.2% and an average of 5.2% compared to the FEA. These findings indicate that the proposed MEC model can be effectively utilized in rapid prototyping and optimization procedures of MFL system design by providing fast solutions with reasonable accuracy.",
      "publication_date": "2024-05-10",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5336257faf7eeaceb742b55f81d4154f89998e42",
      "title": "Mesh-Based 3D MEC Modeling of a Novel Hybrid Claw Pole Generator",
      "abstract": "A brushless parallel hybrid excitation claw pole generator (HECPG) is proposed for electric vehicle (EV) application. Permanent magnet (PM) excitation method can reduce the volume of the machine and improve the power density and efficiency. Moreover, the voltage regulation can be ensured by field excitation. The flux path of the proposed HECPG is complex, and it will take a long time for 3D finite element analysis (FEA) to process it. To reduce simulation time, the mathematical model of the generator is given by a mesh-based 3D magnet equivalent circuit (MEC) network method considering radial and axial flux, magnetic saturation, and magnetic flux leakage. The performance of the generator is analyzed by FEA and prototype experiment. Finally, the results of 3D MEC, FEA, and experiment are compared. There is little difference between the three results, so 3D MEC can ensure the accuracy and significantly reduce the simulation time. The efficiency of the proposed HECPG is 90%, and the DC-Bus voltage can be modulated by changing the amplitude of field current.",
      "publication_date": "2022-02-24",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ed185db19b0b1025817bcde562b7e9b7b2ba1f7e",
      "title": "A Novel Modeling Technique via Coupled Magnetic Equivalent Circuit With Vector Hysteresis Characteristics of Laminated Steels",
      "abstract": "This paper proposes a method to include the anisotropic hysteresis characteristics of soft-magnetic laminated steels in the magnetic equivalent circuit (MEC) modeling. The loop-based MEC formulation is improved to handle the nonlinearity of the anisotropic magnetic hysteresis, including the dynamic classical eddy-current and excess fields. The developed MEC model is coupled with both the single-valued $B$-$H$ curve (SVC) in magnetostatic and the dynamic vector hysteresis model (VHM) in transient analysis. Results with a single elementary MEC element show that an alternating magnetic field in a single direction with a peak value smaller than 300 A/m causes a discrepancy of more than 10% between the magnetic flux densities calculated by the VHM and SVC at 50 and 200 Hz excitation frequencies. Moreover, the proposed modeling technique is verified experimentally using the laminated transformer core of TEAM problem 32. The induced voltage calculated by the MEC model with the VHM demonstrates a good agreement with the measurements, while the MEC model with the SVC calculates inaccurate voltage waveforms. Lastly, the total iron loss dissipated in the transformer's iron core is investigated to verify the proposed technique under different excitation levels and frequencies up to 500 Hz. It is observed that the proposed MEC model with the vector hysteresis characteristics of laminated steels is able to calculate the iron loss accurately, while the conventional single-valued curve method fails to estimate the iron loss.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "1b34f47656f7d490bc433aeb0d4875f2247cb7d3",
      "title": "Improved Configuration Proposal for Axial Reluctance Resolver Using 3-D Magnetic Equivalent Circuit Model and Winding Function Approach",
      "abstract": "Resolver is a position sensor that is widely employed in motor servo systems under harsh working conditions. In this article, a novel axial flux resolver with a fractional-slot sinusoidal distributed winding configuration is developed, which has the advantages of a simple structure and convenient mass production. Sinusoidal distribution winding expands the optional range of pole\u2013slot combinations, and its harmonic suppression capability is proposed and verified by the winding function approach (WFA). Furthermore, the magnetic equivalent circuit (MEC) model is formulated to analyze the impact of structural parameters on the induced voltage envelope, through which the optimal parameters on the resolver are developed. In terms of computation time and accuracy, the results of the MEC model under healthy and eccentric conditions are compared with the finite-element method (FEM). In addition, the antiaxial offset capability of the half-wave resolver is illustrated and verified. The effectiveness and superiority of the proposed resolver are illustrated and verified based on numerical simulations and experimental tests.",
      "publication_date": "2023-03-01",
      "venue": "",
      "year": 2023,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4771f1a1f4e723e7efdc1b7747b00ff17b26ec6e",
      "title": "Two-Segment Magnet Transverse Flux Ferrite PM Generator for Direct-Drive Wind Turbine Applications: Nonlinear 3-D MEC Modeling and Experimental Validation",
      "abstract": "Transverse flux PM generator (TFPMG) is a capable option for direct-drive wind turbine (DDWT) applications due to its high-power characteristics at low speeds. NdFeB-based TFPMGs may suffer from a higher total cost and lower thermal capabilities compared to the ferrite-based TFPMGs. Not yet covered in the existing literature, in this paper a transverse flux ferrite PM generator (TFFPMG) is proposed, designed, and modeled, which can also resolve the unipolar flux generation and even-order harmonics in the flux linkage of the conventional TFPMGs that occurred in conventional TFPMGs through its innovative two-segment trapezoidal shape magnet structure. Due to the 3-D flux path nature in the proposed TFFPMG, either 3-D finite element analysis (FEA) or 3-D magnetic equivalent circuit (MEC) modeling should be used. As a computationally efficient modeling method, a nonlinear 3-D MEC is established to model the entire structure of the TFFPMG while the core saturation and nonlinear permeances are also fully considered to improve the modeling accuracy. The electromagnetic performance of the proposed TFFPMG modeled by 3-D MEC in various loading conditions is validated by both 3-D FEA and experimental results using a 2.5 kW TFFPMG. A close agreement between 3-D MEC modeling predictions, 3-D FEA, and test results confirms the reliability of the MEC modeling for the proposed TFFPMG.",
      "publication_date": "2022-09-01",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "fb3dee071c90c2e860cd4023f682f2b11b8955c1",
      "title": "3D Rotor Position-Dependant MEC Modelling of Different Claw Pole Machine Topologies",
      "abstract": "The paper is aimed at a 3D magnetic equivalent circuit (MEC)-based modelling of claw pole synchronous machine topologies. Beyond the magnetic saturation and the armature magnetic reaction, the proposed modelling approach takes into consideration the rotor position variation, yielding the so-called: rotor position-dependant MEC. Accounting for the complexity of the magnetic circuit of claw pole topologies, specific assumptions are adopted prior a general analytical derivation of their MEC models. The developed analytical approach focuses on the air gap reluctance under variable rotor position considering a simplified geometry of the claw. A dedicated numerical procedure based on the Newton-Raphson algorithm is proposed for the resolution of the designed rotor position-dependant MEC. The proposed approach is applied to three claw pole topologies. The two first ones are equipped with a single source of excitation achieved by a field. Their analytically-predicted features are validated by experiments. The third topology has a dual excitation achieved by a field and permanent magnets (PMs) in the rotor. Its analytically-predicted features are validated by 3D finite element analysis (FEA). It is found that both experimental and FEA results are in quite good agreement with the analytical predictions yielded by the proposed rotor position-dependant MEC.",
      "publication_date": null,
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "6aa68970ec381a78a75fc30555884d4e8f1cb672",
      "title": "Place units in the hippocampus of the freely moving rat",
      "abstract": null,
      "publication_date": "1976-12-31",
      "venue": "Experimental Neurology",
      "year": 1976,
      "citation_count": 1807,
      "authors": [
        "J. O\u2019Keefe"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "84c888d94d93137b767ec982c58e6829b10ed88a",
      "title": "Grid cells in mice",
      "abstract": null,
      "publication_date": "2008-12-01",
      "venue": "Hippocampus",
      "year": 2008,
      "citation_count": 174,
      "authors": [
        "M. Fyhn",
        "T. Hafting",
        "M. Witter",
        "E. Moser",
        "M. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "ab7b9e504d427abbe7798e441442566ed7ea967a",
      "title": "Joint Computation Offloading and Radio Resource Allocation in MEC-Based Wireless-Powered Backscatter Communication Networks",
      "abstract": "The multi-access edge computing (MEC)-based wireless-powered backscatter communication networks (WP-BackComNets) allow wireless devices (WDs) to offload computation resources to lightweight and widely deployed MEC servers with the assistance of backscatter devices (BDs), which have substantial application prospects for the emerging Internet-of-Things applications. However, the limited battery capacity of WDs is one of the bottlenecks restricting its further development. Reducing the energy consumption and the computation burden of WDs while ensuring the quality-of-service requirements is an urgent issue. To this end, a joint computation offloading and radio resource allocation problem is formulated to minimize the total energy consumption of WDs for an MEC-based WP-BackComNet by jointly optimizing user association, the transmit power and transmission time of WDs, the computational offloading coefficient of each task, and the reflection coefficients of BDs, where the circuit power consumption of BDs, the computational capabilities of WDs, and the task execution delay budgets are considered. To handle this non-convex problem, we propose an efficient algorithm to obtain a suboptimal solution. Simulation results demonstrate that the proposed scheme can effectively decrease the energy consumption compared with the benchmarks.",
      "publication_date": "2021-05-03",
      "venue": "",
      "year": 2021,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "39201db4005cfccd9290bf96e1d2cb3132db1e16",
      "title": "Development of a Combined Maxwell's Equations and Magnetic Equivalent Circuit Solution for Induction Machines in Electric Vehicle Applications",
      "abstract": "This study introduces a novel analytical technique for analyzing the magnetic field in induction motors (IM). This method combines Maxwell's equations with the magnetic equivalent circuit (MEC) framework to determine flux densities in various regions of the motor. By integrating flux sources from Maxwell's equations into the MEC's network of reluctances, this method accurately considers the distribution of main and leakage magnetic fluxes within the motor. The approach significantly reduces analysis time while maintaining high accuracy and reliability when compared to numerical methods. Furthermore, it provides important motor features such as the radial and tangential components of the flux density in the air-gap, stator and rotor areas, as well as the induced voltage. Finally, to validate the accuracy of the proposed method, the analytical results are compared to the case that no leakage fluxes are included in the analytical model and to the finite-element method (FEM) in terms of computation time and accuracy. As such, this method is suggested to serve as a valuable analytical tool during the design process of IMs.",
      "publication_date": "2024-05-05",
      "venue": "",
      "year": 2024,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "aa8ab6a377cdcc5f0614b8f8beb7f267782319ac",
      "title": "Control of hexagonal plate-like microstructure of in-situ calcium hexaluminate in monolithic refractories",
      "abstract": "ABSTRACT The self-toughening property of calcium hexaluminate (CaO\u20226Al2O3, CA6) in engineering ceramics causes a remarkable interest in the in-situ CA6 in monolithic refractory. The determination of significant factors which controls the formation and microstructure of CA6 in monolithic refractory is mainly discussed in this study. Samples were prepared by using calcium aluminate cement and sintered alumina and the chemical composition was fixed at the molar ratio of CaO: Al2O3 = 1:6. In order to evaluate the suitable sintering condition and SiO2 content for CA6 phase formation, the variation of firing temperature and holding time was first modified at 1400\u20131500\u00b0C for 1\u20135 h. The second factor was done by adding 2\u201310 mass% of SiO2 into the primary mixture. In comparison, the results were confirmed that high firing temperature at 1500\u00b0C provided the highest quantity of CA6 phase and plate-like microstructure. In addition, longer proceeding time contributed to the grain growth of CA6, especially, within 5 h of holding time. SiO2 importantly helped to enhance hexagonal plate-like microstructure due to the ability of ion mobility in low-melting phase of gehlenite but the suitable content of SiO2 should not exceed 2 mass% for the better control ability of CA6 formation and microstructure.",
      "publication_date": "2018-07-03",
      "venue": "",
      "year": 2018,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5b66541e9cc6708be8f70405b6f51b1c41fa34dc",
      "title": "Features of formation of the celsian phase during firing of heat-resistant ceramics in the system BaO\u2013Al2O3\u2013SiO2",
      "abstract": "In the synthesis of heat-resistant ceramics in the system BaO\u2013Al2O3\u2013SiO2 (BAS), it is important to find ways of intensifying the process of transition of hexagonal celsian to monoclinic celsian without an increase in sintering temperature. Monoclinic form of celsian is characterized by higher thermal, electro-insulating, and mechanical properties. This paper deals with the features of formation of the phase composition of celsian ceramics when using BAS glass of eutectic composition and glass in the system Li2O\u2013Al2O3\u2013B2O3\u2013SiO2 (LABS) of spodumene composition as modifying components. It is shown that monoclinic celsian is the final crystalline phase formed in ceramics synthesized on the basis of barium carbonate and kaolin. Monoclinic celsian is formed stepwise; and the hexagonal celsian appears first. The complete transition hexagonal celsian\uf0aemonoclinic celsian occurs only in the process of high temperature firing at 12500C. Notably, the degree of ceramic sintering remains low (water absorption is 11.0%). Introduction of BAS glass contributes to the complete transition of hexagonal celsian to monoclinic celsian at a reduced temperature of 11000C. Maximum effect in the formation of monoclinic celsian is achieved by the introduction of LABS glass. As a result, the temperature of formation of this modification maximally decreases to 8000C. In this case, complete sintering of celsian ceramics is achieved at the temperature of 12500\u0421.",
      "publication_date": "2022-06-01",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "e73959ee703c09e2b05531cf5ef1bff51598abe3",
      "title": "Bioelectrochemical enhancement of methane production from exhausted vine shoot fermentation broth by integration of MEC with anaerobic digestion",
      "abstract": "A microbial electrolysis cell integrated in an anaerobic digestion system (MEC-AD) is an efficient configuration to produce methane from an exhausted vine shoot fermentation broth (EVS). The cell worked in a single-chamber two-electrode configuration at an applied potential of 1\u00a0V with a feeding ratio of 30/70 (30% EVS to 70% synthetic medium). In addition, an identical cell operated in an open circuit was used as a control reactor. Experimental results showed similar behavior in terms of carbon removal (70\u201376%), while the specific averaged methane production from cycle 7 was more stable and higher in the connected cell (MECAD) compared with the unpolarized one (OCAD) accounting for 403.7\u2009\u00b1\u200933.6 L CH4\u00b7kg VS\u22121 and 121.3\u2009\u00b1\u200949.7 L CH4\u00b7kg VS\u22121, respectively. In addition, electrochemical impedance spectroscopy revealed that the electrical capacitance of the bioanode in MECAD was twice the capacitance shown by OCAD. The bacterial community in both cells was similar but a clear adaptation of Methanosarcina Archaea was exhibited in MECAD, which could explain the increased yields in CH4 production. In summary, the results reported here confirm the advantages of integrating MEC-AD for the treatment of real organic liquid waste instead of traditional AD treatment.",
      "publication_date": "2022-06-08",
      "venue": "",
      "year": 2022,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "ddf9626f10f4a5fd38e2332f1720c3f505783632",
      "title": "Influence of Firing Atmosphere on the Cubic-Hexagonal Transition and the Chemical State of Titanium in BaTiO3",
      "abstract": "The effect of reducing gas mixture, H2-N2 or CO-N2, on the cubic-hexagonal transition in BaTiO3 has been investigated by X-ray diffraction, ESR, ESCA, PAS and chemical analysis. The firing temperatures of BaTiO3 were 1380\u00b0C and 1500\u00b0C, and the transition temperature in air is already known to be 1460\u00b0C. The fractional conversion to the hexagonal phase increased with increasing concentrations of reducing gases, larger for samples fired in H2-N2 than those fired in CO-N2 for the same concentrations of the reducing gases. The hexagonal phase in all samples increased proportionally to the amount of Ti3+ produced by reduction of Ti4+ ions, regardless of the reducing gas species and firing temperatures. It was found that the minimum amount of Ti3+ ions to stabilize the hexagonal BaTiO3 at room temperature was 0.3% of total Ti ions. ESCA spectra due to Ti3+ were observed in the sample fired in 100% H2 at 1380\u00b0C.",
      "publication_date": null,
      "venue": "",
      "year": 1987,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "4a5f9b6232a0ac08c09ef473c419e691295e8a23",
      "title": "Effect of Firing Atmosphere on the Cubic-Hexagonal Transition in Ba0.99Sr0.01TiO3",
      "abstract": "Ba0.99Sr0.01Tio3 ceramics were fired for 1 hour at a temperature from 1410\u00b0 to 1460\u00b0C in H2-N2 atmospheres. A pure hexagonal phase, which was not formed by calcination in air, was obtained by firing for 1 hour at 1430\u00b0C in H2 atmosphere. A part of the oxygen in Ba0.99Sr0.01TiO3 was removed under the reducing condition, which led to the formation of the hexagonal phase. The amount of the hexagonal phase of Ba0.99Sr0.01TiO3-x increased proportionally with an increase in oxygen deficiency (x). The value of x required for stabilizing the hexagonal phase at room temperature was found to be larger than 0.008. The transition temperature from cubic to hexagonal phase of Ba0.99Sr0.01TiO3-x was higher than that of BaTiO3-x, and oxygen deficiency required for stabilizing the hexagonal phase was larger than that for BaTiO3-x. These results indicated that Ba0.99Sr0.01TiO3-x does not easily transformed into the hexagonal phase.",
      "publication_date": "1990-08-01",
      "venue": "",
      "year": 1990,
      "citation_count": 0,
      "authors": "",
      "novel": null,
      "cited_paper": false
    },
    {
      "paper_id": "5e2fdf42d985b3eb9eeb8387e1f7d093ade81525",
      "title": "Vector-based navigation using grid-like representations in artificial agents",
      "abstract": null,
      "publication_date": "2018-05-09",
      "venue": "Nature",
      "year": 2018,
      "citation_count": 612,
      "authors": [
        "Andrea Banino",
        "C. Barry",
        "Benigno Uria",
        "C. Blundell",
        "T. Lillicrap",
        "Piotr Wojciech Mirowski",
        "A. Pritzel",
        "M. Chadwick",
        "T. Degris",
        "Joseph Modayil",
        "Greg Wayne",
        "Hubert Soyer",
        "Fabio Viola",
        "Brian Zhang",
        "Ross Goroshin",
        "Neil C. Rabinowitz",
        "Razvan Pascanu",
        "Charlie Beattie",
        "Stig Petersen",
        "Amir Sadik",
        "Stephen Gaffney",
        "Helen King",
        "K. Kavukcuoglu",
        "D. Hassabis",
        "R. Hadsell",
        "D. Kumaran"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "72823437d781996a412428b097b4b562fe846716",
      "title": "Prediction and memory: A predictive coding account",
      "abstract": null,
      "publication_date": "2020-05-21",
      "venue": "Progress in neurobiology",
      "year": 2020,
      "citation_count": 144,
      "authors": [
        "Helen C. Barron",
        "R. Auksztulewicz",
        "Karl J. Friston"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "57070167d647bfa23bc5c5895623af13576db66b",
      "title": "A tutorial on the free-energy framework for modelling perception and learning",
      "abstract": null,
      "publication_date": "2017-02-01",
      "venue": "Journal of Mathematical Psychology",
      "year": 2017,
      "citation_count": 269,
      "authors": [
        "R. Bogacz"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "198acc8504895b1ba3211ea3f1e8fe0cf106b753",
      "title": "Impression learning: Online representation learning with synaptic plasticity",
      "abstract": null,
      "publication_date": null,
      "venue": "Neural Information Processing Systems",
      "year": 2021,
      "citation_count": 9,
      "authors": [
        "C. Bredenberg",
        "Benjamin Lyo",
        "Eero P. Simoncelli",
        "Cristina Savin"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "28c3c3d07d27b854babfac7b109ba15fca09437c",
      "title": "Evidence for area CA1 as a match/mismatch detector: A high\u2010resolution fMRI study of the human hippocampus",
      "abstract": null,
      "publication_date": "2012-03-01",
      "venue": "Hippocampus",
      "year": 2012,
      "citation_count": 238,
      "authors": [
        "Katherine Duncan",
        "Nicholas A. Ketz",
        "S. Inati",
        "L. Davachi"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "c04713059cfdbff7f3cce0b10a5fd6fc7cdee43e",
      "title": "Relating Hippocampal Circuitry to Function Recall of Memory Sequences by Reciprocal Dentate\u2013CA3 Interactions",
      "abstract": null,
      "publication_date": "1999-02-01",
      "venue": "Neuron",
      "year": 1999,
      "citation_count": 632,
      "authors": [
        "J. Lisman"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "3edd190cc7d541f6ae410e5be3d82868532a39f6",
      "title": "Development of the Spatial Representation System in the Rat",
      "abstract": null,
      "publication_date": "2010-06-18",
      "venue": "Science",
      "year": 2010,
      "citation_count": 600,
      "authors": [
        "R. Langston",
        "J. Ainge",
        "J. J. Couey",
        "Cathrin B. Canto",
        "T. Bjerknes",
        "M. Witter",
        "E. Moser",
        "M. Moser"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
      "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
      "abstract": null,
      "publication_date": "1996-06-13",
      "venue": "Nature",
      "year": 1996,
      "citation_count": 6169,
      "authors": [
        "B. Olshausen",
        "D. Field"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "8fcbc38e7196b0dc8748f04cd6101e71f92c158e",
      "title": "A theory of cortical responses",
      "abstract": null,
      "publication_date": "2005-04-29",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences",
      "year": 2005,
      "citation_count": 3972,
      "authors": [
        "Karl J. Friston"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "1c9852b8cbeb0668cfd5e3aeef01f9ce9a123843",
      "title": "The emergence of grid cells: Intelligent design or just adaptation?",
      "abstract": null,
      "publication_date": "2008-12-01",
      "venue": "Hippocampus",
      "year": 2008,
      "citation_count": 245,
      "authors": [
        "E. Kropff",
        "A. Treves"
      ],
      "novel": null,
      "cited_paper": true
    },
    {
      "paper_id": "42ce761c85bdb0d422917b03751ab9cbc72a3417",
      "title": "Backpropagation through time and the brain",
      "abstract": null,
      "publication_date": "2019-03-07",
      "venue": "Current Opinion in Neurobiology",
      "year": 2019,
      "citation_count": 127,
      "authors": [
        "T. Lillicrap",
        "Adam Santoro"
      ],
      "novel": null,
      "cited_paper": true
    }
  ]
}